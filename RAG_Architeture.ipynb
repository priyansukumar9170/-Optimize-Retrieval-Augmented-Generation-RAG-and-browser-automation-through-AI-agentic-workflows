{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11372,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 5388,
          "modelId": 3533
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10ff596a43f94c1482213cad2f88f6de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72d9daf6f8a7472cb83dde24739a7ad5",
              "IPY_MODEL_3d21612ed6154500a2cf8c08b81fe855",
              "IPY_MODEL_deaa884184eb4fd5aeb88cddf4b3f7f3"
            ],
            "layout": "IPY_MODEL_aed94c2db0a045559066b210603a6c14"
          }
        },
        "72d9daf6f8a7472cb83dde24739a7ad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43edfcb9e06c4bf6bec0fd30e7a1da7c",
            "placeholder": "​",
            "style": "IPY_MODEL_318ef2631c3c4490a833af1495ba8cd1",
            "value": ""
          }
        },
        "3d21612ed6154500a2cf8c08b81fe855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a4c3e1960b427b96fdb886d0ba6eee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1d973c81c5341e78c04b7c63943b8c2",
            "value": 1
          }
        },
        "deaa884184eb4fd5aeb88cddf4b3f7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e66139b8cf549f080c17abcde1381eb",
            "placeholder": "​",
            "style": "IPY_MODEL_066be66235564b7d88e24000a5af671c",
            "value": " 15/? [00:00&lt;00:00, 39.93it/s]"
          }
        },
        "aed94c2db0a045559066b210603a6c14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43edfcb9e06c4bf6bec0fd30e7a1da7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "318ef2631c3c4490a833af1495ba8cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "46a4c3e1960b427b96fdb886d0ba6eee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d1d973c81c5341e78c04b7c63943b8c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e66139b8cf549f080c17abcde1381eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066be66235564b7d88e24000a5af671c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1d590dfdd54ed8bce095f002758635": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fdb65cae0bbf439ab2da163951dd2d96",
              "IPY_MODEL_6bb6e8c7e6d040a6a5eff0e3dc02ec81",
              "IPY_MODEL_2dc009815ae24155be0b0de48b6032fa"
            ],
            "layout": "IPY_MODEL_10460e9582c84eceadb3e9e2a3d0544e"
          }
        },
        "fdb65cae0bbf439ab2da163951dd2d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a30dd291d484d2b977e26fecdb43fcf",
            "placeholder": "​",
            "style": "IPY_MODEL_119039938e574c5c97c830090925867a",
            "value": "100%"
          }
        },
        "6bb6e8c7e6d040a6a5eff0e3dc02ec81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_419ccdc6e795429ea16252df76b680f0",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c390a76c915146a18258bd86024f3441",
            "value": 15
          }
        },
        "2dc009815ae24155be0b0de48b6032fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803ed70d631a472d936fccc1411eb6d3",
            "placeholder": "​",
            "style": "IPY_MODEL_adb98018912746268669ad145cee4fc5",
            "value": " 15/15 [00:00&lt;00:00, 96.33it/s]"
          }
        },
        "10460e9582c84eceadb3e9e2a3d0544e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a30dd291d484d2b977e26fecdb43fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "119039938e574c5c97c830090925867a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "419ccdc6e795429ea16252df76b680f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c390a76c915146a18258bd86024f3441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "803ed70d631a472d936fccc1411eb6d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adb98018912746268669ad145cee4fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7751821216bf49cfa338d67d528078f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_266f949dd15542eeb56bd1fddd5e4b0e",
              "IPY_MODEL_7337059e2e114298a4c3c9ca1e60879c",
              "IPY_MODEL_f066f6f3b1e1436f9b6f770306290779"
            ],
            "layout": "IPY_MODEL_1b3d46b1153b4beea5b6363988a87ae2"
          }
        },
        "266f949dd15542eeb56bd1fddd5e4b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5172b74c7de1421c8ce9b92a9c6eef42",
            "placeholder": "​",
            "style": "IPY_MODEL_a86e9d6469544c38bb6539e9d31dd4b4",
            "value": "100%"
          }
        },
        "7337059e2e114298a4c3c9ca1e60879c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ede91bac2a7421c86cedfa5f09e3961",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f475ba3be7bc4361b4408d5a1fe60688",
            "value": 15
          }
        },
        "f066f6f3b1e1436f9b6f770306290779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff238a030f3457d853a2a61efeab48c",
            "placeholder": "​",
            "style": "IPY_MODEL_51b8979e91d742f9b336de39b46427de",
            "value": " 15/15 [00:00&lt;00:00, 1471.31it/s]"
          }
        },
        "1b3d46b1153b4beea5b6363988a87ae2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5172b74c7de1421c8ce9b92a9c6eef42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86e9d6469544c38bb6539e9d31dd4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ede91bac2a7421c86cedfa5f09e3961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f475ba3be7bc4361b4408d5a1fe60688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aff238a030f3457d853a2a61efeab48c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b8979e91d742f9b336de39b46427de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3d4cd80759c40af8525e2c958e26a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f505fb50f0534ce58db4fd1cc9658818",
              "IPY_MODEL_699690705d244129858c6462f1bff509",
              "IPY_MODEL_2ae4f430ef844a74b887bf0cbfe97919"
            ],
            "layout": "IPY_MODEL_81e8a6c0adab4a949892f52e543c7c47"
          }
        },
        "f505fb50f0534ce58db4fd1cc9658818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a30e618bfad24276b19970e67fa4a269",
            "placeholder": "​",
            "style": "IPY_MODEL_8b75e642358c4ab8a59bb9a190922669",
            "value": "100%"
          }
        },
        "699690705d244129858c6462f1bff509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_376ba7d272b248878ebc08a25086556e",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_569ec45736ec4432b3275667d3091b65",
            "value": 15
          }
        },
        "2ae4f430ef844a74b887bf0cbfe97919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4aa7565b1454046962468a8364355c9",
            "placeholder": "​",
            "style": "IPY_MODEL_f9078e4457444e4592ff0fd20ff77119",
            "value": " 15/15 [00:00&lt;00:00, 926.93it/s]"
          }
        },
        "81e8a6c0adab4a949892f52e543c7c47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a30e618bfad24276b19970e67fa4a269": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b75e642358c4ab8a59bb9a190922669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "376ba7d272b248878ebc08a25086556e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "569ec45736ec4432b3275667d3091b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4aa7565b1454046962468a8364355c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9078e4457444e4592ff0fd20ff77119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aea3bfcc8c234abaa6d9ac9a67bb822f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddac13736d5449d4bacda13fc6318aa4",
              "IPY_MODEL_dd426df76dc54f28896f14fdc78b9865",
              "IPY_MODEL_3525a51b6597485785b58c471fb5670b"
            ],
            "layout": "IPY_MODEL_1132e2705ff946838764a5ebd39ee6f1"
          }
        },
        "ddac13736d5449d4bacda13fc6318aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff75dbcffde24779982d452b475fcfe9",
            "placeholder": "​",
            "style": "IPY_MODEL_3e4982932b9e45d991e9bc6e48a63db7",
            "value": "modules.json: 100%"
          }
        },
        "dd426df76dc54f28896f14fdc78b9865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92fbd2ea4bc647e08e240bda81fc1812",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c39a8f315b874e6e9dc1269bc11cdc81",
            "value": 349
          }
        },
        "3525a51b6597485785b58c471fb5670b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cdf489b29e644c394293ae41f3528ab",
            "placeholder": "​",
            "style": "IPY_MODEL_1b9675a25b1146de8e557f40fa7a0d3a",
            "value": " 349/349 [00:00&lt;00:00, 34.2kB/s]"
          }
        },
        "1132e2705ff946838764a5ebd39ee6f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff75dbcffde24779982d452b475fcfe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4982932b9e45d991e9bc6e48a63db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92fbd2ea4bc647e08e240bda81fc1812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c39a8f315b874e6e9dc1269bc11cdc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cdf489b29e644c394293ae41f3528ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b9675a25b1146de8e557f40fa7a0d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22eb1ad113c440eebaca39ca005f4c38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d3e9c1ec30c4aa49db27cd4321cf49b",
              "IPY_MODEL_edb08652f23a498f824a96e98a1cb157",
              "IPY_MODEL_7190ab7c34f449dd898188e470623204"
            ],
            "layout": "IPY_MODEL_7abdd556473d4f5a8289e3705e5034cf"
          }
        },
        "7d3e9c1ec30c4aa49db27cd4321cf49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_374d49fb0ea44f8a82125d228468b352",
            "placeholder": "​",
            "style": "IPY_MODEL_2273104f1b8d4785b57498df48f215e7",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "edb08652f23a498f824a96e98a1cb157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63946ebbf7934626847b9def822917b2",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9903d74f505e4fb79acc2cbd04908c11",
            "value": 116
          }
        },
        "7190ab7c34f449dd898188e470623204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc8421d6228149aea78c928309c9855c",
            "placeholder": "​",
            "style": "IPY_MODEL_19debcad98d542e5b07b103e159b95cb",
            "value": " 116/116 [00:00&lt;00:00, 11.8kB/s]"
          }
        },
        "7abdd556473d4f5a8289e3705e5034cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "374d49fb0ea44f8a82125d228468b352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2273104f1b8d4785b57498df48f215e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63946ebbf7934626847b9def822917b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9903d74f505e4fb79acc2cbd04908c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc8421d6228149aea78c928309c9855c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19debcad98d542e5b07b103e159b95cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2987b3b7b0d74e9aa7f9f3d7150d7e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d6ee919744548938da89457d5febf67",
              "IPY_MODEL_1f6c601f6e534a80853166ad08de14e7",
              "IPY_MODEL_3a851740e0514df992df34f299593490"
            ],
            "layout": "IPY_MODEL_ca6bed010c9e4c608fe73ebf8bb17ed8"
          }
        },
        "8d6ee919744548938da89457d5febf67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98091a42df1d4dae8b4fce3dc99cd710",
            "placeholder": "​",
            "style": "IPY_MODEL_13712679714d4c489167433681d3046a",
            "value": "README.md: 100%"
          }
        },
        "1f6c601f6e534a80853166ad08de14e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a593054201604be788a4c76a3dc95a45",
            "max": 10415,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_884874da51e04727a67de7233b1c3df5",
            "value": 10415
          }
        },
        "3a851740e0514df992df34f299593490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_784475da561b413480e8a88618acd77e",
            "placeholder": "​",
            "style": "IPY_MODEL_25eae10840214cceb8a1657e6e65cfed",
            "value": " 10.4k/10.4k [00:00&lt;00:00, 1.01MB/s]"
          }
        },
        "ca6bed010c9e4c608fe73ebf8bb17ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98091a42df1d4dae8b4fce3dc99cd710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13712679714d4c489167433681d3046a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a593054201604be788a4c76a3dc95a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "884874da51e04727a67de7233b1c3df5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "784475da561b413480e8a88618acd77e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25eae10840214cceb8a1657e6e65cfed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c36b997dc234f2a9cc33a372efdfdd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40179b536ffe4f1ca35611c4041083c5",
              "IPY_MODEL_ac9d2a04b762483885307ce533c37eeb",
              "IPY_MODEL_c4f65d06c4e34e9b817d3efaf2c76cfa"
            ],
            "layout": "IPY_MODEL_280dda81615343ed8d92af59c03db315"
          }
        },
        "40179b536ffe4f1ca35611c4041083c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de1e164c373b4ce4b9617b6a30b15c63",
            "placeholder": "​",
            "style": "IPY_MODEL_241c2d1624dd4fb29b21b129b559cd32",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "ac9d2a04b762483885307ce533c37eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_350d63a90f9f42fb86a58bacac9a06ea",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac163c44f4e84820929850640f9d6ce1",
            "value": 53
          }
        },
        "c4f65d06c4e34e9b817d3efaf2c76cfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0266f48fcc854ed0b54119bf4e535028",
            "placeholder": "​",
            "style": "IPY_MODEL_cd1fa6c889eb461da149d372a7614284",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.82kB/s]"
          }
        },
        "280dda81615343ed8d92af59c03db315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de1e164c373b4ce4b9617b6a30b15c63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "241c2d1624dd4fb29b21b129b559cd32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "350d63a90f9f42fb86a58bacac9a06ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac163c44f4e84820929850640f9d6ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0266f48fcc854ed0b54119bf4e535028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1fa6c889eb461da149d372a7614284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "869797c73fe84ba78f8f74090a7886bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3469b0d8325e458b9449ae599bc3559f",
              "IPY_MODEL_10a695234c954169b299b46b9033e1b8",
              "IPY_MODEL_2cd6de721694423983b76b05e8fdec76"
            ],
            "layout": "IPY_MODEL_708d8bfde5c24575a4b471da298640f2"
          }
        },
        "3469b0d8325e458b9449ae599bc3559f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_050de6dc596340609cbe652871e872c8",
            "placeholder": "​",
            "style": "IPY_MODEL_8b1c534cca334ffd86b81299f6f867ec",
            "value": "config.json: 100%"
          }
        },
        "10a695234c954169b299b46b9033e1b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92e0c9dae34d4b92b3f6013ce6314fef",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_717781c958264c96b954ef7c1584c409",
            "value": 571
          }
        },
        "2cd6de721694423983b76b05e8fdec76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_928a76c83174433ba8e23db4509aca5f",
            "placeholder": "​",
            "style": "IPY_MODEL_2359c98bf2874153baedd93778fb1f26",
            "value": " 571/571 [00:00&lt;00:00, 30.6kB/s]"
          }
        },
        "708d8bfde5c24575a4b471da298640f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "050de6dc596340609cbe652871e872c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b1c534cca334ffd86b81299f6f867ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92e0c9dae34d4b92b3f6013ce6314fef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717781c958264c96b954ef7c1584c409": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "928a76c83174433ba8e23db4509aca5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2359c98bf2874153baedd93778fb1f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb4e31c56ea84ac8a28e6dca71e8a3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01d07fda8d2c4077a2cb5ec8e5291a33",
              "IPY_MODEL_8707dd9f0f054b09896f86cf631c436c",
              "IPY_MODEL_7df78c9de0574f6ca1c3ac69c9c92e27"
            ],
            "layout": "IPY_MODEL_4986152b4eeb499aaa82a8b829b39d7b"
          }
        },
        "01d07fda8d2c4077a2cb5ec8e5291a33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da47b3c92f864fb99d63f6ce388d8701",
            "placeholder": "​",
            "style": "IPY_MODEL_717f5f4168cf40199473fcad6ff96416",
            "value": "model.safetensors: 100%"
          }
        },
        "8707dd9f0f054b09896f86cf631c436c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d51e5e5f748344e5a1b2479a07b35739",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1285bfa576b14c269be103c0f49891db",
            "value": 437971872
          }
        },
        "7df78c9de0574f6ca1c3ac69c9c92e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_110937b8e0f44966bc98a764c3417d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_11b0a071caaa47f3be99bba588bd2895",
            "value": " 438M/438M [00:04&lt;00:00, 125MB/s]"
          }
        },
        "4986152b4eeb499aaa82a8b829b39d7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da47b3c92f864fb99d63f6ce388d8701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "717f5f4168cf40199473fcad6ff96416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d51e5e5f748344e5a1b2479a07b35739": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1285bfa576b14c269be103c0f49891db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "110937b8e0f44966bc98a764c3417d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b0a071caaa47f3be99bba588bd2895": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9a19a989a174aeebc16ce427dae0410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10c15905e61b4f2599112a1d53c9936a",
              "IPY_MODEL_554495acc7dd47aeb195da87631179c9",
              "IPY_MODEL_de498663710a4410b3b129ea99284f33"
            ],
            "layout": "IPY_MODEL_d2509c908be841cba0b05867c6e56d6e"
          }
        },
        "10c15905e61b4f2599112a1d53c9936a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3363e55e7b8c4f7280a6aa5ab0d5816b",
            "placeholder": "​",
            "style": "IPY_MODEL_9f2fcd7c9a934540a97cf51c05b4b23b",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "554495acc7dd47aeb195da87631179c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c160fe5f4d4648cda1a49565a667aacc",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de9b0547f8be468ea76dfe7924d0f642",
            "value": 363
          }
        },
        "de498663710a4410b3b129ea99284f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f80cd7c6fa22473ebccf1677cbdaf8dc",
            "placeholder": "​",
            "style": "IPY_MODEL_ad91d230557444b19e6941718dd93c11",
            "value": " 363/363 [00:00&lt;00:00, 36.1kB/s]"
          }
        },
        "d2509c908be841cba0b05867c6e56d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3363e55e7b8c4f7280a6aa5ab0d5816b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f2fcd7c9a934540a97cf51c05b4b23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c160fe5f4d4648cda1a49565a667aacc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de9b0547f8be468ea76dfe7924d0f642": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f80cd7c6fa22473ebccf1677cbdaf8dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad91d230557444b19e6941718dd93c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce5c2cb9432241f3bec883832f652574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8f56fb40fd2465696746bf6af7a5d3b",
              "IPY_MODEL_2917931ea17448699e675d4d916737cd",
              "IPY_MODEL_a507d6bca8dc44b3ba3a4abf7214e7fe"
            ],
            "layout": "IPY_MODEL_9128fa39e3c14f119668b0c5779081c8"
          }
        },
        "d8f56fb40fd2465696746bf6af7a5d3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0eb00989ed54d68b83bf157299f1088",
            "placeholder": "​",
            "style": "IPY_MODEL_c249793c260a4b8fa858850e72c53bf6",
            "value": "vocab.txt: 100%"
          }
        },
        "2917931ea17448699e675d4d916737cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5af833fc7ea54b8e84256051b04c9fa5",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df04652ceba24e9b900cdd622359f2d5",
            "value": 231536
          }
        },
        "a507d6bca8dc44b3ba3a4abf7214e7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec51759b9c72425fb2b2f22339e1bd2d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f93ec99e0a543a79d0fe6d3d8e2bee7",
            "value": " 232k/232k [00:00&lt;00:00, 6.80MB/s]"
          }
        },
        "9128fa39e3c14f119668b0c5779081c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0eb00989ed54d68b83bf157299f1088": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c249793c260a4b8fa858850e72c53bf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af833fc7ea54b8e84256051b04c9fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df04652ceba24e9b900cdd622359f2d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec51759b9c72425fb2b2f22339e1bd2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f93ec99e0a543a79d0fe6d3d8e2bee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b307da5f26b42d8abb4c4ed82f6c56e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_052a224a54be46e686437e53ad980d33",
              "IPY_MODEL_3ed8be2f07b640d7afc906207e518d9e",
              "IPY_MODEL_e9448f689a384c2f92985e7341535a1c"
            ],
            "layout": "IPY_MODEL_c7b490885b634240968fd7c3ffb21ce5"
          }
        },
        "052a224a54be46e686437e53ad980d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9da138bc6f84e518761a9c7c244ad38",
            "placeholder": "​",
            "style": "IPY_MODEL_d60f880b523a41919e21dde46a7b2c29",
            "value": "tokenizer.json: 100%"
          }
        },
        "3ed8be2f07b640d7afc906207e518d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a671e18f7ffd40dab448e2929fb9dc4f",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7433853a2e4c4d788975987ddcda5d02",
            "value": 466021
          }
        },
        "e9448f689a384c2f92985e7341535a1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c89197efa4496da98d16725ae87f70",
            "placeholder": "​",
            "style": "IPY_MODEL_ff0aa0cf45ed4590a6b6840fb0f7b8b8",
            "value": " 466k/466k [00:00&lt;00:00, 1.04MB/s]"
          }
        },
        "c7b490885b634240968fd7c3ffb21ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9da138bc6f84e518761a9c7c244ad38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d60f880b523a41919e21dde46a7b2c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a671e18f7ffd40dab448e2929fb9dc4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7433853a2e4c4d788975987ddcda5d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "70c89197efa4496da98d16725ae87f70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff0aa0cf45ed4590a6b6840fb0f7b8b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58c22290961f483ba4715357c939acf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89dde1c55eec44c18310eb61cfff320c",
              "IPY_MODEL_5319521d0dd848b09f17cdfb151705bb",
              "IPY_MODEL_6667ac62ff54499bb060beb98b09214f"
            ],
            "layout": "IPY_MODEL_b1d46f27f004462ead718c08f77f3eea"
          }
        },
        "89dde1c55eec44c18310eb61cfff320c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34c29c1149c149ef861817a7ec5e6623",
            "placeholder": "​",
            "style": "IPY_MODEL_13a142e538c941158aa3d0fbac389066",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "5319521d0dd848b09f17cdfb151705bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94270a894da04d548b7a25ddf6dafaac",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_800ef320d318428286332d7c7c236747",
            "value": 239
          }
        },
        "6667ac62ff54499bb060beb98b09214f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a407f78e6ba14c52a088b4edd4e5c897",
            "placeholder": "​",
            "style": "IPY_MODEL_91d48e31f01f4817b71b312eb00ab95a",
            "value": " 239/239 [00:00&lt;00:00, 27.0kB/s]"
          }
        },
        "b1d46f27f004462ead718c08f77f3eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34c29c1149c149ef861817a7ec5e6623": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a142e538c941158aa3d0fbac389066": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94270a894da04d548b7a25ddf6dafaac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "800ef320d318428286332d7c7c236747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a407f78e6ba14c52a088b4edd4e5c897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91d48e31f01f4817b71b312eb00ab95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b99ae19a0f3b4da7845afda1d83a638c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_362a0804d2b5465c8d83cf3bc8babc67",
              "IPY_MODEL_cda1cd1a5a7c4b7eb7314ecaf0565756",
              "IPY_MODEL_cf29f9b4b2cf4cb287eeda8439616589"
            ],
            "layout": "IPY_MODEL_9d8a6f7430e64e939bd8cfd8af7be03f"
          }
        },
        "362a0804d2b5465c8d83cf3bc8babc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7cb72966b204a64a548d692f9f7f33d",
            "placeholder": "​",
            "style": "IPY_MODEL_afec6a9f0d394c35aad0aadf45674145",
            "value": "config.json: 100%"
          }
        },
        "cda1cd1a5a7c4b7eb7314ecaf0565756": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02e1a4abeb6d4f0ca3fc6e0de096dc28",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f57cbb9c096f4c0f9aa47077554a8982",
            "value": 190
          }
        },
        "cf29f9b4b2cf4cb287eeda8439616589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac052000901e40398c14763820180cba",
            "placeholder": "​",
            "style": "IPY_MODEL_fe7cdb2ca855468ca161c78dff38b971",
            "value": " 190/190 [00:00&lt;00:00, 17.5kB/s]"
          }
        },
        "9d8a6f7430e64e939bd8cfd8af7be03f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7cb72966b204a64a548d692f9f7f33d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afec6a9f0d394c35aad0aadf45674145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02e1a4abeb6d4f0ca3fc6e0de096dc28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f57cbb9c096f4c0f9aa47077554a8982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac052000901e40398c14763820180cba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe7cdb2ca855468ca161c78dff38b971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "681b1073b94c47a6bfee4235f4aef9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0759076db1444aa08e136a77385369c3",
              "IPY_MODEL_ee0cd6df65ea45da8a0138ed3f4aeb88",
              "IPY_MODEL_a680fd6bc43f4d8ea91e529502893751"
            ],
            "layout": "IPY_MODEL_51e29ab7246748a99d2e80bee156930a"
          }
        },
        "0759076db1444aa08e136a77385369c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ea3ffc9ebc4f4ca24adcca0f38ded5",
            "placeholder": "​",
            "style": "IPY_MODEL_eb4de251da7d4e4b8b5953abc1eef71a",
            "value": "100%"
          }
        },
        "ee0cd6df65ea45da8a0138ed3f4aeb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55de6a1b865044098bfb08768f8dc49e",
            "max": 17,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ebe3627e1c1415ba4151fbea5627930",
            "value": 17
          }
        },
        "a680fd6bc43f4d8ea91e529502893751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d353bcb98aea43ffa0f5c38842f94003",
            "placeholder": "​",
            "style": "IPY_MODEL_5c6d91a4d00c432eb448adb07fa0beb5",
            "value": " 17/17 [00:01&lt;00:00, 20.15it/s]"
          }
        },
        "51e29ab7246748a99d2e80bee156930a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ea3ffc9ebc4f4ca24adcca0f38ded5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb4de251da7d4e4b8b5953abc1eef71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55de6a1b865044098bfb08768f8dc49e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ebe3627e1c1415ba4151fbea5627930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d353bcb98aea43ffa0f5c38842f94003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6d91a4d00c432eb448adb07fa0beb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What is RAG?\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Each step can be roughly broken down to:\n",
        "\n",
        "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
        "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
        "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt."
      ],
      "metadata": {
        "id": "zDnl3s7f6s5U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "eDehAxjj6s5X"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why RAG?\n",
        "\n",
        "The main goal of RAG is to improve the generation outptus of LLMs.\n",
        "\n",
        "Two primary improvements can be seen as:\n",
        "1. **Preventing hallucinations** - LLMs are incredible but they are prone to potential hallucination, as in, generating something that *looks* correct but isn't. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn't seem correct, because of retrieval, you also have access to the sources where it came from.\n",
        "2. **Work with custom data** - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.\n",
        "\n",
        "\n",
        "RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data.\n"
      ],
      "metadata": {
        "id": "WY_FYTr86s5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## What kind of problems can RAG be used for?\n",
        "\n",
        "RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n",
        "\n",
        "For example you could use RAG for:\n",
        "* **Customer support Q&A chat** - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\".\n",
        "* **Email chain analysis** - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n",
        "* **Company internal documentation chat** - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n",
        "* **Textbook Q&A** - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n",
        "\n",
        "All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "mCvh_Laa6s5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll write the code to:\n",
        "1. Open a PDF document (you could use almost any PDF here).\n",
        "2. Format the text of the PDF textbook ready for an embedding model (this process is known as text splitting/chunking).\n",
        "3. Embed all of the chunks of text in the textbook and turn them into numerical representation which we can store for later.\n",
        "4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n",
        "5. Create a prompt that incorporates the retrieved pieces of text.\n",
        "6. Generate an answer to a query based on passages from the textbook.\n"
      ],
      "metadata": {
        "id": "SL23Wb9Q6s5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Document/Text Processing and Embedding Creation\n",
        "\n",
        "Ingredients:\n",
        "* PDF document of choice.\n",
        "* Embedding model of choice.\n",
        "\n",
        "Steps:\n",
        "1. Import PDF document.\n",
        "2. Process text for embedding (e.g. split into chunks of sentences).\n",
        "3. Embed text chunks with embedding model.\n",
        "4. Save embeddings to file for later use"
      ],
      "metadata": {
        "id": "sYjgldvM6s5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Get PDF document path\n",
        "pdf_path = \"hehe.pdf\"\n",
        "\n",
        "# Download PDF\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(\"[INFO] File doesn't exist, downloading...\")\n",
        "\n",
        "    # Enter the URL of the PDF\n",
        "    url = \"https://gcatnjust.github.io/ChenGong/paper/wei_tnnls19.pdf\"\n",
        "\n",
        "    # The local filename to save the downloaded file\n",
        "    filename = pdf_path\n",
        "\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Open the file and save it\n",
        "        with open(filename, \"wb\") as file:\n",
        "            file.write(response.content)\n",
        "        print(f\"[INFO] The file has been download and saved as {filename}\")\n",
        "    else:\n",
        "        print(f\"[INFO] Failed to download the file. Status code: {reponse.status_code}\")\n",
        "\n",
        "else:\n",
        "    print(f\"File {pdf_path} exists.\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:20:34.969296Z",
          "iopub.execute_input": "2024-10-17T17:20:34.969706Z",
          "iopub.status.idle": "2024-10-17T17:20:35.236635Z",
          "shell.execute_reply.started": "2024-10-17T17:20:34.969665Z",
          "shell.execute_reply": "2024-10-17T17:20:35.235742Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cezaHRBt6s5Z",
        "outputId": "8c7ec0ec-fb4d-478e-d2a6-8e1b6e0cb011"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] File doesn't exist, downloading...\n",
            "[INFO] The file has been download and saved as hehe.pdf\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:20:35.705392Z",
          "iopub.execute_input": "2024-10-17T17:20:35.705763Z",
          "iopub.status.idle": "2024-10-17T17:21:03.100244Z",
          "shell.execute_reply.started": "2024-10-17T17:20:35.705727Z",
          "shell.execute_reply": "2024-10-17T17:21:03.099126Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRWc1scR6s5Z",
        "outputId": "9cb4b096-a186-4099-e844-3bd78a754583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Text preprocessing"
      ],
      "metadata": {
        "id": "B6IJ9mM56s5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz #for opening document\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Performs minor formatting on text.\"\"\"\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
        "    return cleaned_text\n",
        "\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"Opens a PDF file, reads its text content page by page, and collects statistics.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    pages_and_texts = []\n",
        "    for page_number, page in tqdm(enumerate(doc)):\n",
        "        text = page.get_text()\n",
        "        text = text_formatter(text=text)\n",
        "        pages_and_texts.append({\"page_number\": page_number - 0, # adjusted page numbers since our PDF starts on page 42\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\", \")),\n",
        "                                \"page_token_count\": len(text) / 4, #1 token has approx 4 characters\n",
        "                                \"text\": text})\n",
        "    return pages_and_texts\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.102655Z",
          "iopub.execute_input": "2024-10-17T17:21:03.103432Z",
          "iopub.status.idle": "2024-10-17T17:21:03.515924Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.10338Z",
          "shell.execute_reply": "2024-10-17T17:21:03.514993Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "10ff596a43f94c1482213cad2f88f6de",
            "72d9daf6f8a7472cb83dde24739a7ad5",
            "3d21612ed6154500a2cf8c08b81fe855",
            "deaa884184eb4fd5aeb88cddf4b3f7f3",
            "aed94c2db0a045559066b210603a6c14",
            "43edfcb9e06c4bf6bec0fd30e7a1da7c",
            "318ef2631c3c4490a833af1495ba8cd1",
            "46a4c3e1960b427b96fdb886d0ba6eee",
            "d1d973c81c5341e78c04b7c63943b8c2",
            "9e66139b8cf549f080c17abcde1381eb",
            "066be66235564b7d88e24000a5af671c"
          ]
        },
        "collapsed": true,
        "id": "SN9S7tbK6s5a",
        "outputId": "450cb12e-a3d2-40ef-e88a-de237574c91c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10ff596a43f94c1482213cad2f88f6de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 0,\n",
              "  'page_char_count': 7049,\n",
              "  'page_word_count': 1036,\n",
              "  'page_sentence_count_raw': 88,\n",
              "  'page_token_count': 1762.25,\n",
              "  'text': 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classiﬁcation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract—Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled. Such noisy labels will mislead the classiﬁer training and severely decrease the classiﬁcation performance. Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization. However, they are only suitable for binary classiﬁcation and also require strong prior knowledge. Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem. We denote our proposed method as “label noise handling via side information” (LNSI). Speciﬁcally, the observed label matrix is decomposed as the sum of two parts, in which the ﬁrst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix. The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufﬁciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019. This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the “Summit of the Six Top Talents” Program under Grant DZXX-027, in part by the “Young Elite Scientists Sponsorship Program” by Jiangsu Province, in part by the “Young Elite Scientists Sponsorship Program” by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the “111” Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.) Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn). S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn). T. Liu and D. Tao are with the UBTECH Sydney Artiﬁcial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au). Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org. Digital Object Identiﬁer 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems. Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classiﬁcation error of LNSI is upper bounded. The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets conﬁrm the superiority of LNSI to state-of-the-art approaches on label noise handling. Index Terms—Classiﬁcation, generalization bound, label noise, matrix recovery, side information. I. INTRODUCTION T RADITIONALLY, a reliable supervised classiﬁer, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the sufﬁ- cient correctly labeled data. Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1]. For instance, as the numerous examples in many applications (e.g., image classiﬁcation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue. Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors. These noisy labels will signiﬁcantly mislead the classiﬁer training and then severely decrease the classiﬁcation performance [2]. Hence, designing algorithms that account for the data with noisy labels is of great signiﬁcance and has become a critical issue in the machine learning community. Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]–[6]. For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part. Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results. However, they are only applicable to binary classiﬁcation and the extension to multi-class is nontrivial [7]. Moreover, these methods require the estimation of class prior, which is actually quite difﬁcult in the presence of corrupted observed data. On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.'},\n",
              " {'page_number': 1,\n",
              "  'page_char_count': 6177,\n",
              "  'page_word_count': 969,\n",
              "  'page_sentence_count_raw': 39,\n",
              "  'page_token_count': 1544.25,\n",
              "  'text': 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 1. Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a). By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z∗and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label. task, which has been widely used in many machine learning ﬁelds such as clustering [8] and multi-label learning [9]. For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and ﬁrst introduce the side information to aid the clustering process. Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning. From the review, we know that the side information has not been utilized for removing noisy labels. Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information. Speciﬁcally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]–[11]. Therefore, our proposed method is named as “label noise handling via side information” (LNSI). The paradigm of this article is shown in Fig. 1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information. Fig. 1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted. As illustrated in Fig. 1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z∗on the example features (i.e., T = X Z∗), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classiﬁcation, which does not need the one-versus-one or one-versus-the-rest operations; 2) sufﬁcient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classiﬁer parameter optimization into a uniﬁed framework. Due to the above merits, a reliable classiﬁer can be learned to accurately classify the unseen test examples with different levels of training label noise. Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classiﬁer. II. RELATED WORK This section brieﬂy reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI. A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance. One straightforward idea to address this problem is to improve the quality of training data. Since the training data are associated with noisy labels, the early-stage approaches ﬁrst detect and eliminate label noise and then conduct the standard supervised classiﬁcation algorithm. To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble ﬁl- ters [14]. Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use. To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels. Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise. Conse- quently, various surrogate loss functions can be designed. Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the inﬂuence caused by label noise. Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise. Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classiﬁcation and lack the theoretical guarantee of exact recovery on accurate labels. Recently, some methods try to extend deep learning models to the case of noisy labels. For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data. Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise. Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels. Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling. However, these deep learning-based methods are only suitable for speciﬁc tasks related to image analysis or natural language processing [17]. Moreover, the perfor- mance of these methods generally lacks theoretical guarantees. Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.sample(pages_and_texts, k=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.516902Z",
          "iopub.execute_input": "2024-10-17T17:21:03.517173Z",
          "iopub.status.idle": "2024-10-17T17:21:03.524072Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.517144Z",
          "shell.execute_reply": "2024-10-17T17:21:03.523138Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C7bEvRAS6s5a",
        "outputId": "239ae7e7-0891-4956-a9dd-bc60f079905b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 6,\n",
              "  'page_char_count': 4175,\n",
              "  'page_word_count': 857,\n",
              "  'page_sentence_count_raw': 45,\n",
              "  'page_token_count': 1043.75,\n",
              "  'text': 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. WEI et al.: HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 7 Lemma 6 [40]: The function F : Rd×c →R deﬁned as F(W) = (1/2)∥W∥2 2,2 is (1/2)-strongly convex with respect to ∥· ∥2,2 over Rd×c, where ∥· ∥2,2 := ∥· ∥F. By combining Lemmas 5 and 6 with the bound given in Lemma 4, we obtain the following two corollaries. Corollary 7: Let W = {W : ∥W∥2,1 ≤W2,1} and A = {A ∈Rn×c : ∥A∥2,∞≤A2,∞}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 2,q for q = (ln(c)/(ln(c) −1)) is bounded as Eσ \\x13 sup f ∈F 1 nr nr \\x02 α=1 σαtr(W⊤A(α)) \\x14 ≤W2,1A2,∞ \\x12 3 ln(c) nr (23) with the fact that the dual norm of ℓ2,1 is ℓ2,∞. Corollary 8: Let W = {W : ∥W∥F ≤WF} and A = {A ∈ Rd×c : ∥A∥F ≤AF}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 2,2 is bounded as Eσ \\x13 sup f ∈F 1 nr nr \\x02 α=1 σαtr(W⊤A(α)) \\x14 ≤WFAF \\x12 2 nr (24) with the fact that the dual norm of the Frobenius norm is the Frobenius norm. Lemma 9 [10]: Let W = {W : ∥W∥∗≤W∗} and A = {A ∈Rd×c : ∥A∥2 ≤A2}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 ∗is bounded as Eσ \\x13 sup f ∈F 1 nr nr \\x02 α=1 σαtr(W⊤A(α)) \\x14 ≤W∗A2 \\x12 ln(2dc) nr (25) with the fact that the dual norm of the nuclear norm is the spectral norm and dc = max(d, c). Lemma 10: Let Am1 ∈Rm1×n1 and Am2 ∈Rn1×m2 be two matrices, and then the following inequality holds: λmin \\x03 A⊤ m1 Am1 \\x04 ∥Am2∥2 F ≤∥Am1 Am2∥2 F (26) where λmin(·) represents the smallest eigenvalue of the matrix inside the bracket. Proof: For a given complex Hermitian matrix Mr and nonzero vector xr, the Rayleigh quotient R(Mr, xr) [42], [43] is deﬁned as R(Mr, xr) = x∗ r Mr xr x∗r xr (27) and the following inequality holds: λmin(Mr) ≤R(Mr, xr) ≤λmax(Mr). (28) Let a = Vec(Am2), where Vec(·) is an operator converting a matrix into a vector, and then, ∥Am1 Am2∥2 F can be rewritten in the form of ℓ2-norm as ∥Am1 Am2∥2 F = ∥(I ⊗Am1)a∥2 2 = a⊤(I ⊗Am1)⊤(I ⊗Am1)a = a⊤\\x03 I ⊗A⊤ m1 \\x04 (I ⊗Am1)a = a⊤\\x03 I ⊗A⊤ m1 Am1 \\x04 a (29) where ⊗represents the Kronecker product [44] and I is an identity matrix with proper size. Combining (28) with (29), the following inequality holds, namely: λmin \\x03 A⊤ m1 Am1 \\x04 ∥Am2∥2 F = λmin \\x03 I ⊗A⊤ m1 Am1 \\x04 ∥Am2∥2 F = λmin \\x03 I ⊗A⊤ m1 Am1 \\x04 (a⊤a) ≤a⊤\\x03 I ⊗A⊤ m1 Am1 \\x04 a = ∥Am1 Am2∥2 F (30) which completes the proof. □ Provided Lemma 10, tr((X Z)⊤L(X Z)) ≤Ztr in (21) can be derived into a constraint in the form of Frobenius norm on Z. Speciﬁcally, note that the Laplacian matrix is positive semideﬁne and its SVD is L = U L\\x02LU⊤ L = U L\\x02(1/2) L \\x02(1/2) L U⊤ L , so we have tr((X Z)⊤L(X Z)) = tr \\x03 (X Z)⊤U L\\x02 1 2 L\\x02 1 2 LU⊤ L (X Z) \\x04 = tr \\x0f\\x0f \\x02 1 2 LU⊤ L X Z \\x10⊤\\x0f \\x02 1 2 LU⊤ L X Z \\x10\\x10 = \\x07\\x07\\x07\\x02 1 2 LU⊤ L X Z \\x07\\x07\\x07 2 F ≥λmin(X⊤LX)∥Z∥2 F. (31) If λmintr = λmin(X⊤LX) > 0, we have ∥Z∥F ≤Zt, where Zt = (Ztr/λmintr)1/2. In addition, we manage to rewrite the constraint X Z ∈ [−1, 1]n×c as the form of Frobenius norm on Z. Since X Z ∈ [−1, 1]n×c and ∥X Z∥2 F ≤nc, then similar to (31), we may obtain that ∥Z∥F ≤Zb if λminb = λmin(X⊤X) > 0, where Zb = (nc/λminb)1/2. Taking the three different Frobenius norm-based constraints (i.e., ∥Z∥F ≤√ZF, ∥Z∥F ≤Zt, ∥Z∥F ≤Zb) on the matrix Z into account, and let Zmin = min{√ZF, Zb, Zt}, we have ∥Z∥F ≤ \\x15√ZF, if λmintr ≤0 or λminb ≤0 Zmin, otherwise. (32) For convenience, we denote the upper bound of ∥Z∥F as Z that can be √ZF or Zmin according to different conditions in (32). Herein, we begin to formally derive the generalization error of LNSI. Theorem 11: Let X2 = maxi ∥Xi∥2 and XF = maxi ∥Xi∥F, and then the model complexity of function class F\\r is upper bounded by Rn(F\\r) ≤min \\x15 Z∗X2 \\x16 ln(2dc) nc , ZXF \\x16 2 nc \\x17 + E2,1 \\x16 3 ln(c) nc . (33) Proof: First, the Rademacher complexity of linear function class F\\r in our case can be written as R(F\\r) := Eσ \\x13 sup θ∈\\r 1 nc nc \\x02 α=1 σα \\x03 Xiα ZI jα + Eiα, jα \\x04 \\x14 . (34) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.'},\n",
              " {'page_number': 3,\n",
              "  'page_char_count': 4901,\n",
              "  'page_word_count': 927,\n",
              "  'page_sentence_count_raw': 65,\n",
              "  'page_token_count': 1225.25,\n",
              "  'text': 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in which ∥Z∥∗with nuclear norm is employed to achieve low-rank effect, ∥E∥2,1 with ℓ2,1 norm is utilized to realize row sparsity, and λ1 and λ3 are the nonnegative tradeoff parameters. By solving (2), we can obtain the optimal Z∗, which can be used to compute the label y ∈Rc of a test example x ∈Rd as y = Z∗⊤x. Then x is classiﬁed into the jth class if j = arg max j′=1,2,...,c y j′ with y j′ being the j′th element in the label vector y. It is worth pointing out that although the formulation of (2) is similar to the low-rank representation (LRR) proposed in [27], their usages and implications are quite different. First, LRR is developed for subspace clustering while our method is for label noise removal. Second, LRR aims to select sparse atoms from a predeﬁned dictionary to reconstruct a clean space, while our method tries to learn a proper mapping Z from feature space to label space in presence of the label noise encoded by E. Therefore, these two models are different although they look similar at ﬁrst glance. To sufﬁciently exploit the side information, we further use the Laplacian regularizer based on graph embedding. Graph Laplacian has been widely utilized in several machine learning tasks such as semisupervised learning [28], [29], spectral clustering [30], multi-task learning [31], and metric learning [32]. However, to the best of our knowledge, it has not been used to deal with side information. Let G = {V, E} be an undirected weighted graph with vertex set V consist- ing of all n examples and E is the edge set encoding the similarity between these examples. The symmetric adjacency matrix ˆW ∈Rn×n is utilized to quantify the graph G, where ˆWij = exp(−(∥Xi −X j∥2/2σ 2 k )) [33] (σk is the kernel width) measures the similarity between examples Xi and X j (i, j = 1, 2, . . . , n). The diagonal matrix D and the Laplacian matrix L of the graph G are, respectively, deﬁned as Dii = \\x02 j ˆWij L = D −ˆW. (3) Ideally, we hope that the similar examples revealed by G obtain similar clean labels, and the labels of dissimilar examples can be quite different. Therefore, we have the Laplacian regularizer that is derived as \\x02 i \\x02 j ˆWij ∥Xi Z −X j Z∥2 2 = tr((X Z)⊤L(X Z)) (4) in which “tr(·)” computes the trace of the corresponding matrix. By combining (2) and (4), the proposed LNSI model is ﬁnally formulated as min Z,E ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X Z)⊤L(X Z))+λ3∥E∥2,1 s.t. Y = X Z + E, X Z ∈{−1, 1}n×c (5) in which λ1, λ2, and λ3 are the nonnegative tradeoff parame- ters. We note that (5) falls into an integer programming problem, which is generally NP-hard. To make problem (5) tractable, we relax the discrete constraint X Z ∈{−1, 1}n×c to a contin- uous convex set X Z ∈[−1, 1]n×c. It is a linear programming relaxation, which has been used in several prior works [34], [35]. By doing so, we pursue to solve a simpler problem min Z,E ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X Z)⊤L(X Z))+λ3∥E∥2,1 s.t. Y = X Z + E, X Z ∈[−1, 1]n×c. (6) B. Optimization Directly solving the problem (6) is difﬁcult due to the exis- tence of coupled variables, which will make its optimization not have a closed-form solution. Consequently, we introduce two auxiliary variables J and B, and then, the problem (6) is converted to the following equivalent version: min Z,E,J,B ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 s.t. Y = B + E, B = X J, Z = J, B ∈[−1, 1]n×c. (7) The optimization problem (7) is convex and many off-the-shelf methods can be adopted to solve it. For efﬁciency, here we use the alternating direction method of multipliers (ADMMs), which alternatively optimizes the related variables in an iterative manner. The augmented Lagrangian function of (7) with the continuous convex constraint can be written as L (Z, E, B, J, M1, M2, M3) = ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 +tr \\x03 M⊤ 1 (Y −B −E) \\x04 +tr \\x03 M⊤ 2 (B −X J) \\x04 +tr \\x03 M⊤ 3 (Z −J) \\x04 + μ 2 (∥Y −B −E∥2 F +∥B −X J∥2 F + ∥Z −J∥2 F) (8) where M1, M2, and M3 are the Lagrangian multipliers, and μ > 0 is the penalty coefﬁcient. We can sequentially minimize each of the variables Z, E, B, and J by ﬁxing the others in every iteration. Update Z: The subproblem related to Z is min Z ∥Z∥∗+ λ1∥Z∥2 F + tr\\x03M⊤ 3 (Z −J)\\x04 + μ 2 ∥Z −J∥2 F ⇒min Z ∥Z∥∗+ λ1∥Z∥2 F + tr \\x03 M⊤ 3 Z −M⊤ 3 J \\x04 + μ 2 tr((Z⊤−J⊤)(Z −J)) ⇒min Z ∥Z∥∗+ λ1tr(Z⊤Z) + tr \\x03 M⊤ 3 Z \\x04 + μ 2 tr(Z⊤Z −2J⊤Z) ⇒min Z ∥Z∥∗+ 2λ1 + μ 2 × tr \\x05 Z⊤Z − 2 2λ1 + μ(μJ −M3)⊤Z \\x06 ⇒min Z 1 2λ1 + μ∥Z∥∗+ 1 2 \\x07\\x07\\x07\\x07Z − 1 μ + 2λ1 (μJ −M3) \\x07\\x07\\x07\\x07 2 F ⇒min Z τ∥Z∥∗+ 1 2∥Z −ˆT∥2 F (9) where ˆT = (1/(μ + 2λ1))(μJ −M3) and τ = (1/(2λ1 + μ)). Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Making dataframe\n",
        "\n",
        "Let's perform a rough exploratory data analysis (EDA) to get an idea of the size of the texts (e.g. character counts, word counts etc) we're working with.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t7r3oYUM6s5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.526153Z",
          "iopub.execute_input": "2024-10-17T17:21:03.526468Z",
          "iopub.status.idle": "2024-10-17T17:21:03.857612Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.526429Z",
          "shell.execute_reply": "2024-10-17T17:21:03.856439Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "T5BngErx6s5b",
        "outputId": "86bc3955-dd1a-4e44-e439-72ce274a99e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "0            0             7049             1036                       88   \n",
              "1            1             6177              969                       39   \n",
              "2            2             6151             1044                       56   \n",
              "3            3             4901              927                       65   \n",
              "4            4             4164              833                       44   \n",
              "\n",
              "   page_token_count                                               text  \n",
              "0           1762.25  This article has been accepted for inclusion i...  \n",
              "1           1544.25  This article has been accepted for inclusion i...  \n",
              "2           1537.75  This article has been accepted for inclusion i...  \n",
              "3           1225.25  This article has been accepted for inclusion i...  \n",
              "4           1041.00  This article has been accepted for inclusion i...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4fa8780-10bf-4cd7-b469-2b960bf2329e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7049</td>\n",
              "      <td>1036</td>\n",
              "      <td>88</td>\n",
              "      <td>1762.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6177</td>\n",
              "      <td>969</td>\n",
              "      <td>39</td>\n",
              "      <td>1544.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6151</td>\n",
              "      <td>1044</td>\n",
              "      <td>56</td>\n",
              "      <td>1537.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4901</td>\n",
              "      <td>927</td>\n",
              "      <td>65</td>\n",
              "      <td>1225.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4164</td>\n",
              "      <td>833</td>\n",
              "      <td>44</td>\n",
              "      <td>1041.00</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4fa8780-10bf-4cd7-b469-2b960bf2329e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4fa8780-10bf-4cd7-b469-2b960bf2329e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4fa8780-10bf-4cd7-b469-2b960bf2329e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-943fdfbf-1710-4821-9625-3542f9e04cdf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-943fdfbf-1710-4821-9625-3542f9e04cdf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-943fdfbf-1710-4821-9625-3542f9e04cdf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1312,\n        \"min\": 3377,\n        \"max\": 7785,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          3377,\n          4223,\n          7049\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 186,\n        \"min\": 538,\n        \"max\": 1298,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          538,\n          688,\n          1036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 53,\n        \"min\": 31,\n        \"max\": 240,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          31,\n          46,\n          88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 328.2180713808312,\n        \"min\": 844.25,\n        \"max\": 1946.25,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          844.25,\n          1055.75,\n          1762.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 3. Experimental results of the compared methods on \\ufb01ve UCI benchmark data sets. (a)\\u2013(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively. TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET. THE CLASSIFICATION ACCURACIES (MEAN \\u00b1 STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. \\u2022/\\u25e6INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL). THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET. THE CLASSIFICATION ACCURACIES (MEAN \\u00b1 STD.) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. \\u2022/\\u25e6INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL). THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise. D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203. COIL20 is a popular public data set for object classi\\ufb01cation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles. The resolution of each gray-level image is 32 \\u00d7 32 [46]. We use the output of the \\ufb01rst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions. Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data. Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label. In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with \\u03c3k = 0.1. In addition, the tradeoff parameters in (6) such as \\u03bb1, \\u03bb2, and \\u03bb3 were tuned by searching the grid {10\\u22122, 10\\u22121, . . . , 103} in order to obtain the satisfactory results. The classi\\ufb01cation accuracies of all compared methods under different label noise levels are presented in Table III. It can be observed that the performances of all methods decrease with the increase in noise level. However, LNSI achieves the best results in most cases when compared with other baseline methods. Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%. E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition. Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes. The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 4. Illustration of convergence process of the ADMM method adopted by LNSI on the four practical data sets. For each data set, we present the convergence curves under different convergence criteria in the Algorithm 1. (a)\\u2013(c) ISOLET data set. (d)\\u2013(f) COIL20 data set. (g)\\u2013(i) MNIST data set. (j)\\u2013(l) CIFAR-10 data set. tr((X Z)\\u22a4L(X Z)). To this end, we study the performances of three different settings on the above four practical data sets (such as ISOLET, COIL20, MNIST, and CIFAR-10). First, both low-rank regularizer and Laplacian regularizer are reserved to constitute the original model (abbreviated as \\u201cLNSI\\u201d); second, the Laplacian regularizer is removed from the original model to see how this term in\\ufb02uences the model performance (abbreviated as \\u201cNo Laplacian\\u201d); third, we remove the low-rank regularizer while keeping the Lapla- cian regularizer to observe the effect of low-rank regularizer (abbreviated as \\u201cNo Low-Rank\\u201d). The experimental results of these three models are illus- trated in Fig. 5, in which 40% and 60% of training examples have incorrect labels. The results reveal that LNSI achieves the best performance on all four data sets, especially when the label noise rate is relatively high. By contrast, the accuracy of LNSI will drop without any of the two terms such as low-rank regularizer and Laplacian regularizer, and therefore, these two regularization terms are essential to boost the performance of LNSI. I. Parametric Sensitivity Note that the objective function (8) in LNSI contains three tradeoff parameters \\u03bb1, \\u03bb2, and \\u03bb3 that should be manually Fig. 5. Results of ablation study on four practical data sets. For convenience, the original model is denoted as \\u201cLNSI,\\u201d the setting without the Laplacian regularization term is dubbed as \\u201cNo Laplacian,\\u201d and the setting without the low-rank term is named as \\u201cNo Low-Rank.\\u201d (a) ISOLET data set. (b) COIL20 Data set. (c) MNIST data set. (d) CIFAR-10 data set. tuned. Therefore, in this section, we discuss whether the choices of them will signi\\ufb01cantly in\\ufb02uence the performance of LNSI. To this end, we examine the classi\\ufb01cation accuracy at two different levels (20% and 60%) of label noise via changing one of \\u03bb1, \\u03bb2, and \\u03bb3, and meanwhile \\ufb01xing the others to the optimal constant values under different data sets and different noise rates. The above-mentioned four practical data sets are adopted here, and the results are shown in Fig. 6. Fig. 6(a)\\u2013(c) shows the experiments on the ISOLET data set, (d)\\u2013(f) shows the experiments on the COIL20 data set, (g)\\u2013(i) shows the experiments on the MNIST data set, and (j)\\u2013(l) shows the experiments on the CIFAR-10 data set. The results reveal that LNSI is robust to the variations of \\u03bb1 and \\u03bb2 in a wide range, so they can be easily tuned for practical use. Meanwhile, the performance of LNSI varies when \\u03bb3 changes in a wide range, as \\u03bb3 controls the capability of our method for capturing the label noise via the error matrix E. Speci\\ufb01cally, if \\u03bb3 is large, the label noise will be greatly ignored, leading to the performance degradation of LNSI on COIL20 and MNIST when the noise rate is 60%. J. Summary of Experiments Based on the above-mentioned experimental results of Sections VI-B\\u2013VI-I, we observe that: 1) LNSI performs bet- ter than other baseline algorithms in most cases, both on benchmark data sets and practical data sets; 2) the proposed algorithm in Algorithm 1 converges quickly to a stationary point; 3) LNSI is robust to the variation of the two tradeoff parameters including \\u03bb1 and \\u03bb2; 4) when the label noise is serious, the performance of LNSI might drop when \\u03bb3 is set to a large value; 5) the introduced low-rank regularizer and Laplacian regularizer are both bene\\ufb01cial to improve the classi\\ufb01cation performance; and 6) the proposed LNSI outper- forms other compared baselines when 40% and 60% labels Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classi\\ufb01cation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract\\u2014Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled. Such noisy labels will mislead the classi\\ufb01er training and severely decrease the classi\\ufb01cation performance. Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization. However, they are only suitable for binary classi\\ufb01cation and also require strong prior knowledge. Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem. We denote our proposed method as \\u201clabel noise handling via side information\\u201d (LNSI). Speci\\ufb01cally, the observed label matrix is decomposed as the sum of two parts, in which the \\ufb01rst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix. The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been suf\\ufb01ciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019. This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the \\u201cSummit of the Six Top Talents\\u201d Program under Grant DZXX-027, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by Jiangsu Province, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the \\u201c111\\u201d Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.) Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi\\u2019an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn). S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn). T. Liu and D. Tao are with the UBTECH Sydney Arti\\ufb01cial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au). Color versions of one or more of the \\ufb01gures in this article are available online at http://ieeexplore.ieee.org. Digital Object Identi\\ufb01er 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems. Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classi\\ufb01cation error of LNSI is upper bounded. The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets con\\ufb01rm the superiority of LNSI to state-of-the-art approaches on label noise handling. Index Terms\\u2014Classi\\ufb01cation, generalization bound, label noise, matrix recovery, side information. I. INTRODUCTION T RADITIONALLY, a reliable supervised classi\\ufb01er, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the suf\\ufb01- cient correctly labeled data. Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1]. For instance, as the numerous examples in many applications (e.g., image classi\\ufb01cation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue. Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors. These noisy labels will signi\\ufb01cantly mislead the classi\\ufb01er training and then severely decrease the classi\\ufb01cation performance [2]. Hence, designing algorithms that account for the data with noisy labels is of great signi\\ufb01cance and has become a critical issue in the machine learning community. Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]\\u2013[6]. For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part. Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results. However, they are only applicable to binary classi\\ufb01cation and the extension to multi-class is nontrivial [7]. Moreover, these methods require the estimation of class prior, which is actually quite dif\\ufb01cult in the presence of corrupted observed data. On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X \\u00a9 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.859031Z",
          "iopub.execute_input": "2024-10-17T17:21:03.859478Z",
          "iopub.status.idle": "2024-10-17T17:21:03.87162Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.859441Z",
          "shell.execute_reply": "2024-10-17T17:21:03.87059Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_v_lfE506s5b",
        "outputId": "9522e9f0-c4ea-49a6-8b43-57ef98fa99f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "10           10             4482              736                       37   \n",
              "11           11             4223              688                       37   \n",
              "12           12             3635              655                       46   \n",
              "13           13             7785             1298                      240   \n",
              "14           14             6466              983                      135   \n",
              "\n",
              "    page_token_count                                               text  \n",
              "10           1120.50  This article has been accepted for inclusion i...  \n",
              "11           1055.75  This article has been accepted for inclusion i...  \n",
              "12            908.75  This article has been accepted for inclusion i...  \n",
              "13           1946.25  This article has been accepted for inclusion i...  \n",
              "14           1616.50  This article has been accepted for inclusion i...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3c44e8c-9f1e-444b-9d97-e9678c10b9d0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>4482</td>\n",
              "      <td>736</td>\n",
              "      <td>37</td>\n",
              "      <td>1120.50</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>4223</td>\n",
              "      <td>688</td>\n",
              "      <td>37</td>\n",
              "      <td>1055.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>3635</td>\n",
              "      <td>655</td>\n",
              "      <td>46</td>\n",
              "      <td>908.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>7785</td>\n",
              "      <td>1298</td>\n",
              "      <td>240</td>\n",
              "      <td>1946.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>6466</td>\n",
              "      <td>983</td>\n",
              "      <td>135</td>\n",
              "      <td>1616.50</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3c44e8c-9f1e-444b-9d97-e9678c10b9d0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b3c44e8c-9f1e-444b-9d97-e9678c10b9d0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b3c44e8c-9f1e-444b-9d97-e9678c10b9d0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-716a748f-42a2-40ab-b726-a6ff14f53f6d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-716a748f-42a2-40ab-b726-a6ff14f53f6d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-716a748f-42a2-40ab-b726-a6ff14f53f6d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 10,\n        \"max\": 14,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          11,\n          14,\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1741,\n        \"min\": 3635,\n        \"max\": 7785,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4223,\n          6466,\n          3635\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 270,\n        \"min\": 655,\n        \"max\": 1298,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          688,\n          983,\n          655\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 88,\n        \"min\": 37,\n        \"max\": 240,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          46,\n          135,\n          37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 435.4308713791433,\n        \"min\": 908.75,\n        \"max\": 1946.25,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1055.75,\n          1616.5,\n          908.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 4. Illustration of convergence process of the ADMM method adopted by LNSI on the four practical data sets. For each data set, we present the convergence curves under different convergence criteria in the Algorithm 1. (a)\\u2013(c) ISOLET data set. (d)\\u2013(f) COIL20 data set. (g)\\u2013(i) MNIST data set. (j)\\u2013(l) CIFAR-10 data set. tr((X Z)\\u22a4L(X Z)). To this end, we study the performances of three different settings on the above four practical data sets (such as ISOLET, COIL20, MNIST, and CIFAR-10). First, both low-rank regularizer and Laplacian regularizer are reserved to constitute the original model (abbreviated as \\u201cLNSI\\u201d); second, the Laplacian regularizer is removed from the original model to see how this term in\\ufb02uences the model performance (abbreviated as \\u201cNo Laplacian\\u201d); third, we remove the low-rank regularizer while keeping the Lapla- cian regularizer to observe the effect of low-rank regularizer (abbreviated as \\u201cNo Low-Rank\\u201d). The experimental results of these three models are illus- trated in Fig. 5, in which 40% and 60% of training examples have incorrect labels. The results reveal that LNSI achieves the best performance on all four data sets, especially when the label noise rate is relatively high. By contrast, the accuracy of LNSI will drop without any of the two terms such as low-rank regularizer and Laplacian regularizer, and therefore, these two regularization terms are essential to boost the performance of LNSI. I. Parametric Sensitivity Note that the objective function (8) in LNSI contains three tradeoff parameters \\u03bb1, \\u03bb2, and \\u03bb3 that should be manually Fig. 5. Results of ablation study on four practical data sets. For convenience, the original model is denoted as \\u201cLNSI,\\u201d the setting without the Laplacian regularization term is dubbed as \\u201cNo Laplacian,\\u201d and the setting without the low-rank term is named as \\u201cNo Low-Rank.\\u201d (a) ISOLET data set. (b) COIL20 Data set. (c) MNIST data set. (d) CIFAR-10 data set. tuned. Therefore, in this section, we discuss whether the choices of them will signi\\ufb01cantly in\\ufb02uence the performance of LNSI. To this end, we examine the classi\\ufb01cation accuracy at two different levels (20% and 60%) of label noise via changing one of \\u03bb1, \\u03bb2, and \\u03bb3, and meanwhile \\ufb01xing the others to the optimal constant values under different data sets and different noise rates. The above-mentioned four practical data sets are adopted here, and the results are shown in Fig. 6. Fig. 6(a)\\u2013(c) shows the experiments on the ISOLET data set, (d)\\u2013(f) shows the experiments on the COIL20 data set, (g)\\u2013(i) shows the experiments on the MNIST data set, and (j)\\u2013(l) shows the experiments on the CIFAR-10 data set. The results reveal that LNSI is robust to the variations of \\u03bb1 and \\u03bb2 in a wide range, so they can be easily tuned for practical use. Meanwhile, the performance of LNSI varies when \\u03bb3 changes in a wide range, as \\u03bb3 controls the capability of our method for capturing the label noise via the error matrix E. Speci\\ufb01cally, if \\u03bb3 is large, the label noise will be greatly ignored, leading to the performance degradation of LNSI on COIL20 and MNIST when the noise rate is 60%. J. Summary of Experiments Based on the above-mentioned experimental results of Sections VI-B\\u2013VI-I, we observe that: 1) LNSI performs bet- ter than other baseline algorithms in most cases, both on benchmark data sets and practical data sets; 2) the proposed algorithm in Algorithm 1 converges quickly to a stationary point; 3) LNSI is robust to the variation of the two tradeoff parameters including \\u03bb1 and \\u03bb2; 4) when the label noise is serious, the performance of LNSI might drop when \\u03bb3 is set to a large value; 5) the introduced low-rank regularizer and Laplacian regularizer are both bene\\ufb01cial to improve the classi\\ufb01cation performance; and 6) the proposed LNSI outper- forms other compared baselines when 40% and 60% labels Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. WEI et al.: HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 15 [39] P. L. Bartlett and S. Mendelson, \\u201cRademacher and Gaussian complexities: Risk bounds and structural results,\\u201d J. Mach. Learn. Res., vol. 3, pp. 463\\u2013482, Mar. 2003. [40] S. Kakade, S. Shalev-Shwartz, and A. Tewari. (2009). On the Duality of Strong Convexity and Strong Smoothness: Learn- ing Applications and Matrix Regularization. [Online]. Available: http://ttic.uchicago.edu/shai/papers/KakadeShalevTewari09.pdf [41] S. M. Kakade, S. Shalev-Shwartz, and A. Tewari, \\u201cRegularization techniques for learning with matrices,\\u201d J. Mach. Learn. Res., vol. 13, pp. 1865\\u20131890, Jun. 2012. [42] P.-A. Absil and P. Van Dooren, \\u201cTwo-sided Grassmann\\u2013Rayleigh quo- tient iteration,\\u201d Numerische Mathematik, vol. 114, no. 4, pp. 549\\u2013571, 2010. [43] R. Mahony and P.-A. Absil, \\u201cThe continuous-time Rayleigh quotient \\ufb02ow on the sphere,\\u201d Linear Algebra Appl., vol. 368, pp. 343\\u2013357, Jul. 2003. [44] H. Zhang and F. Ding, \\u201cOn the kronecker products and their applica- tions,\\u201d J. Appl. Math., vol. 2013, Jun. 2013, Art. no. 296185. [45] R. Meir and T. Zhang, \\u201cGeneralization error bounds for Bayesian mix- ture algorithms,\\u201d J. Mach. Learn. Res., vol. 4, pp. 839\\u2013860, Dec. 2003. [46] J. Yu, D. Tao, J. Li, and J. Cheng, \\u201cSemantic preserving distance metric learning and applications,\\u201d Inf. Sci., vol. 281, pp. 674\\u2013686, Oct. 2014. [47] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, \\u201cDistributed optimization and statistical learning via the alternating direction method of multipliers,\\u201d Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1\\u2013122, Jan. 2011. Yang Wei received the B.S. degree from the School of Computer Science and Engineering, Nanjing Uni- versity of Science and Technology, Nanjing, China, in 2015, where she is currently pursuing the Ph.D. degree. Her current research interests include pattern recognition, incomplete data-based learning, and deep learning. Chen Gong (M\\u201917) received the dual Ph.D. degree from Shanghai Jiao Tong University (SJTU), Shang- hai, China, and the University of Technology Sydney (UTS), Ultimo, NSW, Australia, in 2016. He is currently a Professor with the School of Computer Science and Engineering, Nanjing Uni- versity of Science and Technology, Nanjing, China. He has authored or coauthored more than 50 techni- cal articles at prominent journals and conferences such as the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), the IEEE TRANSACTIONS ON IMAGE PROCESSING (T-IP), the IEEE TRANS- ACTIONS ON CYBERNETICS (T-CYB), CVPR, AAAI, IJCAI, ICDM, and so on. His current research interests include machine learning and data mining. Dr. Gong was a recipient of the Excellent Doctoral Dissertation Award by SJTU and the Chinese Association for Arti\\ufb01cial Intelligence (CAAI). He was also enrolled by the Summit of the Six Top Talents Program of Jiangsu Province, China. Shuo Chen received the B.S. degree from the School of Computer Science and Engineering, Jinling Insti- tute of Technology, Nanjing, China, in 2014. He is currently pursuing the Ph.D. degree with the Nanjing University of Science and Technology, Nanjing. His current research interests include pattern recognition, metric learning, and deep learning. Tongliang Liu (M\\u201914) is currently a Lecturer with the School of Computer Science and the Fac- ulty of Engineering, and a Core Member with the UBTECH Sydney AI Centre, The University of Sydney, Darlington, NSW, Australia. He has authored and coauthored more than 60 research arti- cles, including the IEEE TRANSACTIONS ON PAT- TERN ANALYSIS AND MACHINE INTELLIGENCE (T-PAMI), the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), the IEEE TRANSACTIONS ON IMAGE PROCESSING (T-IP), ICML, AAAI, IJCAI, CVPR, ECCV, KDD, and ICME, with best paper awards. His current research interests include machine learning, computer vision, and data mining. Mr. Liu was a recipient of the 2019 ICME Best Paper Award and the Dis- covery Early Career Researcher Award (DECRA) from Australian Research Council (ARC). Jian Yang (M\\u201908) received the Ph.D. degree in pattern recognition and intelligence systems from the Nanjing University of Science and Technology (NUST), Nanjing, China, in 2002. In 2003, he was a Post-Doctoral Researcher with the University of Zaragoza, Zaragoza, Spain. From 2004 to 2006, he was a Post-Doctoral Fellow with the Biometrics Centre, The Hong Kong Polytechnic University, Hong Kong. From 2006 to 2007, he was a Post-Doctoral Fellow with the Department of Com- puter Science, New Jersey Institute of Technology, Newark, NJ, USA. He is currently a Chang-Jiang Professor with the School of Computer Science and Technology, NUST. He has authored more than 200 scienti\\ufb01c articles in pattern recognition and computer vision. His articles have been cited more than 5000 times in the Web of Science and 13000 times in the Scholar Google. His current research interests include pattern recogni- tion, computer vision, and machine learning. Dr. Yang is a fellow of IAPR. He is/was currently an Associate Editor of Pattern Recognition, Pattern Recognition Letters, the IEEE TRANSAC- TIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), and Neurocomputing. Dacheng Tao (F\\u201915) is a Professor of computer sci- ence and an ARC Laureate Fellow with the School of Computer Science and the Faculty of Engineering, and the Inaugural Director of the UBTECH Syd- ney Arti\\ufb01cial Intelligence Centre, The University of Sydney, Darlington, NSW, Australia. His research results in arti\\ufb01cial intelligence have expounded in one monograph. He has authored or coauthored more than 200 publications at prestigious journals and prominent conferences, such as the IEEE TRANS- ACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (T-PAMI), the International Journal of Computer Vision (IJCV), the Journal of Machine Learning Research (JMLR), AAAI, IJCAI, NIPS, ICML, CVPR, ICCV, ECCV, ICDM, and KDD, with several best paper awards. Prof. Tao is a fellow of the Australian Academy of Science. He was a recipient of the 2018 IEEE ICDM Research Contributions Award and the 2015 Australian Scopus-Eureka Prize. Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. WEI et al.: HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 13 Fig. 6. Parametric sensitivity of LNSI. (a)\\u2013(c), (d)\\u2013(f), (g)\\u2013(i), and (j)\\u2013(l) ISOLET, COIL20, MNIST, and CIFAR-10 data sets, respectively. (a), (d), (g), and (j) Variation in accuracy with respect to the tradeoff parameter \\u03bb1 when \\u03bb2 and \\u03bb3 are \\ufb01xed to the values indicated in the legend. (b), (e), (h), and (k) In\\ufb02uence of \\u03bb2 to the classi\\ufb01cation performance with other parameters \\ufb01xed. (c), (f), (i), and (l) Effect of \\u03bb3 while other two tradeoff parameters are \\ufb01xed. of training examples are incorrect because the error matrix E can capture the label errors successfully and the Laplacian regularizer is essential to boost the performance of LNSI. VII. CONCLUSION To solve the label inaccuracy that often occurs in plenty of real-world data sets for classi\\ufb01cation, this article provides a novel paradigm that formulates the noisy label removal problem as a matrix recovery problem and treats the example features as the side information to aid the recovery process. Our proposed LNSI seamlessly forms the label noise removal and classi\\ufb01er parameter optimization into a uni\\ufb01ed framework. The convergence property, computational complexity, and gen- eralization bound are also theoretically analyzed. We tested LNSI on various benchmark and practical data sets under different levels of label noise and found that LNSI outperforms other compared baseline methods and achieves robust results to different levels of label noise. APPENDIX PROOF OF THE CONVERGENCE To begin with, we provide the following two lemmas for the general ADMM solver. Lemma 12 [47]: Given the optimization problem with lin- ear constraints as min P, Q f (P) + g( Q) s.t. AP P + BQ Q = C (40) where AP and BQ are the coef\\ufb01cient matrices, and f (P), g( Q) are two functions with respect to the variables P, Q, respectively. C is a constant. Then, the Lagrangian function of (40) is L\\u03bc = f (P) + g( Q) + tr(M\\u2032\\u22a4(AP P + BQ Q \\u2212C)) + \\u03bc 2 \\u2225AP P + BQ Q \\u2212C\\u22252 F (41) where M\\u2032 is the Lagrangian multiplier and \\u03bc > 0 is the penalty coef\\ufb01cient. ADMM consists of the iterations Pk+1 := arg min P L\\u03bc(P, Qk, M\\u2032k) (42) Qk+1 := arg min Q L\\u03bc(Pk+1, Q, M\\u2032k) (43) M\\u2032k+1 := M\\u2032k + \\u03bc(AP Pk+1 + BQ Qk+1 \\u2212C). (44) If f (P) and g( Q) are convex, proper, and closed functions, and the unaugmented Lagrangian L0 = f (P) + g( Q) + tr(M\\u2032\\u22a4(AP P + BQ Q \\u2212C)) has a saddle point, the sequences {Pk, Qk, M\\u2032k} generated by the ADMM algorithm are guar- anteed to converge. Lemma 13 [47]: The generic-constrained convex optimiza- tion problem regarding the variable P is min P f (P) s.t. P \\u2208C (45) where f (\\u00b7) and C are convex. Problem (45) can then be rewritten in the form of ADMM as min f (P) + g(Zc) s.t. P \\u2212Zc = O (46) where g is the indicator function of C. Now we begin to formally verify the convergence of Algorithm 1. The proof of Theorem 2 is presented as follows. Proof: To facilitate the proof, we rewrite the optimization problem (7) in the formation of (40) according to Lemma 12. First, the optimization problem (7) with the convex set constraint can be transformed to the formation of (46) in Lemma 13. Therefore, problem (7) is equivalent to min Z,E \\u2225Z \\u2225\\u2217+ \\u03bb1\\u2225Z\\u22252 F + \\u03bb2tr((X J)\\u22a4L(X J)) + \\u03bb3\\u2225E\\u22252,1 + IC(K) s.t. Y = B + E, B = X J, J = Z, K = B (47) where IC(x) = \\u0015 x, if x \\u2208C \\u221e, othrewise. (48) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.872998Z",
          "iopub.execute_input": "2024-10-17T17:21:03.873444Z",
          "iopub.status.idle": "2024-10-17T17:21:03.880925Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.873407Z",
          "shell.execute_reply": "2024-10-17T17:21:03.879894Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2jE034J6s5b",
        "outputId": "a8153faf-86c2-4b9f-fca2-0262440e8ee1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.882095Z",
          "iopub.execute_input": "2024-10-17T17:21:03.882452Z",
          "iopub.status.idle": "2024-10-17T17:21:03.910181Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.882414Z",
          "shell.execute_reply": "2024-10-17T17:21:03.909303Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "mX84TrS76s5c",
        "outputId": "d8ff4de7-b748-4d64-ab8e-d330c72eefee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "count        15.00            15.00            15.00                    15.00   \n",
              "mean          7.00          5167.93           888.20                    70.73   \n",
              "std           4.47          1312.87           186.89                    53.88   \n",
              "min           0.00          3377.00           538.00                    31.00   \n",
              "25%           3.50          4199.00           784.50                    41.50   \n",
              "50%           7.00          4839.00           927.00                    51.00   \n",
              "75%          10.50          6164.00           982.00                    73.50   \n",
              "max          14.00          7785.00          1298.00                   240.00   \n",
              "\n",
              "       page_token_count  \n",
              "count             15.00  \n",
              "mean            1291.98  \n",
              "std              328.22  \n",
              "min              844.25  \n",
              "25%             1049.75  \n",
              "50%             1209.75  \n",
              "75%             1541.00  \n",
              "max             1946.25  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9b828956-ac82-4bd2-aa82-85524302e8e1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.00</td>\n",
              "      <td>5167.93</td>\n",
              "      <td>888.20</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1291.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.47</td>\n",
              "      <td>1312.87</td>\n",
              "      <td>186.89</td>\n",
              "      <td>53.88</td>\n",
              "      <td>328.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00</td>\n",
              "      <td>3377.00</td>\n",
              "      <td>538.00</td>\n",
              "      <td>31.00</td>\n",
              "      <td>844.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.50</td>\n",
              "      <td>4199.00</td>\n",
              "      <td>784.50</td>\n",
              "      <td>41.50</td>\n",
              "      <td>1049.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.00</td>\n",
              "      <td>4839.00</td>\n",
              "      <td>927.00</td>\n",
              "      <td>51.00</td>\n",
              "      <td>1209.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>10.50</td>\n",
              "      <td>6164.00</td>\n",
              "      <td>982.00</td>\n",
              "      <td>73.50</td>\n",
              "      <td>1541.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.00</td>\n",
              "      <td>7785.00</td>\n",
              "      <td>1298.00</td>\n",
              "      <td>240.00</td>\n",
              "      <td>1946.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b828956-ac82-4bd2-aa82-85524302e8e1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b828956-ac82-4bd2-aa82-85524302e8e1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b828956-ac82-4bd2-aa82-85524302e8e1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3bade2de-5f94-4a4d-b018-2265b6a564b6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3bade2de-5f94-4a4d-b018-2265b6a564b6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3bade2de-5f94-4a4d-b018-2265b6a564b6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.198362207739774,\n        \"min\": 0.0,\n        \"max\": 15.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          15.0,\n          7.0,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2521.7147826939395,\n        \"min\": 15.0,\n        \"max\": 7785.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5167.93,\n          4839.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 429.27170734487703,\n        \"min\": 15.0,\n        \"max\": 1298.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          888.2,\n          927.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70.56648414033798,\n        \"min\": 15.0,\n        \"max\": 240.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          70.73,\n          51.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 627.8271069786194,\n        \"min\": 15.0,\n        \"max\": 1946.25,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1291.98,\n          1209.75,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Further text processing (splitting pages into sentences)\n",
        "We will to follow the workflow of:\n",
        "\n",
        "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
        "\n",
        "Why split into sentences?\n",
        "\n",
        "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
        "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
        "\n",
        "\n",
        "We will use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`."
      ],
      "metadata": {
        "id": "Z5aXe8pS6s5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.en import English\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:03.911389Z",
          "iopub.execute_input": "2024-10-17T17:21:03.911716Z",
          "iopub.status.idle": "2024-10-17T17:21:10.028223Z",
          "shell.execute_reply.started": "2024-10-17T17:21:03.911684Z",
          "shell.execute_reply": "2024-10-17T17:21:10.027305Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fd1d590dfdd54ed8bce095f002758635",
            "fdb65cae0bbf439ab2da163951dd2d96",
            "6bb6e8c7e6d040a6a5eff0e3dc02ec81",
            "2dc009815ae24155be0b0de48b6032fa",
            "10460e9582c84eceadb3e9e2a3d0544e",
            "3a30dd291d484d2b977e26fecdb43fcf",
            "119039938e574c5c97c830090925867a",
            "419ccdc6e795429ea16252df76b680f0",
            "c390a76c915146a18258bd86024f3441",
            "803ed70d631a472d936fccc1411eb6d3",
            "adb98018912746268669ad145cee4fc5"
          ]
        },
        "id": "lwguDxE36s5d",
        "outputId": "1b808703-3fd4-47b7-cc37-50ab6724479a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd1d590dfdd54ed8bce095f002758635"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_texts, k=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:10.029533Z",
          "iopub.execute_input": "2024-10-17T17:21:10.030181Z",
          "iopub.status.idle": "2024-10-17T17:21:10.038463Z",
          "shell.execute_reply.started": "2024-10-17T17:21:10.030137Z",
          "shell.execute_reply": "2024-10-17T17:21:10.037338Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OCM9xmbn6s5e",
        "outputId": "44dae5f0-a155-4134-c9e0-72ca240e1c13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 0,\n",
              "  'page_char_count': 7049,\n",
              "  'page_word_count': 1036,\n",
              "  'page_sentence_count_raw': 88,\n",
              "  'page_token_count': 1762.25,\n",
              "  'text': 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classiﬁcation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract—Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled. Such noisy labels will mislead the classiﬁer training and severely decrease the classiﬁcation performance. Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization. However, they are only suitable for binary classiﬁcation and also require strong prior knowledge. Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem. We denote our proposed method as “label noise handling via side information” (LNSI). Speciﬁcally, the observed label matrix is decomposed as the sum of two parts, in which the ﬁrst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix. The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufﬁciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019. This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the “Summit of the Six Top Talents” Program under Grant DZXX-027, in part by the “Young Elite Scientists Sponsorship Program” by Jiangsu Province, in part by the “Young Elite Scientists Sponsorship Program” by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the “111” Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.) Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn). S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn). T. Liu and D. Tao are with the UBTECH Sydney Artiﬁcial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au). Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org. Digital Object Identiﬁer 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems. Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classiﬁcation error of LNSI is upper bounded. The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets conﬁrm the superiority of LNSI to state-of-the-art approaches on label noise handling. Index Terms—Classiﬁcation, generalization bound, label noise, matrix recovery, side information. I. INTRODUCTION T RADITIONALLY, a reliable supervised classiﬁer, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the sufﬁ- cient correctly labeled data. Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1]. For instance, as the numerous examples in many applications (e.g., image classiﬁcation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue. Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors. These noisy labels will signiﬁcantly mislead the classiﬁer training and then severely decrease the classiﬁcation performance [2]. Hence, designing algorithms that account for the data with noisy labels is of great signiﬁcance and has become a critical issue in the machine learning community. Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]–[6]. For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part. Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results. However, they are only applicable to binary classiﬁcation and the extension to multi-class is nontrivial [7]. Moreover, these methods require the estimation of class prior, which is actually quite difﬁcult in the presence of corrupted observed data. On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.',\n",
              "  'sentences': ['This article has been accepted for inclusion in a future issue of this journal.',\n",
              "   'Content is final as presented, with the exception of pagination.',\n",
              "   'IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classiﬁcation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract—Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.',\n",
              "   'Such noisy labels will mislead the classiﬁer training and severely decrease the classiﬁcation performance.',\n",
              "   'Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.',\n",
              "   'However, they are only suitable for binary classiﬁcation and also require strong prior knowledge.',\n",
              "   'Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.',\n",
              "   'We denote our proposed method as “label noise handling via side information” (LNSI).',\n",
              "   'Speciﬁcally, the observed label matrix is decomposed as the sum of two parts, in which the ﬁrst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.',\n",
              "   'The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufﬁciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.',\n",
              "   'This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the “Summit of the Six Top Talents” Program under Grant DZXX-027, in part by the “Young Elite Scientists Sponsorship Program” by Jiangsu Province, in part by the “Young Elite Scientists Sponsorship Program” by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the “111” Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (',\n",
              "   'Corresponding author: Chen Gong.)',\n",
              "   'Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).',\n",
              "   'S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).',\n",
              "   'T. Liu and D. Tao are with the UBTECH Sydney Artiﬁcial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).',\n",
              "   'Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org.',\n",
              "   'Digital Object Identiﬁer 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.',\n",
              "   'Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classiﬁcation error of LNSI is upper bounded.',\n",
              "   'The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets conﬁrm the superiority of LNSI to state-of-the-art approaches on label noise handling.',\n",
              "   'Index Terms—Classiﬁcation, generalization bound, label noise, matrix recovery, side information.',\n",
              "   'I. INTRODUCTION T RADITIONALLY, a reliable supervised classiﬁer, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the sufﬁ- cient correctly labeled data.',\n",
              "   'Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].',\n",
              "   'For instance, as the numerous examples in many applications (e.g., image classiﬁcation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.',\n",
              "   'Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.',\n",
              "   'These noisy labels will signiﬁcantly mislead the classiﬁer training and then severely decrease the classiﬁcation performance [2].',\n",
              "   'Hence, designing algorithms that account for the data with noisy labels is of great signiﬁcance and has become a critical issue in the machine learning community.',\n",
              "   'Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]–[6].',\n",
              "   'For example, Gao et al. [',\n",
              "   '3] and Patrini et al. [',\n",
              "   '6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.',\n",
              "   'Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.',\n",
              "   'However, they are only applicable to binary classiﬁcation and the extension to multi-class is nontrivial [7].',\n",
              "   'Moreover, these methods require the estimation of class prior, which is actually quite difﬁcult in the presence of corrupted observed data.',\n",
              "   'On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X © 2019 IEEE.',\n",
              "   'Personal use is permitted, but republication/redistribution requires IEEE permission.',\n",
              "   'See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.',\n",
              "   'Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.',\n",
              "   'Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.',\n",
              "   ' Restrictions apply.'],\n",
              "  'page_sentence_count_spacy': 39}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:10.042404Z",
          "iopub.execute_input": "2024-10-17T17:21:10.04276Z",
          "iopub.status.idle": "2024-10-17T17:21:10.070477Z",
          "shell.execute_reply.started": "2024-10-17T17:21:10.042728Z",
          "shell.execute_reply": "2024-10-17T17:21:10.069505Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "collapsed": true,
        "id": "Sgc3K-626s5e",
        "outputId": "66bf5514-8b58-4441-d06d-bf3cfe9ca47c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "0             0             7049             1036                       88   \n",
              "1             1             6177              969                       39   \n",
              "2             2             6151             1044                       56   \n",
              "3             3             4901              927                       65   \n",
              "4             4             4164              833                       44   \n",
              "5             5             4839              939                       79   \n",
              "6             6             4175              857                       45   \n",
              "7             7             4358              839                       51   \n",
              "8             8             5737              981                       68   \n",
              "9             9             3377              538                       31   \n",
              "10           10             4482              736                       37   \n",
              "11           11             4223              688                       37   \n",
              "12           12             3635              655                       46   \n",
              "13           13             7785             1298                      240   \n",
              "14           14             6466              983                      135   \n",
              "\n",
              "    page_token_count                                               text  \\\n",
              "0            1762.25  This article has been accepted for inclusion i...   \n",
              "1            1544.25  This article has been accepted for inclusion i...   \n",
              "2            1537.75  This article has been accepted for inclusion i...   \n",
              "3            1225.25  This article has been accepted for inclusion i...   \n",
              "4            1041.00  This article has been accepted for inclusion i...   \n",
              "5            1209.75  This article has been accepted for inclusion i...   \n",
              "6            1043.75  This article has been accepted for inclusion i...   \n",
              "7            1089.50  This article has been accepted for inclusion i...   \n",
              "8            1434.25  This article has been accepted for inclusion i...   \n",
              "9             844.25  This article has been accepted for inclusion i...   \n",
              "10           1120.50  This article has been accepted for inclusion i...   \n",
              "11           1055.75  This article has been accepted for inclusion i...   \n",
              "12            908.75  This article has been accepted for inclusion i...   \n",
              "13           1946.25  This article has been accepted for inclusion i...   \n",
              "14           1616.50  This article has been accepted for inclusion i...   \n",
              "\n",
              "                                            sentences  \\\n",
              "0   [This article has been accepted for inclusion ...   \n",
              "1   [This article has been accepted for inclusion ...   \n",
              "2   [This article has been accepted for inclusion ...   \n",
              "3   [This article has been accepted for inclusion ...   \n",
              "4   [This article has been accepted for inclusion ...   \n",
              "5   [This article has been accepted for inclusion ...   \n",
              "6   [This article has been accepted for inclusion ...   \n",
              "7   [This article has been accepted for inclusion ...   \n",
              "8   [This article has been accepted for inclusion ...   \n",
              "9   [This article has been accepted for inclusion ...   \n",
              "10  [This article has been accepted for inclusion ...   \n",
              "11  [This article has been accepted for inclusion ...   \n",
              "12  [This article has been accepted for inclusion ...   \n",
              "13  [This article has been accepted for inclusion ...   \n",
              "14  [This article has been accepted for inclusion ...   \n",
              "\n",
              "    page_sentence_count_spacy  \n",
              "0                          39  \n",
              "1                          56  \n",
              "2                          53  \n",
              "3                          33  \n",
              "4                          30  \n",
              "5                          35  \n",
              "6                          20  \n",
              "7                          28  \n",
              "8                          55  \n",
              "9                          32  \n",
              "10                         36  \n",
              "11                         37  \n",
              "12                         35  \n",
              "13                        268  \n",
              "14                         84  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-35ff9458-52e8-4596-90f0-516d5595e8ba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>text</th>\n",
              "      <th>sentences</th>\n",
              "      <th>page_sentence_count_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7049</td>\n",
              "      <td>1036</td>\n",
              "      <td>88</td>\n",
              "      <td>1762.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>6177</td>\n",
              "      <td>969</td>\n",
              "      <td>39</td>\n",
              "      <td>1544.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>56</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>6151</td>\n",
              "      <td>1044</td>\n",
              "      <td>56</td>\n",
              "      <td>1537.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4901</td>\n",
              "      <td>927</td>\n",
              "      <td>65</td>\n",
              "      <td>1225.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4164</td>\n",
              "      <td>833</td>\n",
              "      <td>44</td>\n",
              "      <td>1041.00</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>4839</td>\n",
              "      <td>939</td>\n",
              "      <td>79</td>\n",
              "      <td>1209.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>4175</td>\n",
              "      <td>857</td>\n",
              "      <td>45</td>\n",
              "      <td>1043.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>4358</td>\n",
              "      <td>839</td>\n",
              "      <td>51</td>\n",
              "      <td>1089.50</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>5737</td>\n",
              "      <td>981</td>\n",
              "      <td>68</td>\n",
              "      <td>1434.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>3377</td>\n",
              "      <td>538</td>\n",
              "      <td>31</td>\n",
              "      <td>844.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>4482</td>\n",
              "      <td>736</td>\n",
              "      <td>37</td>\n",
              "      <td>1120.50</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>4223</td>\n",
              "      <td>688</td>\n",
              "      <td>37</td>\n",
              "      <td>1055.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>3635</td>\n",
              "      <td>655</td>\n",
              "      <td>46</td>\n",
              "      <td>908.75</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>7785</td>\n",
              "      <td>1298</td>\n",
              "      <td>240</td>\n",
              "      <td>1946.25</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>6466</td>\n",
              "      <td>983</td>\n",
              "      <td>135</td>\n",
              "      <td>1616.50</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>[This article has been accepted for inclusion ...</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-35ff9458-52e8-4596-90f0-516d5595e8ba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-35ff9458-52e8-4596-90f0-516d5595e8ba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-35ff9458-52e8-4596-90f0-516d5595e8ba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-12495437-8146-4a9b-b32b-e96e29bcab6b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12495437-8146-4a9b-b32b-e96e29bcab6b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-12495437-8146-4a9b-b32b-e96e29bcab6b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_1864b9ec-69ec-4f89-b258-a3440a3d7e2a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1864b9ec-69ec-4f89-b258-a3440a3d7e2a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1312,\n        \"min\": 3377,\n        \"max\": 7785,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          3377,\n          4223,\n          7049\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 186,\n        \"min\": 538,\n        \"max\": 1298,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          538,\n          688,\n          1036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 53,\n        \"min\": 31,\n        \"max\": 240,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          31,\n          46,\n          88\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 328.2180713808312,\n        \"min\": 844.25,\n        \"max\": 1946.25,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          844.25,\n          1055.75,\n          1762.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 3. Experimental results of the compared methods on \\ufb01ve UCI benchmark data sets. (a)\\u2013(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively. TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET. THE CLASSIFICATION ACCURACIES (MEAN \\u00b1 STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. \\u2022/\\u25e6INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL). THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET. THE CLASSIFICATION ACCURACIES (MEAN \\u00b1 STD.) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. \\u2022/\\u25e6INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL). THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise. D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203. COIL20 is a popular public data set for object classi\\ufb01cation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles. The resolution of each gray-level image is 32 \\u00d7 32 [46]. We use the output of the \\ufb01rst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions. Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data. Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label. In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with \\u03c3k = 0.1. In addition, the tradeoff parameters in (6) such as \\u03bb1, \\u03bb2, and \\u03bb3 were tuned by searching the grid {10\\u22122, 10\\u22121, . . . , 103} in order to obtain the satisfactory results. The classi\\ufb01cation accuracies of all compared methods under different label noise levels are presented in Table III. It can be observed that the performances of all methods decrease with the increase in noise level. However, LNSI achieves the best results in most cases when compared with other baseline methods. Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%. E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition. Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes. The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 4. Illustration of convergence process of the ADMM method adopted by LNSI on the four practical data sets. For each data set, we present the convergence curves under different convergence criteria in the Algorithm 1. (a)\\u2013(c) ISOLET data set. (d)\\u2013(f) COIL20 data set. (g)\\u2013(i) MNIST data set. (j)\\u2013(l) CIFAR-10 data set. tr((X Z)\\u22a4L(X Z)). To this end, we study the performances of three different settings on the above four practical data sets (such as ISOLET, COIL20, MNIST, and CIFAR-10). First, both low-rank regularizer and Laplacian regularizer are reserved to constitute the original model (abbreviated as \\u201cLNSI\\u201d); second, the Laplacian regularizer is removed from the original model to see how this term in\\ufb02uences the model performance (abbreviated as \\u201cNo Laplacian\\u201d); third, we remove the low-rank regularizer while keeping the Lapla- cian regularizer to observe the effect of low-rank regularizer (abbreviated as \\u201cNo Low-Rank\\u201d). The experimental results of these three models are illus- trated in Fig. 5, in which 40% and 60% of training examples have incorrect labels. The results reveal that LNSI achieves the best performance on all four data sets, especially when the label noise rate is relatively high. By contrast, the accuracy of LNSI will drop without any of the two terms such as low-rank regularizer and Laplacian regularizer, and therefore, these two regularization terms are essential to boost the performance of LNSI. I. Parametric Sensitivity Note that the objective function (8) in LNSI contains three tradeoff parameters \\u03bb1, \\u03bb2, and \\u03bb3 that should be manually Fig. 5. Results of ablation study on four practical data sets. For convenience, the original model is denoted as \\u201cLNSI,\\u201d the setting without the Laplacian regularization term is dubbed as \\u201cNo Laplacian,\\u201d and the setting without the low-rank term is named as \\u201cNo Low-Rank.\\u201d (a) ISOLET data set. (b) COIL20 Data set. (c) MNIST data set. (d) CIFAR-10 data set. tuned. Therefore, in this section, we discuss whether the choices of them will signi\\ufb01cantly in\\ufb02uence the performance of LNSI. To this end, we examine the classi\\ufb01cation accuracy at two different levels (20% and 60%) of label noise via changing one of \\u03bb1, \\u03bb2, and \\u03bb3, and meanwhile \\ufb01xing the others to the optimal constant values under different data sets and different noise rates. The above-mentioned four practical data sets are adopted here, and the results are shown in Fig. 6. Fig. 6(a)\\u2013(c) shows the experiments on the ISOLET data set, (d)\\u2013(f) shows the experiments on the COIL20 data set, (g)\\u2013(i) shows the experiments on the MNIST data set, and (j)\\u2013(l) shows the experiments on the CIFAR-10 data set. The results reveal that LNSI is robust to the variations of \\u03bb1 and \\u03bb2 in a wide range, so they can be easily tuned for practical use. Meanwhile, the performance of LNSI varies when \\u03bb3 changes in a wide range, as \\u03bb3 controls the capability of our method for capturing the label noise via the error matrix E. Speci\\ufb01cally, if \\u03bb3 is large, the label noise will be greatly ignored, leading to the performance degradation of LNSI on COIL20 and MNIST when the noise rate is 60%. J. Summary of Experiments Based on the above-mentioned experimental results of Sections VI-B\\u2013VI-I, we observe that: 1) LNSI performs bet- ter than other baseline algorithms in most cases, both on benchmark data sets and practical data sets; 2) the proposed algorithm in Algorithm 1 converges quickly to a stationary point; 3) LNSI is robust to the variation of the two tradeoff parameters including \\u03bb1 and \\u03bb2; 4) when the label noise is serious, the performance of LNSI might drop when \\u03bb3 is set to a large value; 5) the introduced low-rank regularizer and Laplacian regularizer are both bene\\ufb01cial to improve the classi\\ufb01cation performance; and 6) the proposed LNSI outper- forms other compared baselines when 40% and 60% labels Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classi\\ufb01cation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract\\u2014Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled. Such noisy labels will mislead the classi\\ufb01er training and severely decrease the classi\\ufb01cation performance. Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization. However, they are only suitable for binary classi\\ufb01cation and also require strong prior knowledge. Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem. We denote our proposed method as \\u201clabel noise handling via side information\\u201d (LNSI). Speci\\ufb01cally, the observed label matrix is decomposed as the sum of two parts, in which the \\ufb01rst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix. The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been suf\\ufb01ciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019. This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the \\u201cSummit of the Six Top Talents\\u201d Program under Grant DZXX-027, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by Jiangsu Province, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the \\u201c111\\u201d Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.) Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi\\u2019an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn). S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn). T. Liu and D. Tao are with the UBTECH Sydney Arti\\ufb01cial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au). Color versions of one or more of the \\ufb01gures in this article are available online at http://ieeexplore.ieee.org. Digital Object Identi\\ufb01er 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems. Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classi\\ufb01cation error of LNSI is upper bounded. The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets con\\ufb01rm the superiority of LNSI to state-of-the-art approaches on label noise handling. Index Terms\\u2014Classi\\ufb01cation, generalization bound, label noise, matrix recovery, side information. I. INTRODUCTION T RADITIONALLY, a reliable supervised classi\\ufb01er, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the suf\\ufb01- cient correctly labeled data. Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1]. For instance, as the numerous examples in many applications (e.g., image classi\\ufb01cation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue. Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors. These noisy labels will signi\\ufb01cantly mislead the classi\\ufb01er training and then severely decrease the classi\\ufb01cation performance [2]. Hence, designing algorithms that account for the data with noisy labels is of great signi\\ufb01cance and has become a critical issue in the machine learning community. Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]\\u2013[6]. For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part. Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results. However, they are only applicable to binary classi\\ufb01cation and the extension to multi-class is nontrivial [7]. Moreover, these methods require the estimation of class prior, which is actually quite dif\\ufb01cult in the presence of corrupted observed data. On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X \\u00a9 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentences\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 60,\n        \"min\": 20,\n        \"max\": 268,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          32,\n          37,\n          39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().round(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:10.071714Z",
          "iopub.execute_input": "2024-10-17T17:21:10.072021Z",
          "iopub.status.idle": "2024-10-17T17:21:10.098931Z",
          "shell.execute_reply.started": "2024-10-17T17:21:10.071988Z",
          "shell.execute_reply": "2024-10-17T17:21:10.098018Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "wC1dBOsM6s5e",
        "outputId": "77696036-62e3-456c-d122-65c96625996b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "count        15.00            15.00            15.00                    15.00   \n",
              "mean          7.00          5167.93           888.20                    70.73   \n",
              "std           4.47          1312.87           186.89                    53.88   \n",
              "min           0.00          3377.00           538.00                    31.00   \n",
              "25%           3.50          4199.00           784.50                    41.50   \n",
              "50%           7.00          4839.00           927.00                    51.00   \n",
              "75%          10.50          6164.00           982.00                    73.50   \n",
              "max          14.00          7785.00          1298.00                   240.00   \n",
              "\n",
              "       page_token_count  page_sentence_count_spacy  \n",
              "count             15.00                      15.00  \n",
              "mean            1291.98                      56.07  \n",
              "std              328.22                      60.67  \n",
              "min              844.25                      20.00  \n",
              "25%             1049.75                      32.50  \n",
              "50%             1209.75                      36.00  \n",
              "75%             1541.00                      54.00  \n",
              "max             1946.25                     268.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f30648b1-8055-40b4-8538-22059590cd06\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>page_sentence_count_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.00</td>\n",
              "      <td>5167.93</td>\n",
              "      <td>888.20</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1291.98</td>\n",
              "      <td>56.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.47</td>\n",
              "      <td>1312.87</td>\n",
              "      <td>186.89</td>\n",
              "      <td>53.88</td>\n",
              "      <td>328.22</td>\n",
              "      <td>60.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00</td>\n",
              "      <td>3377.00</td>\n",
              "      <td>538.00</td>\n",
              "      <td>31.00</td>\n",
              "      <td>844.25</td>\n",
              "      <td>20.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.50</td>\n",
              "      <td>4199.00</td>\n",
              "      <td>784.50</td>\n",
              "      <td>41.50</td>\n",
              "      <td>1049.75</td>\n",
              "      <td>32.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.00</td>\n",
              "      <td>4839.00</td>\n",
              "      <td>927.00</td>\n",
              "      <td>51.00</td>\n",
              "      <td>1209.75</td>\n",
              "      <td>36.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>10.50</td>\n",
              "      <td>6164.00</td>\n",
              "      <td>982.00</td>\n",
              "      <td>73.50</td>\n",
              "      <td>1541.00</td>\n",
              "      <td>54.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.00</td>\n",
              "      <td>7785.00</td>\n",
              "      <td>1298.00</td>\n",
              "      <td>240.00</td>\n",
              "      <td>1946.25</td>\n",
              "      <td>268.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f30648b1-8055-40b4-8538-22059590cd06')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f30648b1-8055-40b4-8538-22059590cd06 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f30648b1-8055-40b4-8538-22059590cd06');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7ed218d5-da37-46b3-9c64-f8fa10a56dc7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ed218d5-da37-46b3-9c64-f8fa10a56dc7')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7ed218d5-da37-46b3-9c64-f8fa10a56dc7 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.198362207739774,\n        \"min\": 0.0,\n        \"max\": 15.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          15.0,\n          7.0,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2521.7147826939395,\n        \"min\": 15.0,\n        \"max\": 7785.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5167.93,\n          4839.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 429.27170734487703,\n        \"min\": 15.0,\n        \"max\": 1298.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          888.2,\n          927.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70.56648414033798,\n        \"min\": 15.0,\n        \"max\": 240.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          70.73,\n          51.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 627.8271069786194,\n        \"min\": 15.0,\n        \"max\": 1946.25,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1291.98,\n          1209.75,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82.62558977528866,\n        \"min\": 15.0,\n        \"max\": 268.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          56.07,\n          36.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Chunking our sentences together\n",
        "Why do we do this?\n",
        "\n",
        "1. Easier to manage similar sized chunks of text.\n",
        "2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n",
        "3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible."
      ],
      "metadata": {
        "id": "ZmWM2_Kc6s5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 100\n",
        "def split_list(input_list: list[str],\n",
        "               slice_size: int=chunk_size) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
        "\n",
        "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
        "    \"\"\"\n",
        "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]\n",
        "\n",
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                         slice_size=chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:47.460495Z",
          "iopub.execute_input": "2024-10-17T17:21:47.461415Z",
          "iopub.status.idle": "2024-10-17T17:21:47.483264Z",
          "shell.execute_reply.started": "2024-10-17T17:21:47.461369Z",
          "shell.execute_reply": "2024-10-17T17:21:47.482267Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7751821216bf49cfa338d67d528078f7",
            "266f949dd15542eeb56bd1fddd5e4b0e",
            "7337059e2e114298a4c3c9ca1e60879c",
            "f066f6f3b1e1436f9b6f770306290779",
            "1b3d46b1153b4beea5b6363988a87ae2",
            "5172b74c7de1421c8ce9b92a9c6eef42",
            "a86e9d6469544c38bb6539e9d31dd4b4",
            "2ede91bac2a7421c86cedfa5f09e3961",
            "f475ba3be7bc4361b4408d5a1fe60688",
            "aff238a030f3457d853a2a61efeab48c",
            "51b8979e91d742f9b336de39b46427de"
          ]
        },
        "id": "TUlFccYW6s5e",
        "outputId": "97c2dace-cfc8-4b6d-b797-f7afa6f85812"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7751821216bf49cfa338d67d528078f7"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_texts,k=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:48.157385Z",
          "iopub.execute_input": "2024-10-17T17:21:48.157791Z",
          "iopub.status.idle": "2024-10-17T17:21:48.166502Z",
          "shell.execute_reply.started": "2024-10-17T17:21:48.157752Z",
          "shell.execute_reply": "2024-10-17T17:21:48.165496Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8e1iECuM6s5e",
        "outputId": "f80d221c-b598-4f32-90f7-27056508f23b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 9,\n",
              "  'page_char_count': 3377,\n",
              "  'page_word_count': 538,\n",
              "  'page_sentence_count_raw': 31,\n",
              "  'page_token_count': 844.25,\n",
              "  'text': 'This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination. 10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig. 3. Experimental results of the compared methods on ﬁve UCI benchmark data sets. (a)–(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively. TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET. THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL). THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET. THE CLASSIFICATION ACCURACIES (MEAN ± STD.) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL). THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise. D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203. COIL20 is a popular public data set for object classiﬁcation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles. The resolution of each gray-level image is 32 × 32 [46]. We use the output of the ﬁrst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions. Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data. Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label. In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with σk = 0.1. In addition, the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−2, 10−1, . . . , 103} in order to obtain the satisfactory results. The classiﬁcation accuracies of all compared methods under different label noise levels are presented in Table III. It can be observed that the performances of all methods decrease with the increase in noise level. However, LNSI achieves the best results in most cases when compared with other baseline methods. Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%. E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition. Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes. The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY. Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.  Restrictions apply.',\n",
              "  'sentences': ['This article has been accepted for inclusion in a future issue of this journal.',\n",
              "   'Content is final as presented, with the exception of pagination.',\n",
              "   '10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.',\n",
              "   '3.',\n",
              "   'Experimental results of the compared methods on ﬁve UCI benchmark data sets. (',\n",
              "   'a)–(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively.',\n",
              "   'TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET.',\n",
              "   'THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/',\n",
              "   '◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).',\n",
              "   'THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET.',\n",
              "   'THE CLASSIFICATION ACCURACIES (MEAN ± STD.)',\n",
              "   'UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/',\n",
              "   '◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).',\n",
              "   'THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise.',\n",
              "   'D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203.',\n",
              "   'COIL20 is a popular public data set for object classiﬁcation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles.',\n",
              "   'The resolution of each gray-level image is 32 × 32 [46].',\n",
              "   'We use the output of the ﬁrst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions.',\n",
              "   'Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data.',\n",
              "   'Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label.',\n",
              "   'In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with σk = 0.1.',\n",
              "   'In addition, the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,',\n",
              "   '103} in order to obtain the satisfactory results.',\n",
              "   'The classiﬁcation accuracies of all compared methods under different label noise levels are presented in Table III.',\n",
              "   'It can be observed that the performances of all methods decrease with the increase in noise level.',\n",
              "   'However, LNSI achieves the best results in most cases when compared with other baseline methods.',\n",
              "   'Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%.',\n",
              "   'E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition.',\n",
              "   'Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes.',\n",
              "   'The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.',\n",
              "   'Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.',\n",
              "   ' Restrictions apply.'],\n",
              "  'page_sentence_count_spacy': 32,\n",
              "  'sentence_chunks': [['This article has been accepted for inclusion in a future issue of this journal.',\n",
              "    'Content is final as presented, with the exception of pagination.',\n",
              "    '10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.',\n",
              "    '3.',\n",
              "    'Experimental results of the compared methods on ﬁve UCI benchmark data sets. (',\n",
              "    'a)–(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively.',\n",
              "    'TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET.',\n",
              "    'THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/',\n",
              "    '◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).',\n",
              "    'THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET.',\n",
              "    'THE CLASSIFICATION ACCURACIES (MEAN ± STD.)',\n",
              "    'UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/',\n",
              "    '◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).',\n",
              "    'THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise.',\n",
              "    'D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203.',\n",
              "    'COIL20 is a popular public data set for object classiﬁcation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles.',\n",
              "    'The resolution of each gray-level image is 32 × 32 [46].',\n",
              "    'We use the output of the ﬁrst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions.',\n",
              "    'Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data.',\n",
              "    'Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label.',\n",
              "    'In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with σk = 0.1.',\n",
              "    'In addition, the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,',\n",
              "    '103} in order to obtain the satisfactory results.',\n",
              "    'The classiﬁcation accuracies of all compared methods under different label noise levels are presented in Table III.',\n",
              "    'It can be observed that the performances of all methods decrease with the increase in noise level.',\n",
              "    'However, LNSI achieves the best results in most cases when compared with other baseline methods.',\n",
              "    'Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%.',\n",
              "    'E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition.',\n",
              "    'Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes.',\n",
              "    'The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.',\n",
              "    'Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore.',\n",
              "    ' Restrictions apply.']],\n",
              "  'num_chunks': 1}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:48.80437Z",
          "iopub.execute_input": "2024-10-17T17:21:48.804895Z",
          "iopub.status.idle": "2024-10-17T17:21:48.844123Z",
          "shell.execute_reply.started": "2024-10-17T17:21:48.804853Z",
          "shell.execute_reply": "2024-10-17T17:21:48.843134Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Cnh2yPEk6s5e",
        "outputId": "231eb0af-2847-4aaa-cb68-1a39090b6078"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
              "count        15.00            15.00            15.00                    15.00   \n",
              "mean          7.00          5167.93           888.20                    70.73   \n",
              "std           4.47          1312.87           186.89                    53.88   \n",
              "min           0.00          3377.00           538.00                    31.00   \n",
              "25%           3.50          4199.00           784.50                    41.50   \n",
              "50%           7.00          4839.00           927.00                    51.00   \n",
              "75%          10.50          6164.00           982.00                    73.50   \n",
              "max          14.00          7785.00          1298.00                   240.00   \n",
              "\n",
              "       page_token_count  page_sentence_count_spacy  num_chunks  \n",
              "count             15.00                      15.00       15.00  \n",
              "mean            1291.98                      56.07        1.13  \n",
              "std              328.22                      60.67        0.52  \n",
              "min              844.25                      20.00        1.00  \n",
              "25%             1049.75                      32.50        1.00  \n",
              "50%             1209.75                      36.00        1.00  \n",
              "75%             1541.00                      54.00        1.00  \n",
              "max             1946.25                     268.00        3.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6fcb5369-fd14-40eb-9d02-9af73b367737\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>page_char_count</th>\n",
              "      <th>page_word_count</th>\n",
              "      <th>page_sentence_count_raw</th>\n",
              "      <th>page_token_count</th>\n",
              "      <th>page_sentence_count_spacy</th>\n",
              "      <th>num_chunks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "      <td>15.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.00</td>\n",
              "      <td>5167.93</td>\n",
              "      <td>888.20</td>\n",
              "      <td>70.73</td>\n",
              "      <td>1291.98</td>\n",
              "      <td>56.07</td>\n",
              "      <td>1.13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.47</td>\n",
              "      <td>1312.87</td>\n",
              "      <td>186.89</td>\n",
              "      <td>53.88</td>\n",
              "      <td>328.22</td>\n",
              "      <td>60.67</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00</td>\n",
              "      <td>3377.00</td>\n",
              "      <td>538.00</td>\n",
              "      <td>31.00</td>\n",
              "      <td>844.25</td>\n",
              "      <td>20.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.50</td>\n",
              "      <td>4199.00</td>\n",
              "      <td>784.50</td>\n",
              "      <td>41.50</td>\n",
              "      <td>1049.75</td>\n",
              "      <td>32.50</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>7.00</td>\n",
              "      <td>4839.00</td>\n",
              "      <td>927.00</td>\n",
              "      <td>51.00</td>\n",
              "      <td>1209.75</td>\n",
              "      <td>36.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>10.50</td>\n",
              "      <td>6164.00</td>\n",
              "      <td>982.00</td>\n",
              "      <td>73.50</td>\n",
              "      <td>1541.00</td>\n",
              "      <td>54.00</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.00</td>\n",
              "      <td>7785.00</td>\n",
              "      <td>1298.00</td>\n",
              "      <td>240.00</td>\n",
              "      <td>1946.25</td>\n",
              "      <td>268.00</td>\n",
              "      <td>3.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6fcb5369-fd14-40eb-9d02-9af73b367737')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6fcb5369-fd14-40eb-9d02-9af73b367737 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6fcb5369-fd14-40eb-9d02-9af73b367737');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b83dec45-4a81-46ec-a68e-b118d4093f54\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b83dec45-4a81-46ec-a68e-b118d4093f54')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b83dec45-4a81-46ec-a68e-b118d4093f54 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.198362207739774,\n        \"min\": 0.0,\n        \"max\": 15.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          15.0,\n          7.0,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2521.7147826939395,\n        \"min\": 15.0,\n        \"max\": 7785.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5167.93,\n          4839.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 429.27170734487703,\n        \"min\": 15.0,\n        \"max\": 1298.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          888.2,\n          927.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_raw\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70.56648414033798,\n        \"min\": 15.0,\n        \"max\": 240.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          70.73,\n          51.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 627.8271069786194,\n        \"min\": 15.0,\n        \"max\": 1946.25,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1291.98,\n          1209.75,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"page_sentence_count_spacy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82.62558977528866,\n        \"min\": 15.0,\n        \"max\": 268.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          56.07,\n          36.0,\n          15.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_chunks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.922716838436736,\n        \"min\": 0.52,\n        \"max\": 15.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.13,\n          3.0,\n          0.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Splitting each chunk into its own item\n"
      ],
      "metadata": {
        "id": "x8wdyhf36s5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_and_texts):\n",
        "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "        chunk_dict = {}\n",
        "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
        "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \",\" \").strip()\n",
        "        joined_sentence_chunk = re.sub(r'\\.(A-Z)', r'. \\1', joined_sentence_chunk) # convert \".A\"to \". A\"(only for capital letter)\n",
        "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4\n",
        "\n",
        "        pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "len(pages_and_chunks)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:49.9694Z",
          "iopub.execute_input": "2024-10-17T17:21:49.970106Z",
          "iopub.status.idle": "2024-10-17T17:21:49.996218Z",
          "shell.execute_reply.started": "2024-10-17T17:21:49.970064Z",
          "shell.execute_reply": "2024-10-17T17:21:49.995404Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f3d4cd80759c40af8525e2c958e26a83",
            "f505fb50f0534ce58db4fd1cc9658818",
            "699690705d244129858c6462f1bff509",
            "2ae4f430ef844a74b887bf0cbfe97919",
            "81e8a6c0adab4a949892f52e543c7c47",
            "a30e618bfad24276b19970e67fa4a269",
            "8b75e642358c4ab8a59bb9a190922669",
            "376ba7d272b248878ebc08a25086556e",
            "569ec45736ec4432b3275667d3091b65",
            "d4aa7565b1454046962468a8364355c9",
            "f9078e4457444e4592ff0fd20ff77119"
          ]
        },
        "id": "fzB7hcuO6s5e",
        "outputId": "c240005f-1d72-4a70-9437-4be4659fda53"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3d4cd80759c40af8525e2c958e26a83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_chunks, k=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:50.623408Z",
          "iopub.execute_input": "2024-10-17T17:21:50.62413Z",
          "iopub.status.idle": "2024-10-17T17:21:51.00086Z",
          "shell.execute_reply.started": "2024-10-17T17:21:50.62409Z",
          "shell.execute_reply": "2024-10-17T17:21:50.99984Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9QUQ9G3i6s5f",
        "outputId": "38ac4678-25c1-43fa-d3c5-9553722797ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 1,\n",
              "  'sentence_chunk': 'This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z∗and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning ﬁelds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and ﬁrst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speciﬁcally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]–[11].Therefore, our proposed method is named as “label noise handling via side information” (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z∗on the example features (i.e., T = X Z∗), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classiﬁcation, which does not need the one-versus-one or one-versus-the-rest operations; 2) sufﬁcient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classiﬁer parameter optimization into a uniﬁed framework.Due to the above merits, a reliable classiﬁer can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classiﬁer.II.RELATED WORK This section brieﬂy reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches ﬁrst detect and eliminate label noise and then conduct the standard supervised classiﬁcation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble ﬁl- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the inﬂuence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classiﬁcation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speciﬁc tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.',\n",
              "  'chunk_char_count': 6134,\n",
              "  'chunk_word_count': 926,\n",
              "  'chunk_token_count': 1533.5}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(pages_and_chunks)\n",
        "df.describe().round(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:51.244692Z",
          "iopub.execute_input": "2024-10-17T17:21:51.245673Z",
          "iopub.status.idle": "2024-10-17T17:21:51.27267Z",
          "shell.execute_reply.started": "2024-10-17T17:21:51.245627Z",
          "shell.execute_reply": "2024-10-17T17:21:51.271658Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "GuLWeC_66s5f",
        "outputId": "33513686-6f1f-494a-bb20-0564ed0c388f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       page_number  chunk_char_count  chunk_word_count  chunk_token_count\n",
              "count        17.00             17.00             17.00              17.00\n",
              "mean          7.71           4517.41            741.29            1129.35\n",
              "std           4.63           1417.67            231.79             354.42\n",
              "min           0.00           1922.00            253.00             480.50\n",
              "25%           4.00           3607.00            627.00             901.75\n",
              "50%           8.00           4333.00            814.00            1083.25\n",
              "75%          12.00           5685.00            909.00            1421.25\n",
              "max          14.00           7014.00           1001.00            1753.50"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8799ec60-251b-4b70-8ff8-ea2a491b4a25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>chunk_char_count</th>\n",
              "      <th>chunk_word_count</th>\n",
              "      <th>chunk_token_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17.00</td>\n",
              "      <td>17.00</td>\n",
              "      <td>17.00</td>\n",
              "      <td>17.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>7.71</td>\n",
              "      <td>4517.41</td>\n",
              "      <td>741.29</td>\n",
              "      <td>1129.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>4.63</td>\n",
              "      <td>1417.67</td>\n",
              "      <td>231.79</td>\n",
              "      <td>354.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1922.00</td>\n",
              "      <td>253.00</td>\n",
              "      <td>480.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>4.00</td>\n",
              "      <td>3607.00</td>\n",
              "      <td>627.00</td>\n",
              "      <td>901.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>8.00</td>\n",
              "      <td>4333.00</td>\n",
              "      <td>814.00</td>\n",
              "      <td>1083.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>12.00</td>\n",
              "      <td>5685.00</td>\n",
              "      <td>909.00</td>\n",
              "      <td>1421.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.00</td>\n",
              "      <td>7014.00</td>\n",
              "      <td>1001.00</td>\n",
              "      <td>1753.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8799ec60-251b-4b70-8ff8-ea2a491b4a25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8799ec60-251b-4b70-8ff8-ea2a491b4a25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8799ec60-251b-4b70-8ff8-ea2a491b4a25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-309e8f74-da24-440d-8057-d788128004a1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-309e8f74-da24-440d-8057-d788128004a1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-309e8f74-da24-440d-8057-d788128004a1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.644802286807117,\n        \"min\": 0.0,\n        \"max\": 17.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          7.71,\n          8.0,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2323.4041205339818,\n        \"min\": 17.0,\n        \"max\": 7014.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4517.41,\n          4333.0,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 361.2174033538726,\n        \"min\": 17.0,\n        \"max\": 1001.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          741.29,\n          814.0,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 578.0806686601546,\n        \"min\": 17.0,\n        \"max\": 1753.5,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1129.35,\n          1083.25,\n          17.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        " df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:55.860675Z",
          "iopub.execute_input": "2024-10-17T17:21:55.861045Z",
          "iopub.status.idle": "2024-10-17T17:21:55.873259Z",
          "shell.execute_reply.started": "2024-10-17T17:21:55.86101Z",
          "shell.execute_reply": "2024-10-17T17:21:55.872191Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "34V_EEjB6s5f",
        "outputId": "c9bd193e-d27e-49dd-a586-b358c14c8259"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   page_number                                     sentence_chunk  \\\n",
              "0            0  This article has been accepted for inclusion i...   \n",
              "1            1  This article has been accepted for inclusion i...   \n",
              "2            2  This article has been accepted for inclusion i...   \n",
              "3            3  This article has been accepted for inclusion i...   \n",
              "4            4  This article has been accepted for inclusion i...   \n",
              "\n",
              "   chunk_char_count  chunk_word_count  chunk_token_count  \n",
              "0              7014              1001            1753.50  \n",
              "1              6134               926            1533.50  \n",
              "2              6107              1000            1526.75  \n",
              "3              4871               897            1217.75  \n",
              "4              4136               805            1034.00  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0d510c8-bdb2-4b63-a7de-c4d27c94e077\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>sentence_chunk</th>\n",
              "      <th>chunk_char_count</th>\n",
              "      <th>chunk_word_count</th>\n",
              "      <th>chunk_token_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>7014</td>\n",
              "      <td>1001</td>\n",
              "      <td>1753.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6134</td>\n",
              "      <td>926</td>\n",
              "      <td>1533.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6107</td>\n",
              "      <td>1000</td>\n",
              "      <td>1526.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4871</td>\n",
              "      <td>897</td>\n",
              "      <td>1217.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4136</td>\n",
              "      <td>805</td>\n",
              "      <td>1034.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0d510c8-bdb2-4b63-a7de-c4d27c94e077')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0d510c8-bdb2-4b63-a7de-c4d27c94e077 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0d510c8-bdb2-4b63-a7de-c4d27c94e077');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-666011f3-86ca-4d8a-8fa1-6a7e3b3c4e38\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-666011f3-86ca-4d8a-8fa1-6a7e3b3c4e38')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-666011f3-86ca-4d8a-8fa1-6a7e3b3c4e38 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classi\\ufb01cation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract\\u2014Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classi\\ufb01er training and severely decrease the classi\\ufb01cation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classi\\ufb01cation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as \\u201clabel noise handling via side information\\u201d (LNSI).Speci\\ufb01cally, the observed label matrix is decomposed as the sum of two parts, in which the \\ufb01rst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been suf\\ufb01ciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the \\u201cSummit of the Six Top Talents\\u201d Program under Grant DZXX-027, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by Jiangsu Province, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the \\u201c111\\u201d Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi\\u2019an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Arti\\ufb01cial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the \\ufb01gures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identi\\ufb01er 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classi\\ufb01cation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets con\\ufb01rm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms\\u2014Classi\\ufb01cation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classi\\ufb01er, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the suf\\ufb01- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classi\\ufb01cation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signi\\ufb01cantly mislead the classi\\ufb01er training and then severely decrease the classi\\ufb01cation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signi\\ufb01cance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]\\u2013[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classi\\ufb01cation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite dif\\ufb01cult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X \\u00a9 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z\\u2217and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning \\ufb01elds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and \\ufb01rst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speci\\ufb01cally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]\\u2013[11].Therefore, our proposed method is named as \\u201clabel noise handling via side information\\u201d (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z\\u2217on the example features (i.e., T = X Z\\u2217), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classi\\ufb01cation, which does not need the one-versus-one or one-versus-the-rest operations; 2) suf\\ufb01cient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classi\\ufb01er parameter optimization into a uni\\ufb01ed framework.Due to the above merits, a reliable classi\\ufb01er can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classi\\ufb01er.II.RELATED WORK This section brie\\ufb02y reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches \\ufb01rst detect and eliminate label noise and then conduct the standard supervised classi\\ufb01cation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble \\ufb01l- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the in\\ufb02uence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classi\\ufb01cation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speci\\ufb01c tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Algorithm 1 Algorithm for Solving LNSI Input: feature matrix X, observed label matrix Y; trade-off parameters: \\u03bb1, \\u03bb2, and \\u03bb3; Z = O, J = Z, E = O, B = O, M1 = O, M2 = O, M3 = O; \\u03bc = 10\\u22123, \\u03bcmax = 106, \\u03c1 = 1.2, \\u03f5 = 10\\u22126, iter_max = 1000; iter = 0; 1: Construct graph G and calculate the Laplacian matrix L via (3); 2: while not converge do 3: Update Z via (10), 4: Update E via (14), 5: Update J via (16), 6: Update B via (19), 7: Update the multipliers M1 := M1 + \\u03bc(Y \\u2212B \\u2212E), M2 := M2 + \\u03bc(B \\u2212X J), M3 := M3 + \\u03bc(Z \\u2212J), 8: Update the parameter \\u03bc by \\u03bc := min(\\u03c1\\u03bc, \\u03bcmax), 9: iter := iter + 1, 10: Check the convergence conditions: \\u2225Y \\u2212B\\u2212E\\u2225F \\u2264\\u03f5 and \\u2225B\\u2212X J\\u2225F \\u2264\\u03f5 and \\u2225Z\\u2212J\\u2225F \\u2264 \\u03f5; or iter > iter_max.11: end while Output: optimized Z\\u2217and E\\u2217. B. Computational Complexity This section studies the computational complexity of Algorithm 1.The graph construction in Line 1 of Algorithm 1 takes O(n2) complexity.Line 3 is accomplished by using the SVD, of which the complexity is O(min(d2c, dc2)).In Line 4, one should compute the \\u21132-norm of each row of a n \\u00d7 c matrix E, so the complexity is O(nc).Note that a d \\u00d7 d matrix should be inverted in Line 5, so the complexity of this step is O(d3).Therefore, the total complexity of our proposed algorithm is O(n2 +(min(d2c, dc2)+nc+d3)k) by assuming that Lines 2\\u20139 are iterated k times.Note that the complexity of Algorithm 1 is squared to the number of training examples n, so its complexity is acceptable.C. Generalization Bound In this section, we derive the generalization bound of LNSI.1) Preliminaries: Recall that our goal is to \\ufb01nd a suit- able project matrix Z by recovering the clean label matrix X Z, given the observed noisy label matrix Y and example features X. Similar to [10], (6) can be reformulated to the following expression with hard constraints, namely: min Z,E \\u0002 (i, j)\\u2208{1,...,n}\\u00d7{1,...,c} \\u2113((X Z + E)ij , Y ij ) s.t.\\u2225Z\\u2225\\u2217\\u2264Z\\u2217, \\u2225Z\\u22252 F \\u2264ZF, X Z \\u2208[\\u22121, 1]n\\u00d7c tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1. (21) Let \\u03b8 = (Z, E) be any feasible solution, and \\r = {(Z, E) | \\u2225Z\\u2225\\u2217 \\u2264 Z\\u2217, \\u2225Z\\u2225F \\u2264 \\u221aZF, X Z \\u2208 [\\u22121, 1]n\\u00d7c, tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1} be the feasible solution set.Also, let f\\u03b8(i, j) = Xi ZI j + Eij be the estimation function for Yij parameterized by \\u03b8 = (Z, E), and F\\r = { f\\u03b8 | \\u03b8 \\u2208\\r} be the set of feasible functions.I j is the jth column of identity matrix I \\u2208Rc\\u00d7c.We are interested in the following two \\u201c\\u2113-risk\\u201d quantities: 1) expected \\u2113-risk: R\\u2113( f ) = Ei, j [\\u2113( f (i, j), Yij )]; 2) empirical \\u2113-risk: \\u02c6R\\u2113( f ) = (1/nr) \\u0011 (i, j) \\u2113( f (i, j)Yij ), where nr is the number of observed entries.Thus, LNSI is to \\ufb01nd a proper \\u03b8\\u2217= (Z\\u2217, E\\u2217) that parameterizes f \\u2217= arg min f \\u2208F\\r \\u02c6R\\u2113( f ).2) Generalization Bound of LNSI: To bound the generaliza- tion error of LNSI, we \\ufb01rst link the quality of training labels to Rademacher complexity, which theoretically measures the complexity of a function class.We will show that high-quality labels of training examples will result in a lower model complexity and thus a smaller error bound.To begin with, we apply the following lemma to bound the expected \\u2113-risk.Lemma 3 (Bound on Expected \\u2113-Risk [39]): Let \\u2113be the loss function bounded by B with Lipschitz constant L\\u2113, and \\u03b4 be a constant where 0 < \\u03b4 < 1.With probability at least 1 \\u2212\\u03b4, we have max f \\u2208F |R\\u2113( f ) \\u2212\\u02c6R\\u2113( f )| \\u22642L\\u2113Rn(F) + B \\u0012 ln(1/\\u03b4) 2nr where Rn(F) := E[R(F)] is the Rademacher complexity of the function class F and R(F) := E\\u03c3 \\u0013 sup f \\u2208F 1 nr nr \\u0002 \\u03b1=1 \\u03c3\\u03b1 f (\\u03b1) \\u0014 is the empirical Rademacher complexity on the training examples.Note that \\u03c3\\u03b1 (\\u03b1 = 1, 2, . . . ,nr) are independent identically distributed (i.i.d.)Rademacher random variables.Given Lemma 3, we see that the key to derive the upper bound of a function f \\u2208F is to bound the complexity Rn(F\\r).More formally, the Rademacher complexity can be bounded in terms of the constraints in (21).Before diving into the details, we \\ufb01rst provide several useful theorems and lemmas.Lemma 4 (Complexity Bound [40]): Let S be a closed con- vex set and let F : S \\u2192R be \\u03b2-strongly convex with respect to \\u2225\\u00b7 \\u2225. In addition, we assume that F\\u22c6(O) = 0 with F\\u22c6being the Fenchel conjugate of function F. Further, let A = {A : \\u2225A\\u2225\\u22c6\\u2264A} and de\\ufb01ne W = {W \\u2208S : F(W) \\u2264Fmax}.Considering the class of linear functions F = {A \\u2192 W, A : W \\u2208W}, we have R(F) \\u2264A \\u0012 2Fmax \\u03b2nr (22) where W, A = tr(W\\u22a4A).Lemma 5 [41]: The function F : Rn\\u00d7c \\u2192R de\\ufb01ned as F(W) = (1/2)\\u2225W\\u22252 2,q for q = (ln(c)/(ln(c) \\u22121)) is (1/(3 ln(c)))-strongly convex with respect to \\u2225\\u00b7\\u22252,1 over Rn\\u00d7c.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1417,\n        \"min\": 1922,\n        \"max\": 7014,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          7014,\n          6134,\n          4805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 231,\n        \"min\": 253,\n        \"max\": 1001,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1001,\n          926,\n          905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 354.4185164950596,\n        \"min\": 480.5,\n        \"max\": 1753.5,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1753.5,\n          1533.5,\n          1201.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "min_token_length = 30\n",
        "# for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
        "#     print(f'Chunk token count : {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:21:56.320451Z",
          "iopub.execute_input": "2024-10-17T17:21:56.320869Z",
          "iopub.status.idle": "2024-10-17T17:21:56.325541Z",
          "shell.execute_reply.started": "2024-10-17T17:21:56.320829Z",
          "shell.execute_reply": "2024-10-17T17:21:56.324388Z"
        },
        "trusted": true,
        "id": "fnn-U5EO6s5f"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "#filtering rows with token under 30\n",
        "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
        "pages_and_chunks_over_min_token_len[:2]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:22:05.139959Z",
          "iopub.execute_input": "2024-10-17T17:22:05.140624Z",
          "iopub.status.idle": "2024-10-17T17:22:05.15195Z",
          "shell.execute_reply.started": "2024-10-17T17:22:05.140557Z",
          "shell.execute_reply": "2024-10-17T17:22:05.150861Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "r42N_zv-6s5f",
        "outputId": "f04d5a02-feab-48dc-e1b1-97d02cbe4496"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 0,\n",
              "  'sentence_chunk': 'This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classiﬁcation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract—Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classiﬁer training and severely decrease the classiﬁcation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classiﬁcation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as “label noise handling via side information” (LNSI).Speciﬁcally, the observed label matrix is decomposed as the sum of two parts, in which the ﬁrst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufﬁciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the “Summit of the Six Top Talents” Program under Grant DZXX-027, in part by the “Young Elite Scientists Sponsorship Program” by Jiangsu Province, in part by the “Young Elite Scientists Sponsorship Program” by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the “111” Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Artiﬁcial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identiﬁer 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classiﬁcation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets conﬁrm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms—Classiﬁcation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classiﬁer, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the sufﬁ- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classiﬁcation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signiﬁcantly mislead the classiﬁer training and then severely decrease the classiﬁcation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signiﬁcance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]–[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classiﬁcation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite difﬁcult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X © 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.',\n",
              "  'chunk_char_count': 7014,\n",
              "  'chunk_word_count': 1001,\n",
              "  'chunk_token_count': 1753.5},\n",
              " {'page_number': 1,\n",
              "  'sentence_chunk': 'This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z∗and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning ﬁelds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and ﬁrst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speciﬁcally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]–[11].Therefore, our proposed method is named as “label noise handling via side information” (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z∗on the example features (i.e., T = X Z∗), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classiﬁcation, which does not need the one-versus-one or one-versus-the-rest operations; 2) sufﬁcient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classiﬁer parameter optimization into a uniﬁed framework.Due to the above merits, a reliable classiﬁer can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classiﬁer.II.RELATED WORK This section brieﬂy reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches ﬁrst detect and eliminate label noise and then conduct the standard supervised classiﬁcation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble ﬁl- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the inﬂuence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classiﬁcation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speciﬁc tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.',\n",
              "  'chunk_char_count': 6134,\n",
              "  'chunk_word_count': 926,\n",
              "  'chunk_token_count': 1533.5}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "random.sample(pages_and_chunks_over_min_token_len, k=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:22:06.034128Z",
          "iopub.execute_input": "2024-10-17T17:22:06.03497Z",
          "iopub.status.idle": "2024-10-17T17:22:06.041243Z",
          "shell.execute_reply.started": "2024-10-17T17:22:06.034929Z",
          "shell.execute_reply": "2024-10-17T17:22:06.040268Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c3VdN8f6s5f",
        "outputId": "84415a44-98bf-4e1d-94b8-686469b95ed2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 13,\n",
              "  'sentence_chunk': 'This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS It can be easily veriﬁed that the above problem (47) can be represented in the form of (40) by setting P = \\x18B Z \\x19 , Q = ⎡ ⎣ E J K ⎤ ⎦ (49) and AP = ⎡ ⎢⎢⎣ I O I O I O O I ⎤ ⎥⎥⎦, BQ = ⎡ ⎢⎢⎣ I O O O −X O O O −I O −I O ⎤ ⎥⎥⎦, C = ⎡ ⎢⎢⎣ Y O O O ⎤ ⎥⎥⎦ (50) where I and O are the identity matrices and zero matrices with proper sizes, respectively.The functions f (P) and g( Q) in (40) can be, respectively, expressed as f (P) = ∥Z∥∗+ λ1∥Z∥2 F (51) g( Q) = λ2tr((X J)⊤L(X J)) + λ3∥E∥2,1 + IC(K). (52) The unaugmented Lagrangian is formulated as L0 = ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 + tr(M⊤ 1 (Y −B −E)) + tr\\x03M⊤ 2 (B −X J)\\x04 + tr \\x03 M⊤ 3 (Z −J) \\x04 . (53) Obviously, both f (P) and g( Q) are closed, proper, and convex, and the unaugmented Lagrangian L0 has a saddle point, which demonstrate that the optimization process for (7) is convergent.□ REFERENCES [1] R. J. Hickey, “Noise modelling and evaluating learning from examples,” Artif.Intell.,vol.82, nos.1–2, pp.157–179, 1996. [2] C. Gong, H. Zhang, J. Yang, and D. Tao, “Learning with inadequate and incorrect supervision,” in Proc.Int.Conf.Data Mining, Nov. 2017, pp.889–894. [3] W. Gao, L. Wang, Y.-F. Li, and Z.-H. Zhou, “Risk minimization in the presence of label noise,” in Proc.AAAI Conf.Artif.Intell.,2016, pp.1575–1581. [4] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learning with noisy labels,” in Proc.Adv.Neural Inf.Process.Syst.,2013, pp.1196–1204. [5] N. Manwani and P. Sastry, “Noise tolerance under risk minimization,” IEEE Trans.Cybern.,vol.43, no.3, pp.1146–1151, Jun. 2013. [6] G. Patrini, F. Nielsen, R. Nock, and M. Carioni, “Loss factorization, weakly supervised learning and label noise robustness,” in Proc.Int.Conf.Mach.Learn.,2016, pp.708–717. [7] R. Wang, T. Liu, and D. Tao, “Multiclass learning with partially corrupted labels,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.6, pp.2568–2580, Jun. 2018. [8] P. Zhao, Y. Jiang, and Z.-H. Zhou, “Multi-view matrix completion for clustering with side information,” in Proc.Paciﬁc–Asia Conf.Knowl.Discovery Data Mining, 2017, pp.403–415. [9] M. Xu, R. Jin, and Z. Zhou, “Speedup matrix completion with side information: Application to multi-label learning,” in Proc.Adv.Neural Inf.Process.Syst.,2013, pp.2301–2309. [10] K.-Y. Chiang, C.-J. Hsieh, and I. S. Dhillon, “Matrix completion with noisy side information,” in Proc.Adv.Neural Inf.Process.Syst.,2015, pp.3447–3455. [11] Y. Guo, “Convex co-embedding for matrix completion with predic- tive side information,” in Proc.31st AAAI Conf.Artif.Intell.,2017, pp.1955–1961. [12] K.-Y. Chiang, C.-J. Hsieh, and I. Dhillon, “Robust principal component analysis with side information,” in Proc.Int.Conf.Mach.Learn.,2016, pp.2291–2299. [13] F. Muhlenbach, S. Lallich, and D. A. Zighed, “Identifying and handling mislabelled instances,” J. Intell.Inf.Syst.,vol.22, no.1, pp.89–109, 2004. [14] X. Zhu, X. Wu, and Q. Chen, “Eliminating class noise in large datasets,” in Proc.Int.Conf.Mach.Learn.,2003, pp.920–927. [15] B. van Rooyen, A. K. Menon, and R. C. Williamson, “Learning with symmetric label noise: The importance of being unhinged,” in Proc.Adv.Neural Inf.',\n",
              "  'chunk_char_count': 3357,\n",
              "  'chunk_word_count': 523,\n",
              "  'chunk_token_count': 839.25}]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Embedding our text chunks\n",
        "\n",
        "Embeddings of text will mean that similar meaning texts have similar numerical representation.\n",
        "\n",
        "\n",
        "Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n",
        "\n",
        "We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n",
        "\n",
        "We'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n",
        "\n",
        "Specifically, we'll get the `all-mpnet-base-v2` model ( [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."
      ],
      "metadata": {
        "id": "q2L15C4n6s5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers # for embedding models"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:22:08.710911Z",
          "iopub.execute_input": "2024-10-17T17:22:08.711314Z",
          "iopub.status.idle": "2024-10-17T17:22:20.220588Z",
          "shell.execute_reply.started": "2024-10-17T17:22:08.711274Z",
          "shell.execute_reply": "2024-10-17T17:22:20.219426Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNvgbMgd6s5f",
        "outputId": "810deaff-ac37-4c1d-9f99-2af400476395"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.32.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=\"cuda\")\n",
        "\n",
        "\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:22:20.222965Z",
          "iopub.execute_input": "2024-10-17T17:22:20.22342Z",
          "iopub.status.idle": "2024-10-17T17:22:22.509154Z",
          "shell.execute_reply.started": "2024-10-17T17:22:20.223369Z",
          "shell.execute_reply": "2024-10-17T17:22:22.508203Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525,
          "referenced_widgets": [
            "aea3bfcc8c234abaa6d9ac9a67bb822f",
            "ddac13736d5449d4bacda13fc6318aa4",
            "dd426df76dc54f28896f14fdc78b9865",
            "3525a51b6597485785b58c471fb5670b",
            "1132e2705ff946838764a5ebd39ee6f1",
            "ff75dbcffde24779982d452b475fcfe9",
            "3e4982932b9e45d991e9bc6e48a63db7",
            "92fbd2ea4bc647e08e240bda81fc1812",
            "c39a8f315b874e6e9dc1269bc11cdc81",
            "6cdf489b29e644c394293ae41f3528ab",
            "1b9675a25b1146de8e557f40fa7a0d3a",
            "22eb1ad113c440eebaca39ca005f4c38",
            "7d3e9c1ec30c4aa49db27cd4321cf49b",
            "edb08652f23a498f824a96e98a1cb157",
            "7190ab7c34f449dd898188e470623204",
            "7abdd556473d4f5a8289e3705e5034cf",
            "374d49fb0ea44f8a82125d228468b352",
            "2273104f1b8d4785b57498df48f215e7",
            "63946ebbf7934626847b9def822917b2",
            "9903d74f505e4fb79acc2cbd04908c11",
            "fc8421d6228149aea78c928309c9855c",
            "19debcad98d542e5b07b103e159b95cb",
            "2987b3b7b0d74e9aa7f9f3d7150d7e54",
            "8d6ee919744548938da89457d5febf67",
            "1f6c601f6e534a80853166ad08de14e7",
            "3a851740e0514df992df34f299593490",
            "ca6bed010c9e4c608fe73ebf8bb17ed8",
            "98091a42df1d4dae8b4fce3dc99cd710",
            "13712679714d4c489167433681d3046a",
            "a593054201604be788a4c76a3dc95a45",
            "884874da51e04727a67de7233b1c3df5",
            "784475da561b413480e8a88618acd77e",
            "25eae10840214cceb8a1657e6e65cfed",
            "8c36b997dc234f2a9cc33a372efdfdd5",
            "40179b536ffe4f1ca35611c4041083c5",
            "ac9d2a04b762483885307ce533c37eeb",
            "c4f65d06c4e34e9b817d3efaf2c76cfa",
            "280dda81615343ed8d92af59c03db315",
            "de1e164c373b4ce4b9617b6a30b15c63",
            "241c2d1624dd4fb29b21b129b559cd32",
            "350d63a90f9f42fb86a58bacac9a06ea",
            "ac163c44f4e84820929850640f9d6ce1",
            "0266f48fcc854ed0b54119bf4e535028",
            "cd1fa6c889eb461da149d372a7614284",
            "869797c73fe84ba78f8f74090a7886bb",
            "3469b0d8325e458b9449ae599bc3559f",
            "10a695234c954169b299b46b9033e1b8",
            "2cd6de721694423983b76b05e8fdec76",
            "708d8bfde5c24575a4b471da298640f2",
            "050de6dc596340609cbe652871e872c8",
            "8b1c534cca334ffd86b81299f6f867ec",
            "92e0c9dae34d4b92b3f6013ce6314fef",
            "717781c958264c96b954ef7c1584c409",
            "928a76c83174433ba8e23db4509aca5f",
            "2359c98bf2874153baedd93778fb1f26",
            "eb4e31c56ea84ac8a28e6dca71e8a3de",
            "01d07fda8d2c4077a2cb5ec8e5291a33",
            "8707dd9f0f054b09896f86cf631c436c",
            "7df78c9de0574f6ca1c3ac69c9c92e27",
            "4986152b4eeb499aaa82a8b829b39d7b",
            "da47b3c92f864fb99d63f6ce388d8701",
            "717f5f4168cf40199473fcad6ff96416",
            "d51e5e5f748344e5a1b2479a07b35739",
            "1285bfa576b14c269be103c0f49891db",
            "110937b8e0f44966bc98a764c3417d6d",
            "11b0a071caaa47f3be99bba588bd2895",
            "e9a19a989a174aeebc16ce427dae0410",
            "10c15905e61b4f2599112a1d53c9936a",
            "554495acc7dd47aeb195da87631179c9",
            "de498663710a4410b3b129ea99284f33",
            "d2509c908be841cba0b05867c6e56d6e",
            "3363e55e7b8c4f7280a6aa5ab0d5816b",
            "9f2fcd7c9a934540a97cf51c05b4b23b",
            "c160fe5f4d4648cda1a49565a667aacc",
            "de9b0547f8be468ea76dfe7924d0f642",
            "f80cd7c6fa22473ebccf1677cbdaf8dc",
            "ad91d230557444b19e6941718dd93c11",
            "ce5c2cb9432241f3bec883832f652574",
            "d8f56fb40fd2465696746bf6af7a5d3b",
            "2917931ea17448699e675d4d916737cd",
            "a507d6bca8dc44b3ba3a4abf7214e7fe",
            "9128fa39e3c14f119668b0c5779081c8",
            "d0eb00989ed54d68b83bf157299f1088",
            "c249793c260a4b8fa858850e72c53bf6",
            "5af833fc7ea54b8e84256051b04c9fa5",
            "df04652ceba24e9b900cdd622359f2d5",
            "ec51759b9c72425fb2b2f22339e1bd2d",
            "3f93ec99e0a543a79d0fe6d3d8e2bee7",
            "7b307da5f26b42d8abb4c4ed82f6c56e",
            "052a224a54be46e686437e53ad980d33",
            "3ed8be2f07b640d7afc906207e518d9e",
            "e9448f689a384c2f92985e7341535a1c",
            "c7b490885b634240968fd7c3ffb21ce5",
            "a9da138bc6f84e518761a9c7c244ad38",
            "d60f880b523a41919e21dde46a7b2c29",
            "a671e18f7ffd40dab448e2929fb9dc4f",
            "7433853a2e4c4d788975987ddcda5d02",
            "70c89197efa4496da98d16725ae87f70",
            "ff0aa0cf45ed4590a6b6840fb0f7b8b8",
            "58c22290961f483ba4715357c939acf9",
            "89dde1c55eec44c18310eb61cfff320c",
            "5319521d0dd848b09f17cdfb151705bb",
            "6667ac62ff54499bb060beb98b09214f",
            "b1d46f27f004462ead718c08f77f3eea",
            "34c29c1149c149ef861817a7ec5e6623",
            "13a142e538c941158aa3d0fbac389066",
            "94270a894da04d548b7a25ddf6dafaac",
            "800ef320d318428286332d7c7c236747",
            "a407f78e6ba14c52a088b4edd4e5c897",
            "91d48e31f01f4817b71b312eb00ab95a",
            "b99ae19a0f3b4da7845afda1d83a638c",
            "362a0804d2b5465c8d83cf3bc8babc67",
            "cda1cd1a5a7c4b7eb7314ecaf0565756",
            "cf29f9b4b2cf4cb287eeda8439616589",
            "9d8a6f7430e64e939bd8cfd8af7be03f",
            "e7cb72966b204a64a548d692f9f7f33d",
            "afec6a9f0d394c35aad0aadf45674145",
            "02e1a4abeb6d4f0ca3fc6e0de096dc28",
            "f57cbb9c096f4c0f9aa47077554a8982",
            "ac052000901e40398c14763820180cba",
            "fe7cdb2ca855468ca161c78dff38b971",
            "681b1073b94c47a6bfee4235f4aef9c1",
            "0759076db1444aa08e136a77385369c3",
            "ee0cd6df65ea45da8a0138ed3f4aeb88",
            "a680fd6bc43f4d8ea91e529502893751",
            "51e29ab7246748a99d2e80bee156930a",
            "f6ea3ffc9ebc4f4ca24adcca0f38ded5",
            "eb4de251da7d4e4b8b5953abc1eef71a",
            "55de6a1b865044098bfb08768f8dc49e",
            "8ebe3627e1c1415ba4151fbea5627930",
            "d353bcb98aea43ffa0f5c38842f94003",
            "5c6d91a4d00c432eb448adb07fa0beb5"
          ]
        },
        "id": "QeCKk_jp6s5f",
        "outputId": "f4c72563-db82-40bf-bedf-a093b3799f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aea3bfcc8c234abaa6d9ac9a67bb822f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22eb1ad113c440eebaca39ca005f4c38"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2987b3b7b0d74e9aa7f9f3d7150d7e54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c36b997dc234f2a9cc33a372efdfdd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "869797c73fe84ba78f8f74090a7886bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eb4e31c56ea84ac8a28e6dca71e8a3de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9a19a989a174aeebc16ce427dae0410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce5c2cb9432241f3bec883832f652574"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b307da5f26b42d8abb4c4ed82f6c56e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58c22290961f483ba4715357c939acf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b99ae19a0f3b4da7845afda1d83a638c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/17 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "681b1073b94c47a6bfee4235f4aef9c1"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Embeddings model"
      ],
      "metadata": {
        "id": "2cpvzNrJ6s5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the model\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
        "\n",
        "# Function to split a chunk into words and encode each word\n",
        "def get_word_embeddings(text):\n",
        "    words = text.split()  # Split text into individual words\n",
        "    embeddings = embedding_model.encode(words)  # Get embedding for each word\n",
        "    return words, embeddings\n",
        "\n",
        "# Process each chunk in your dataset\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    words, word_embeddings = get_word_embeddings(item[\"sentence_chunk\"])\n",
        "    item[\"words\"] = words\n",
        "    item[\"word_embeddings\"] = word_embeddings\n",
        "\n",
        "# Check shape of word embeddings for the first chunk\n",
        "print(len(pages_and_chunks_over_min_token_len[0][\"word_embeddings\"]),\n",
        "      pages_and_chunks_over_min_token_len[0][\"word_embeddings\"][0].shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:22:29.869751Z",
          "iopub.execute_input": "2024-10-17T17:22:29.870623Z",
          "iopub.status.idle": "2024-10-17T17:22:36.645693Z",
          "shell.execute_reply.started": "2024-10-17T17:22:29.870564Z",
          "shell.execute_reply": "2024-10-17T17:22:36.644775Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzrUoZSd6s5g",
        "outputId": "f2630b46-472e-429f-829c-b9260fb9c6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:07<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1001 (768,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Individual word embeddings"
      ],
      "metadata": {
        "id": "yeJFkSQV6s5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks_over_min_token_len[0][\"word_embeddings\"]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:22:41.757008Z",
          "iopub.execute_input": "2024-10-17T17:22:41.757669Z",
          "iopub.status.idle": "2024-10-17T17:22:41.764774Z",
          "shell.execute_reply.started": "2024-10-17T17:22:41.757613Z",
          "shell.execute_reply": "2024-10-17T17:22:41.763836Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI_oiT8O6s5q",
        "outputId": "11277916-9b89-447f-8416-ec41c78bb87d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.02399354,  0.02436408, -0.01130732, ...,  0.04851779,\n",
              "        -0.03059575, -0.06718535],\n",
              "       [ 0.02872143,  0.06235189, -0.00240356, ..., -0.02024206,\n",
              "        -0.06902064,  0.01966141],\n",
              "       [-0.02656304,  0.04583179, -0.00301339, ...,  0.03154208,\n",
              "        -0.06035382, -0.00432453],\n",
              "       ...,\n",
              "       [-0.02972006, -0.00399144,  0.00966241, ...,  0.01339654,\n",
              "        -0.02797396,  0.00976396],\n",
              "       [ 0.05336843,  0.08734006, -0.00663722, ...,  0.05458362,\n",
              "         0.01032661,  0.00404617],\n",
              "       [ 0.00064343,  0.02722497, -0.03459514, ..., -0.03032272,\n",
              "        -0.03990483, -0.01313225]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Document embedding based on TF-IDF"
      ],
      "metadata": {
        "id": "loztgHcU6s5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the SentenceTransformer model\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", device=\"cuda\")\n",
        "\n",
        "# Extract all sentence chunks to build the TF-IDF vocabulary\n",
        "corpus = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
        "\n",
        "# Fit the TF-IDF vectorizer on the corpus\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)\n",
        "\n",
        "# Function to get embeddings for each word in a chunk\n",
        "def get_word_embeddings(text):\n",
        "    words = text.split()  # Split text into individual words\n",
        "    embeddings = embedding_model.encode(words)  # Get 768-dim embedding for each word\n",
        "    return words, embeddings\n",
        "\n",
        "# Function to compute the TF-IDF weighted document embedding\n",
        "def compute_tfidf_weighted_embedding(words, word_embeddings):\n",
        "    # Get TF-IDF scores for the words in the current chunk\n",
        "    tfidf_scores = vectorizer.transform([\" \".join(words)]).toarray()[0]\n",
        "    word_to_tfidf = dict(zip(vectorizer.get_feature_names_out(), tfidf_scores))\n",
        "\n",
        "    # Initialize the document embedding with zeros\n",
        "    document_embedding = np.zeros(word_embeddings[0].shape)  # Shape: (768,)\n",
        "    total_weight = 0  # To normalize by total weight\n",
        "\n",
        "    # Compute the weighted sum of word embeddings\n",
        "    for i, word in enumerate(words):\n",
        "        if word.lower() in word_to_tfidf:  # Match word with TF-IDF vocab\n",
        "            weight = word_to_tfidf[word.lower()]\n",
        "            document_embedding += weight * word_embeddings[i]\n",
        "            total_weight += weight\n",
        "\n",
        "    # Normalize the document embedding by total weight\n",
        "    if total_weight > 0:\n",
        "        document_embedding /= total_weight\n",
        "\n",
        "    return document_embedding\n",
        "\n",
        "# Process each chunk in your dataset and store results back into the dictionaries\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    # Get words and word embeddings\n",
        "    words, word_embeddings = get_word_embeddings(item[\"sentence_chunk\"])\n",
        "    item[\"words\"] = words\n",
        "    item[\"word_embeddings\"] = word_embeddings\n",
        "\n",
        "    # Compute the TF-IDF weighted document embedding\n",
        "    item[\"document_embedding\"] = compute_tfidf_weighted_embedding(words, word_embeddings)\n",
        "\n",
        "# Check the shape of word embeddings for the first chunk\n",
        "first_chunk = pages_and_chunks_over_min_token_len[0]\n",
        "print(f\"Number of word embeddings: {len(first_chunk['word_embeddings'])}\")\n",
        "print(f\"Shape of first word embedding: {first_chunk['word_embeddings'][0].shape}\")\n",
        "print(f\"Shape of document embedding: {first_chunk['document_embedding'].shape}\")\n",
        "\n",
        "# Optional: Display a sample chunk's words and embeddings (first 5 words)\n",
        "print(f\"First 5 words: {first_chunk['words']}\")\n",
        "print(f\"First word embedding (sample): {first_chunk['word_embeddings'][0]}\")  # Print first 5 values for readability\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:23:27.36Z",
          "iopub.execute_input": "2024-10-17T17:23:27.360947Z",
          "iopub.status.idle": "2024-10-17T17:23:34.496563Z",
          "shell.execute_reply.started": "2024-10-17T17:23:27.360901Z",
          "shell.execute_reply": "2024-10-17T17:23:34.495651Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnwR_ILT6s5q",
        "outputId": "968c62af-c9d9-4635-b651-6fcf337be8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:06<00:00,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of word embeddings: 1001\n",
            "Shape of first word embedding: (768,)\n",
            "Shape of document embedding: (768,)\n",
            "First 5 words: ['This', 'article', 'has', 'been', 'accepted', 'for', 'inclusion', 'in', 'a', 'future', 'issue', 'of', 'this', 'journal.Content', 'is', 'final', 'as', 'presented,', 'with', 'the', 'exception', 'of', 'pagination.IEEE', 'TRANSACTIONS', 'ON', 'NEURAL', 'NETWORKS', 'AND', 'LEARNING', 'SYSTEMS', '1', 'Harnessing', 'Side', 'Information', 'for', 'Classiﬁcation', 'Under', 'Label', 'Noise', 'Yang', 'Wei', ',', 'Chen', 'Gong', ',', 'Member,', 'IEEE,', 'Shuo', 'Chen', ',', 'Tongliang', 'Liu', ',', 'Member,', 'IEEE,', 'Jian', 'Yang', ',', 'Member,', 'IEEE,', 'and', 'Dacheng', 'Tao', ',', 'Fellow,', 'IEEE', 'Abstract—Practical', 'data', 'sets', 'often', 'contain', 'the', 'label', 'noise', 'caused', 'by', 'various', 'human', 'factors', 'or', 'measurement', 'errors,', 'which', 'means', 'that', 'a', 'fraction', 'of', 'training', 'examples', 'might', 'be', 'mistakenly', 'labeled.Such', 'noisy', 'labels', 'will', 'mislead', 'the', 'classiﬁer', 'training', 'and', 'severely', 'decrease', 'the', 'classiﬁcation', 'performance.Existing', 'approaches', 'to', 'handle', 'this', 'problem', 'are', 'usually', 'developed', 'through', 'various', 'surrogate', 'loss', 'functions', 'under', 'the', 'framework', 'of', 'empiri-', 'cal', 'risk', 'minimization.However,', 'they', 'are', 'only', 'suitable', 'for', 'binary', 'classiﬁcation', 'and', 'also', 'require', 'strong', 'prior', 'knowledge.Therefore,', 'this', 'article', 'treats', 'the', 'example', 'features', 'as', 'side', 'information', 'and', 'formulates', 'the', 'noisy', 'label', 'removal', 'problem', 'as', 'a', 'matrix', 'recovery', 'problem.We', 'denote', 'our', 'proposed', 'method', 'as', '“label', 'noise', 'handling', 'via', 'side', 'information”', '(LNSI).Speciﬁcally,', 'the', 'observed', 'label', 'matrix', 'is', 'decomposed', 'as', 'the', 'sum', 'of', 'two', 'parts,', 'in', 'which', 'the', 'ﬁrst', 'part', 'reveals', 'the', 'true', 'labels', 'and', 'can', 'be', 'obtained', 'by', 'conducting', 'a', 'low-rank', 'mapping', 'on', 'the', 'side', 'information;', 'and', 'the', 'second', 'part', 'captures', 'the', 'incorrect', 'labels', 'and', 'is', 'modeled', 'by', 'a', 'row-sparse', 'matrix.The', 'merits', 'of', 'such', 'formulation', 'lie', 'in', 'three', 'aspects:', '1)', 'the', 'strong', 'recovery', 'ability', 'of', 'this', 'strategy', 'has', 'been', 'sufﬁciently', 'demonstrated', 'by', 'intensive', 'theoretical', 'works', 'on', 'side', 'information;', '2)', 'multi-class', 'situations', 'can', 'be', 'directly', 'handled', 'Manuscript', 'received', 'August', '13,', '2018;', 'revised', 'January', '17,', '2019', 'and', 'April', '11,', '2019;', 'accepted', 'August', '26,', '2019.This', 'work', 'was', 'supported', 'by', 'NSF', 'of', 'China', 'under', 'Grant', '61602246,', 'Grant', '61973162,', 'and', 'Grant', 'U1713208,', 'in', 'part', 'by', 'NSF', 'of', 'Jiangsu', 'Province', 'under', 'Grant', 'BK20171430,', 'in', 'part', 'by', 'the', 'Fundamental', 'Research', 'Funds', 'for', 'the', 'Central', 'Universities', 'under', 'Grant', '30918011319,', 'in', 'part', 'by', 'the', 'Open', 'Project', 'of', 'the', 'State', 'Key', 'Laboratory', 'of', 'Integrated', 'Services', 'Networks', 'through', 'Xidian', 'University', 'under', 'Grant', 'ISN19-', '03,', 'in', 'part', 'by', 'the', '“Summit', 'of', 'the', 'Six', 'Top', 'Talents”', 'Program', 'under', 'Grant', 'DZXX-027,', 'in', 'part', 'by', 'the', '“Young', 'Elite', 'Scientists', 'Sponsorship', 'Program”', 'by', 'Jiangsu', 'Province,', 'in', 'part', 'by', 'the', '“Young', 'Elite', 'Scientists', 'Sponsorship', 'Program”', 'by', 'CAST', 'under', 'Grant', '2018QNRC001,', 'in', 'part', 'by', 'the', 'Program', 'for', 'Changjiang', 'Scholars,', 'in', 'part', 'by', 'the', '“111”', 'Program', 'AH92005,', 'in', 'part', 'by', 'the', 'ARC', 'FL-170100117,', 'in', 'part', 'by', 'DP180103424,', 'and', 'in', 'part', 'by', 'DE190101473.', '(Corresponding', 'author:', 'Chen', 'Gong.)Y.', 'Wei', 'and', 'C.', 'Gong', 'are', 'with', 'the', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nanjing', '210094,', 'China,', 'and', 'also', 'with', 'the', 'State', 'Key', 'Laboratory', 'of', 'Integrated', 'Services', 'Networks,', 'Xidian', 'University,', 'Xi’an,', 'China', '(e-mail:', 'csywei@njust.edu.cn;', 'chen.gong@njust.edu.cn).S.', 'Chen', 'and', 'J.', 'Yang', 'are', 'with', 'the', 'PCA', 'Laboratory,', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nanjing', '210094,', 'China,', 'with', 'the', 'Key', 'Laboratory', 'of', 'Intelligent', 'Perception', 'and', 'Systems', 'for', 'High-Dimensional', 'Information', 'of', 'Ministry', 'of', 'Education,', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nanjing', '210094,', 'China,', 'and', 'also', 'with', 'the', 'Jiangsu', 'Key', 'Laboratory', 'of', 'Image', 'and', 'Video', 'Understanding', 'for', 'Social', 'Security,', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nan-', 'jing', '210094,', 'China', '(e-mail:', 'shuochen@njust.edu.cn;', 'csjyang@njust.edu.cn).T.', 'Liu', 'and', 'D.', 'Tao', 'are', 'with', 'the', 'UBTECH', 'Sydney', 'Artiﬁcial', 'Intelligence', 'Centre,', 'School', 'of', 'Computer', 'Science,', 'Faculty', 'of', 'Engineer-', 'ing,', 'The', 'University', 'of', 'Sydney,', 'Darlington,', 'NSW', '2008,', 'Australia', '(e-mail:', 'tongliang.liu@sydney.edu.au;', 'dacheng.tao@sydney.edu.au).Color', 'versions', 'of', 'one', 'or', 'more', 'of', 'the', 'ﬁgures', 'in', 'this', 'article', 'are', 'available', 'online', 'at', 'http://ieeexplore.ieee.org.Digital', 'Object', 'Identiﬁer', '10.1109/TNNLS.2019.2938782', 'with', 'the', 'aid', 'of', 'learned', 'projection', 'matrix;', 'and', '3)', 'only', 'very', 'weak', 'assumptions', 'are', 'required', 'for', 'model', 'design,', 'making', 'LNSI', 'applicable', 'to', 'a', 'wide', 'range', 'of', 'practical', 'problems.Moreover,', 'we', 'theoretically', 'derive', 'the', 'generalization', 'bound', 'of', 'LNSI', 'and', 'show', 'that', 'the', 'expected', 'classiﬁcation', 'error', 'of', 'LNSI', 'is', 'upper', 'bounded.The', 'experimental', 'results', 'on', 'a', 'variety', 'of', 'data', 'sets', 'including', 'UCI', 'benchmark', 'data', 'sets', 'and', 'practical', 'data', 'sets', 'conﬁrm', 'the', 'superiority', 'of', 'LNSI', 'to', 'state-of-the-art', 'approaches', 'on', 'label', 'noise', 'handling.Index', 'Terms—Classiﬁcation,', 'generalization', 'bound,', 'label', 'noise,', 'matrix', 'recovery,', 'side', 'information.I.', 'INTRODUCTION', 'T', 'RADITIONALLY,', 'a', 'reliable', 'supervised', 'classiﬁer,', 'such', 'as', 'support', 'vector', 'machines', '(SVMs)', 'or', 'convolutional', 'neural', 'networks', '(CNNs),', 'is', 'usually', 'trained', 'based', 'on', 'the', 'sufﬁ-', 'cient', 'correctly', 'labeled', 'data.Unfortunately,', 'the', 'real-world', 'data', 'sets', 'often', 'contain', 'the', 'noise', 'in', 'label', 'space,', 'which', 'means', 'that', 'a', 'fraction', 'of', 'training', 'examples', 'are', 'erroneously', 'labeled', '[1].For', 'instance,', 'as', 'the', 'numerous', 'examples', 'in', 'many', 'applications', '(e.g.,', 'image', 'classiﬁcation', 'and', 'document', 'categorization)', 'are', 'manu-', 'ally', 'annotated,', 'the', 'labeling', 'errors', 'are', 'inevitably', 'introduced', 'due', 'to', 'the', 'human', 'fatigue.Disease', 'diagnosis,', 'in', 'which', 'the', 'decision', 'is', 'strongly', 'dependent', 'on', 'the', 'experience', 'and', 'expertise', 'of', 'the', 'doctors,', 'is', 'also', 'very', 'likely', 'to', 'include', 'labeling', 'errors.These', 'noisy', 'labels', 'will', 'signiﬁcantly', 'mislead', 'the', 'classiﬁer', 'training', 'and', 'then', 'severely', 'decrease', 'the', 'classiﬁcation', 'performance', '[2].Hence,', 'designing', 'algorithms', 'that', 'account', 'for', 'the', 'data', 'with', 'noisy', 'labels', 'is', 'of', 'great', 'signiﬁcance', 'and', 'has', 'become', 'a', 'critical', 'issue', 'in', 'the', 'machine', 'learning', 'community.Several', 'approaches', 'have', 'been', 'proposed', 'to', 'deal', 'with', 'the', 'learning', 'problem', 'with', 'label', 'noise', 'to', 'prevent', 'the', 'performance', 'decrease,', 'and', 'most', 'of', 'them', 'are', 'based', 'on', 'the', 'minimization', 'of', 'empirical', 'risk', 'via', 'a', 'conditional', 'probability', 'model', '[3]–[6].For', 'example,', 'Gao', 'et', 'al.', '[3]', 'and', 'Patrini', 'et', 'al.', '[6]', 'analyze', 'the', 'empirical', 'risk', 'minimization', 'in', 'the', 'presence', 'of', 'label', 'noise', 'by', 'decomposing', 'the', 'loss', 'function', 'into', 'a', 'label-independent', 'part', 'and', 'a', 'label-dependent', 'part.Manwani', 'and', 'Sastry', '[5]', 'study', 'the', 'noise', 'tolerance', 'properties', 'of', 'risk', 'minimization', 'under', 'differ-', 'ent', 'loss', 'functions', 'and', 'provide', 'insightful', 'theoretical', 'results.However,', 'they', 'are', 'only', 'applicable', 'to', 'binary', 'classiﬁcation', 'and', 'the', 'extension', 'to', 'multi-class', 'is', 'nontrivial', '[7].Moreover,', 'these', 'methods', 'require', 'the', 'estimation', 'of', 'class', 'prior,', 'which', 'is', 'actually', 'quite', 'difﬁcult', 'in', 'the', 'presence', 'of', 'corrupted', 'observed', 'data.On', 'the', 'other', 'hand,', 'side', 'information', 'is', 'often', 'utilized', 'as', 'additional', 'knowledge', 'to', 'boost', 'the', 'performance', 'of', 'the', 'certain', '2162-237X', '©', '2019', 'IEEE.Personal', 'use', 'is', 'permitted,', 'but', 'republication/redistribution', 'requires', 'IEEE', 'permission.See', 'http://www.ieee.org/publications_standards/publications/rights/index.html', 'for', 'more', 'information.Authorized', 'licensed', 'use', 'limited', 'to:', 'NANJING', 'UNIVERSITY', 'OF', 'SCIENCE', 'AND', 'TECHNOLOGY.Downloaded', 'on', 'July', '20,2020', 'at', '14:14:03', 'UTC', 'from', 'IEEE', 'Xplore.', 'Restrictions', 'apply.']\n",
            "First word embedding (sample): [-2.39935406e-02  2.43640821e-02 -1.13073168e-02 -1.56162949e-02\n",
            "  8.25277250e-03  2.86505353e-02 -1.34336352e-02  2.36815922e-02\n",
            " -1.88494846e-02  1.14451805e-02  3.81586812e-02  3.12398355e-02\n",
            "  4.31255586e-02  6.00992590e-02  7.58270314e-03 -6.79429574e-03\n",
            "  2.88068689e-02  3.18598449e-02 -5.68064190e-02 -1.85533725e-02\n",
            " -1.90781765e-02 -7.13594332e-02  2.57855803e-02 -4.83711138e-02\n",
            "  1.64541323e-02 -6.92185387e-02  6.04629107e-02 -3.03294347e-03\n",
            " -3.67279872e-02 -4.02683392e-02 -2.65136901e-02 -8.40608962e-03\n",
            " -4.32039946e-02 -4.72140461e-02  2.44593571e-06  2.04117633e-02\n",
            "  4.43235151e-02 -2.72206999e-02 -1.26849553e-02  6.33350313e-02\n",
            "  2.12786850e-02 -2.58620530e-02  4.08569388e-02  5.92229031e-02\n",
            " -4.82879207e-03 -2.18244325e-02  3.99270356e-02  8.41352493e-02\n",
            "  4.33280977e-04 -1.47557193e-02  1.73694862e-03 -5.02418317e-02\n",
            " -2.06066910e-02 -4.19260599e-02  7.82026723e-02 -5.57940342e-02\n",
            " -2.23908387e-02 -4.46315929e-02  5.33485599e-02 -4.86587174e-02\n",
            " -2.19079610e-02  4.43600602e-02 -5.64327091e-02  2.55471561e-02\n",
            "  7.71497265e-02 -4.80685150e-03  7.91156441e-02 -2.33988818e-02\n",
            "  6.45576417e-02  2.43044365e-03 -1.22226812e-02  6.05790168e-02\n",
            " -2.30423715e-02  5.06006554e-02 -4.78165876e-03 -2.52345335e-02\n",
            " -2.75597069e-02 -2.34158803e-02 -1.73152015e-02  2.06026882e-02\n",
            " -2.58116499e-02  1.25917748e-01  1.32038370e-02  8.31323266e-02\n",
            "  4.23685554e-03 -2.63340976e-02 -4.17689048e-02  2.30407007e-02\n",
            "  2.95484867e-02  6.38489285e-03  1.74784139e-02 -1.25021143e-02\n",
            " -4.28542905e-02  1.41839553e-02  2.52505094e-02  1.49148209e-02\n",
            "  1.83733683e-02  7.02806935e-03  4.49853484e-03 -1.98603552e-02\n",
            "  4.48127389e-02  5.75037301e-02 -3.21013890e-02  3.73836681e-02\n",
            " -5.30520119e-02 -2.54350193e-02  2.25841347e-02 -7.48878205e-03\n",
            " -3.45664211e-02  4.46251929e-02  1.15676969e-02  2.62084436e-02\n",
            " -3.17648426e-02  3.93285416e-02 -1.02843251e-02 -1.98828056e-02\n",
            " -3.25751528e-02  2.66048219e-02 -2.85115149e-02 -8.77365749e-03\n",
            "  4.41188477e-02  8.70550517e-03 -5.80263622e-02  1.13066109e-02\n",
            "  1.97774731e-03  8.90062451e-02  6.92842109e-03 -1.29303876e-02\n",
            " -1.37047507e-02  3.31585109e-02 -3.78845818e-02  3.54873831e-03\n",
            "  3.61861624e-02  2.18297224e-02  1.63585749e-02  4.53867624e-03\n",
            "  1.14880688e-02  8.23002029e-03 -2.73635127e-02  1.60970306e-03\n",
            "  4.28586304e-02 -7.00717373e-03 -1.32033909e-02 -5.17889764e-03\n",
            " -3.40199322e-02 -9.99697670e-03  3.02541889e-02  2.53148209e-02\n",
            " -5.39039262e-02 -5.45043349e-02 -1.10161426e-02  7.54879937e-02\n",
            " -3.85761596e-02 -1.90773774e-02  5.11946268e-02  8.03722721e-03\n",
            "  3.94093394e-02 -9.13533382e-03 -2.89419368e-02  1.09083357e-03\n",
            "  5.66022005e-04  4.58791703e-02  6.65258663e-03 -1.15977675e-02\n",
            "  1.84767488e-02  1.14115458e-02 -5.57019981e-03  2.71103717e-02\n",
            " -4.51198667e-02  1.25071285e-02 -3.00406497e-02 -3.69497202e-02\n",
            " -6.50326088e-02  4.94354479e-02 -5.99733414e-03  1.35538699e-02\n",
            " -7.83504099e-02 -3.44771892e-02 -9.06936228e-02 -2.22365055e-02\n",
            " -3.42640630e-03 -1.39348656e-01  2.78441086e-02  5.23978844e-02\n",
            " -1.69646479e-02  4.47428823e-02  4.72766459e-02  5.23926551e-03\n",
            " -6.14729933e-02  1.88810267e-02  2.49859262e-02 -6.04860261e-02\n",
            " -7.31831277e-03  6.25313520e-02  2.76150070e-02  3.87808532e-02\n",
            "  6.90752128e-03 -2.46546082e-02 -1.88589655e-02 -8.87524243e-03\n",
            "  1.78548973e-02 -5.57051487e-02  3.05878948e-02 -3.03647202e-03\n",
            " -4.86346968e-02  1.19693531e-02 -1.88218411e-02 -5.23365941e-03\n",
            " -6.44949824e-03  3.51515822e-02 -3.82567830e-02 -9.34483390e-03\n",
            " -3.06300865e-03  2.77089141e-02 -3.82886119e-02  6.49339380e-03\n",
            "  4.75682209e-05 -3.86446863e-02 -9.23518278e-03  3.51430736e-02\n",
            " -3.32407057e-02  5.74328452e-02  1.75868087e-02 -1.13440985e-02\n",
            "  3.72518934e-02 -5.83454110e-02  1.62014887e-02  8.58341344e-03\n",
            "  3.67018133e-02  2.25893967e-02  4.37728949e-02  1.80411916e-02\n",
            " -1.44289704e-02 -1.10462669e-03  2.86339223e-02  5.16308360e-02\n",
            "  2.53788605e-02 -6.16145842e-02  1.78146232e-02 -6.43075109e-02\n",
            " -1.08942799e-02  6.92696497e-02  2.72549819e-02  2.76440606e-02\n",
            "  3.16525325e-02  4.10353877e-02  8.31616819e-02 -4.63995943e-03\n",
            " -2.70572063e-02 -6.46190718e-02 -2.73646135e-02 -7.64597964e-04\n",
            "  4.28084992e-02 -4.04969901e-02  1.22218588e-02  4.05528210e-02\n",
            "  1.85881928e-02  4.66197468e-02 -2.59096874e-03 -1.87016409e-02\n",
            "  3.52492221e-02  2.46402640e-02  1.70533527e-02  1.25837838e-02\n",
            "  6.15618452e-02 -2.31516715e-02  2.26835106e-02 -3.98434922e-02\n",
            " -1.07626341e-01 -7.52423750e-03  2.97342520e-02 -1.82992138e-03\n",
            "  5.42120337e-02  2.51592919e-02 -4.85289209e-02 -3.05215605e-02\n",
            " -1.75690907e-03  5.77791780e-03  9.13255103e-03  1.61033124e-02\n",
            " -1.18085379e-02  3.78172286e-02  1.07168201e-02  8.89386889e-03\n",
            "  8.04783627e-02  9.12860315e-03 -5.23833744e-02 -1.86579842e-02\n",
            "  3.30329873e-02 -4.28706370e-02 -4.16110232e-02 -1.35968300e-02\n",
            "  2.82171345e-03  4.48246108e-04  1.33847399e-02 -4.00661789e-02\n",
            " -1.02461036e-02  9.01043564e-02  1.43270483e-02  1.21045420e-02\n",
            " -4.53762989e-03  6.54365867e-02 -2.56570000e-02 -2.89664115e-03\n",
            "  3.42102572e-02 -4.45017628e-02 -1.01222638e-02 -8.63449648e-03\n",
            "  4.01401259e-02  2.79869884e-02 -1.89752895e-02  2.50749383e-02\n",
            "  1.46396430e-02  1.38758859e-02 -3.01765390e-02 -4.85007465e-03\n",
            "  2.51850784e-02 -7.85568804e-02 -2.53728107e-02 -3.52803208e-02\n",
            " -3.03762238e-02  3.31563354e-02 -3.41632478e-02  9.37112328e-03\n",
            "  3.95614933e-03  2.83309799e-02 -5.23329014e-03 -3.09487581e-02\n",
            " -1.25261685e-02  3.93427946e-02  6.01115124e-03 -1.73623841e-02\n",
            " -5.38124740e-02  1.96537259e-03  3.97735685e-02  3.85716744e-02\n",
            " -1.27674313e-02  2.34267507e-02  2.23401822e-02  1.98240969e-02\n",
            "  2.91374307e-02 -3.48888226e-02 -1.88867096e-02 -4.48479727e-02\n",
            " -1.50488289e-02 -5.27610295e-02 -1.09714895e-01 -4.38614339e-02\n",
            "  1.31632155e-02  1.07103512e-02  7.64771411e-03 -6.14743540e-03\n",
            " -1.28405765e-02 -4.01755124e-02 -1.43043082e-02 -3.87157537e-02\n",
            " -5.47264144e-02  1.96011402e-02 -3.81846540e-02 -4.83774729e-02\n",
            " -9.09400266e-03  2.37237159e-02  8.23159143e-02 -3.33355442e-02\n",
            " -4.63092774e-02  2.33803075e-02  4.22405489e-02  7.14580417e-02\n",
            " -4.89774756e-02  4.06648498e-03 -3.93732265e-02  7.11254328e-02\n",
            " -2.09677052e-02  1.64386686e-02 -3.89857739e-02  3.17286104e-02\n",
            "  1.21793458e-02 -8.06723908e-02  7.80951902e-02 -4.97094244e-02\n",
            " -2.79624797e-02 -6.25923648e-02 -2.31662188e-02 -9.57967481e-04\n",
            " -6.86595304e-05  4.68522689e-04 -3.24372314e-02 -2.32526772e-02\n",
            "  5.35365427e-03  3.93214682e-03  5.83778461e-03  1.03246383e-02\n",
            " -2.07508355e-02  9.20526087e-02  4.41110767e-02 -7.93104768e-02\n",
            " -9.69470106e-03 -5.86347505e-02 -4.58240211e-02  1.34802209e-02\n",
            "  4.50889803e-02  7.41281733e-02  3.99009138e-02  2.90351659e-02\n",
            " -4.87859324e-02 -7.19029317e-03 -3.18737924e-02  7.85052718e-04\n",
            "  1.43106189e-02 -2.81181280e-02  3.38754170e-02  3.49111832e-03\n",
            " -4.10256488e-03 -1.94368996e-02  5.49749248e-02 -3.59872915e-02\n",
            " -2.20671855e-03 -6.78142440e-03  2.61664130e-02  1.64630543e-02\n",
            "  2.28043273e-03 -5.46882227e-02 -9.34344623e-03 -8.75269342e-03\n",
            "  6.30475162e-03 -4.21980023e-02 -2.71397065e-02  6.15442134e-02\n",
            "  5.83786778e-02 -6.33158386e-02 -3.40194032e-02  6.58834577e-02\n",
            "  2.76034158e-02  2.93620192e-02 -1.52893132e-02 -4.35452424e-02\n",
            " -8.52075126e-03  3.65467407e-02 -5.10065407e-02 -2.40396149e-02\n",
            " -4.67465771e-03 -1.29232620e-04  7.26520717e-02  2.45222673e-02\n",
            "  6.81908056e-02  2.37490498e-02  2.22362461e-03  1.38428295e-02\n",
            " -4.16904278e-02 -1.54614998e-02 -6.15685880e-02  3.94369438e-02\n",
            "  1.56935230e-02  1.43195386e-03  8.17414597e-02 -1.57913733e-02\n",
            "  1.58929173e-02 -2.58505289e-02  2.16706339e-02  9.99376178e-03\n",
            "  4.22386862e-02  2.23629531e-02 -1.20341098e-02  2.00570896e-02\n",
            " -6.55505657e-02 -9.12006851e-03  2.20255204e-03  1.63084622e-02\n",
            "  4.12740372e-02 -5.73552214e-03  1.29273264e-02 -4.45453338e-02\n",
            "  2.95430254e-02  2.71868166e-02 -1.71419757e-03  4.79273237e-02\n",
            " -4.48171496e-02 -3.82862873e-02 -1.44963795e-02  1.16638169e-02\n",
            " -2.39065220e-03 -2.00803368e-03 -1.53560452e-02 -1.11215543e-02\n",
            " -6.68948190e-03 -3.69250923e-02  1.32273724e-02 -3.47729176e-02\n",
            " -1.73971504e-02 -2.20029484e-02 -9.78232734e-03 -2.25224886e-02\n",
            " -2.58758347e-02 -4.17699330e-02  4.11157347e-02  6.78384397e-03\n",
            " -2.50726449e-03 -3.46999317e-02 -1.02222562e-02  3.17210448e-03\n",
            " -4.59326460e-04  2.30987743e-02 -2.48302221e-02  4.47266735e-02\n",
            " -2.70405449e-02 -3.45953405e-02 -3.45990318e-03 -3.17185409e-02\n",
            "  3.89901996e-02  6.60978584e-03  1.21870020e-03  2.44322382e-02\n",
            " -4.24889997e-02 -1.36239573e-01 -3.19108143e-02  1.01954665e-03\n",
            "  4.84857932e-02 -3.71406004e-02  6.80777524e-03  3.32657546e-02\n",
            " -2.64972309e-03 -2.93065794e-02  3.07544656e-02 -5.52761033e-02\n",
            " -4.67012376e-02 -4.91289189e-03 -9.68410540e-03 -1.09195644e-02\n",
            "  2.39300746e-02 -1.93774141e-02 -4.42398302e-02 -4.77185547e-02\n",
            " -4.13473090e-03  5.50795533e-02 -6.16327338e-02  4.26180102e-02\n",
            " -1.25241941e-02 -1.44594135e-02  3.40190604e-02  3.22984532e-02\n",
            " -5.85265718e-02 -2.54116096e-02  5.59447557e-02  2.39716638e-02\n",
            "  1.48255797e-02  3.77903692e-02  3.90607677e-02 -4.85490747e-02\n",
            "  2.36142222e-02  2.74866447e-02  4.27721888e-02 -1.98411960e-02\n",
            "  2.22547934e-03 -5.75890951e-02  7.46842567e-03 -9.72606524e-33\n",
            " -1.75882212e-03 -3.19630392e-02 -7.80761288e-03  3.78019810e-02\n",
            " -1.43657541e-02  3.91154401e-02  4.96379100e-02 -4.96830717e-02\n",
            "  1.53983058e-02 -3.53132002e-02 -2.24544890e-02  1.03837294e-04\n",
            "  3.62815969e-02  2.37595290e-02  5.73658664e-03  1.19693798e-03\n",
            "  6.38101026e-02 -2.07646837e-04 -1.65601186e-02  5.59023861e-03\n",
            "  1.25502571e-02  1.79578681e-02 -1.42120961e-02  7.37022012e-02\n",
            " -3.66814584e-02 -2.94096749e-02 -5.18867299e-02 -4.65122536e-02\n",
            "  4.42387909e-03 -7.87069872e-02 -1.71851795e-02 -5.29879443e-02\n",
            "  3.28112543e-02 -1.74350459e-02  8.90584290e-03  7.33262226e-02\n",
            " -3.23764309e-02 -2.23082360e-02 -1.03715323e-02 -1.11245848e-02\n",
            " -2.57607866e-02 -1.91199780e-02  3.94466370e-02  1.19249430e-02\n",
            "  1.41550740e-02 -7.55424351e-02  3.96378106e-03  3.85002084e-02\n",
            "  6.91808295e-04 -3.30452509e-02 -4.89674062e-02 -1.93934012e-02\n",
            "  2.74983514e-03  2.39268988e-02 -2.89734639e-02  1.25037376e-02\n",
            "  7.14946389e-02  6.51167799e-03 -4.17752862e-02  1.12421177e-02\n",
            " -1.92158874e-02  1.12691866e-02 -5.02688251e-02 -8.89045466e-03\n",
            " -4.40192223e-02  1.11055583e-01 -9.15955901e-02 -3.33281644e-02\n",
            " -7.73618137e-03 -2.81935278e-02  1.36607501e-03  9.09502208e-02\n",
            " -4.11135890e-02  2.97713503e-02 -2.68788133e-02 -6.66623935e-02\n",
            " -4.55787592e-02  1.05940755e-02  2.01037619e-02  1.59396026e-02\n",
            " -5.58040366e-02  2.21410077e-02 -5.01230136e-02  5.63987764e-03\n",
            " -3.19870859e-02 -3.29244547e-02 -3.51461098e-02  4.52494919e-02\n",
            "  4.48141526e-03  2.13309377e-02 -3.76425944e-02  1.71082057e-02\n",
            "  1.39408652e-02 -8.28587916e-03 -6.04887158e-02 -7.21370522e-03\n",
            "  4.36342806e-02  5.19970059e-02 -4.24135961e-02 -8.94818734e-03\n",
            " -3.12697701e-03  1.80526208e-02  4.91582416e-02 -8.88505206e-03\n",
            "  4.11175787e-02  1.45311945e-03  4.27985899e-02 -9.91398096e-03\n",
            " -2.85908394e-03 -5.66650368e-03  4.03058148e-05 -5.47480397e-03\n",
            "  6.97435886e-02 -3.30727771e-02  1.83862709e-02  5.63425869e-02\n",
            "  2.27839351e-02 -5.14884759e-03 -1.98524576e-02 -2.69872863e-02\n",
            "  3.43280099e-03  3.61708254e-02  8.68097767e-02 -6.99412194e-04\n",
            " -1.96850467e-02  1.22459913e-02 -1.86352842e-02 -1.32666221e-02\n",
            "  2.94800662e-02 -1.84658226e-02  1.02994451e-02 -4.63674627e-02\n",
            "  3.03166843e-07  1.36604544e-03  2.45481674e-02 -2.33456143e-03\n",
            "  1.09549817e-02 -7.67664425e-03  4.28461097e-02  4.35257740e-02\n",
            "  2.78048497e-02 -1.59282982e-03  5.20780683e-03 -2.14872211e-02\n",
            " -3.89878005e-02  3.26900445e-02  2.72687897e-03 -1.85397267e-02\n",
            " -4.15555350e-02 -2.43056536e-04 -2.00896449e-02 -3.89267690e-03\n",
            " -4.78568226e-02  3.98001634e-02 -2.03556591e-03  4.50126603e-02\n",
            " -2.48110900e-03 -1.26077626e-02  3.30589749e-02  5.30908722e-03\n",
            " -4.89263190e-03  1.21505531e-02 -3.57387029e-02 -2.40037628e-02\n",
            "  1.84944160e-02  2.85277376e-03  2.51418911e-03  2.11825408e-02\n",
            " -2.96975616e-02  3.18346806e-02 -4.21632305e-02 -9.31731483e-04\n",
            " -9.90452361e-04 -8.13053250e-02  3.21828909e-02 -3.42766084e-02\n",
            "  4.24346887e-04  3.41547318e-02  1.18467845e-02 -3.35400142e-02\n",
            "  4.29679491e-02  8.79865547e-04 -2.13639885e-02 -4.61121053e-02\n",
            "  5.35642244e-02 -1.62874833e-02  2.93147266e-02  1.93772912e-02\n",
            " -4.19148337e-03 -2.21298579e-02 -4.08952869e-02  3.25985183e-03\n",
            "  5.69485798e-02  2.33513452e-02 -3.17055583e-02  3.26964743e-02\n",
            "  3.07157598e-02  9.61166155e-03 -7.45405070e-03  9.41518974e-03\n",
            "  8.11958775e-35 -8.77060276e-03 -6.24087937e-02  2.17905063e-02\n",
            "  1.04391510e-02 -2.30984837e-02 -2.98772678e-02  5.72489500e-02\n",
            " -8.93587321e-02  4.85177897e-02 -3.05957515e-02 -6.71853498e-02]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "first_chunk['document_embedding'].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:26:09.712358Z",
          "iopub.execute_input": "2024-10-17T17:26:09.713148Z",
          "iopub.status.idle": "2024-10-17T17:26:09.719233Z",
          "shell.execute_reply.started": "2024-10-17T17:26:09.713109Z",
          "shell.execute_reply": "2024-10-17T17:26:09.718232Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jben02es6s5q",
        "outputId": "4eb77cd2-6192-45f5-b0a6-b73da67d1fb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks_over_min_token_len[0][\"embedding\"].shape\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:26:20.399086Z",
          "iopub.execute_input": "2024-10-17T17:26:20.399486Z",
          "iopub.status.idle": "2024-10-17T17:26:20.406395Z",
          "shell.execute_reply.started": "2024-10-17T17:26:20.399449Z",
          "shell.execute_reply": "2024-10-17T17:26:20.405215Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj_yvj9N6s5r",
        "outputId": "8028d7b7-fea8-4425-f256-66386c13ba2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "pages_and_chunks_over_min_token_len[0][\"embedding\"].shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T14:39:16.987134Z",
          "iopub.execute_input": "2024-10-17T14:39:16.987495Z",
          "iopub.status.idle": "2024-10-17T14:39:16.994734Z",
          "shell.execute_reply.started": "2024-10-17T14:39:16.98746Z",
          "shell.execute_reply": "2024-10-17T14:39:16.99386Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHsKqDds6s5r",
        "outputId": "3cb56731-016a-4802-918b-a2c23c19cf9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space."
      ],
      "metadata": {
        "id": "INX2yT-P6s5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn text chunks into a single list\n",
        "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
        "text_chunks[9]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T17:26:34.536536Z",
          "iopub.execute_input": "2024-10-17T17:26:34.536959Z",
          "iopub.status.idle": "2024-10-17T17:26:34.543885Z",
          "shell.execute_reply.started": "2024-10-17T17:26:34.53692Z",
          "shell.execute_reply": "2024-10-17T17:26:34.542917Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "KROA4m896s5r",
        "outputId": "b5235a75-3a23-4c1d-a05c-80dc730d083c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.3.Experimental results of the compared methods on ﬁve UCI benchmark data sets. (a)–(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively.TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD.)UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise.D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203.COIL20 is a popular public data set for object classiﬁcation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles.The resolution of each gray-level image is 32 × 32 [46].We use the output of the ﬁrst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions.Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data.Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label.In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with σk = 0.1.In addition, the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,103} in order to obtain the satisfactory results.The classiﬁcation accuracies of all compared methods under different label noise levels are presented in Table III.It can be observed that the performances of all methods decrease with the increase in noise level.However, LNSI achieves the best results in most cases when compared with other baseline methods.Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%.E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition.Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes.The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T14:39:46.693453Z",
          "iopub.execute_input": "2024-10-17T14:39:46.694096Z",
          "iopub.status.idle": "2024-10-17T14:39:46.700221Z",
          "shell.execute_reply.started": "2024-10-17T14:39:46.694051Z",
          "shell.execute_reply": "2024-10-17T14:39:46.699211Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAm8Ydkz6s5r",
        "outputId": "3ff8e54d-75ee-477d-e1c0-b4b981b54506"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T20:08:14.478471Z",
          "iopub.execute_input": "2024-10-15T20:08:14.479155Z",
          "iopub.status.idle": "2024-10-15T20:08:14.485084Z",
          "shell.execute_reply.started": "2024-10-15T20:08:14.479111Z",
          "shell.execute_reply": "2024-10-15T20:08:14.484206Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyCFTd9J6s5r",
        "outputId": "e664e262-6bbb-4781-f22b-e136473c9f6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
        "                                               batch_size=16, # Embed all texts in batches\n",
        "                                               convert_to_tensor=True)\n",
        "text_chunk_embeddings[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T20:08:15.783682Z",
          "iopub.execute_input": "2024-10-15T20:08:15.784102Z",
          "iopub.status.idle": "2024-10-15T20:08:17.575597Z",
          "shell.execute_reply.started": "2024-10-15T20:08:15.784062Z",
          "shell.execute_reply": "2024-10-15T20:08:17.574639Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfMdOn-s6s5r",
        "outputId": "cba87dff-7d75-4f31-dd0e-9c9b1420342b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.2124e-02,  3.8127e-02, -9.9315e-03,  6.4407e-02,  3.0248e-02,\n",
              "         4.1535e-02,  2.1948e-02,  2.1881e-02,  1.9086e-02, -1.8840e-02,\n",
              "        -3.5388e-02,  3.7817e-03,  1.2992e-02, -2.3954e-02,  5.4627e-02,\n",
              "        -9.0447e-03,  1.8339e-02, -1.5943e-02,  1.2163e-02,  1.4307e-03,\n",
              "        -4.9970e-03, -4.1084e-02, -1.6222e-02,  6.2714e-03, -7.0415e-02,\n",
              "        -2.5558e-04,  7.2844e-03, -2.6608e-03,  6.3036e-03, -1.0456e-02,\n",
              "         8.2130e-02,  5.9045e-03, -7.9108e-03,  3.5973e-02,  2.4091e-06,\n",
              "        -7.1029e-03, -1.4681e-02, -5.4000e-03, -5.1102e-03,  3.7531e-03,\n",
              "        -1.4898e-02,  1.9143e-02, -2.5063e-02, -1.1821e-02,  1.5855e-02,\n",
              "        -6.5839e-02,  5.9678e-02,  1.0835e-01, -2.5654e-03,  1.7020e-02,\n",
              "        -8.8616e-03, -1.1839e-02, -7.5419e-03, -1.8383e-02,  2.0384e-02,\n",
              "        -7.2165e-02,  6.7357e-02, -7.8544e-02, -1.0627e-01, -3.1834e-03,\n",
              "         2.2758e-02,  3.4900e-02,  2.0432e-02,  3.1416e-02,  1.5606e-02,\n",
              "         4.6315e-02,  9.2130e-03, -1.2025e-02, -1.4516e-02,  5.9579e-02,\n",
              "         1.2957e-02,  2.1756e-02,  3.5949e-02, -1.2412e-02,  3.2006e-02,\n",
              "        -3.5019e-02,  2.2230e-02,  7.2251e-03,  2.5705e-02,  2.1133e-02,\n",
              "        -4.2653e-02,  2.3855e-02,  2.3468e-02, -1.4367e-03,  1.4977e-02,\n",
              "         5.2628e-02,  8.3290e-03, -2.6471e-02,  1.7456e-02, -1.5591e-02,\n",
              "         3.0746e-02, -1.9981e-02,  3.6025e-02,  3.3135e-02,  2.8852e-02,\n",
              "         1.6059e-02, -2.1331e-02, -5.0593e-02,  1.8020e-02, -7.9737e-02,\n",
              "         8.4829e-03, -3.3284e-02, -2.2702e-02,  2.6371e-02, -8.4160e-03,\n",
              "        -1.9425e-02,  4.9018e-03,  8.5094e-02, -5.2180e-02, -2.1078e-02,\n",
              "         3.2697e-02,  1.9886e-02, -1.6984e-02,  8.7192e-02,  1.9203e-02,\n",
              "         2.9916e-02, -7.4958e-02, -3.5931e-02, -5.2338e-02,  1.5146e-02,\n",
              "        -5.4143e-02,  2.8091e-02, -2.9220e-02, -1.3852e-02,  7.0588e-03,\n",
              "        -1.9098e-02,  5.4008e-02,  3.2527e-02, -1.6333e-02, -4.1449e-04,\n",
              "        -1.4624e-02,  2.1603e-02, -1.3072e-02, -2.2717e-02,  3.5014e-02,\n",
              "         7.4661e-02,  1.4185e-02, -3.1629e-02, -9.6526e-02, -1.5975e-02,\n",
              "        -2.9723e-02,  3.9688e-03,  1.2282e-03, -2.1094e-02, -2.2499e-02,\n",
              "        -3.2498e-02,  1.3578e-02, -4.5689e-02,  8.4964e-03, -4.3125e-03,\n",
              "        -5.1162e-02,  5.8355e-02, -4.8362e-02, -1.8682e-02,  6.5764e-02,\n",
              "        -2.7547e-03,  1.5805e-02, -4.1534e-03,  4.2825e-02,  1.4506e-02,\n",
              "         3.5975e-02, -1.5091e-02,  4.1333e-02,  2.7690e-02,  5.4852e-02,\n",
              "        -2.1804e-02, -3.3537e-02, -4.7066e-02,  6.2057e-02, -3.0531e-02,\n",
              "         2.7463e-03,  4.0064e-02,  2.0280e-02, -6.3168e-03,  3.3627e-02,\n",
              "        -3.3524e-02,  5.1398e-03,  2.6298e-02, -1.6413e-02, -5.1842e-02,\n",
              "        -3.9385e-02,  1.7642e-02,  9.5245e-03,  4.7720e-03, -9.1136e-02,\n",
              "        -2.7315e-02,  3.5300e-02, -1.6363e-02, -7.5692e-02, -8.2788e-02,\n",
              "         1.4773e-02, -3.7173e-02,  1.6927e-02,  3.3771e-02,  1.0359e-01,\n",
              "        -5.1749e-03, -9.7006e-04,  3.8864e-02,  5.4785e-02,  3.1727e-04,\n",
              "        -5.2513e-03, -5.2295e-02,  1.4919e-02, -7.1240e-03, -6.9091e-03,\n",
              "        -6.9840e-02, -4.0755e-03,  6.5876e-03, -2.4612e-03, -4.0398e-02,\n",
              "         6.1217e-03, -1.7358e-02,  5.4167e-02, -3.4894e-02, -8.1553e-03,\n",
              "         5.4256e-02,  1.8192e-02,  2.1158e-02, -4.2992e-02, -5.1129e-03,\n",
              "         3.3038e-02, -4.1595e-02, -6.7325e-02,  3.7837e-02,  8.3331e-03,\n",
              "        -4.7894e-02,  4.8422e-02,  5.4026e-02,  2.3557e-02,  9.2344e-03,\n",
              "        -1.4896e-02, -4.1971e-02,  4.5056e-02, -2.7809e-02, -7.0770e-03,\n",
              "        -7.3972e-02, -2.3948e-02,  8.9081e-03,  6.6196e-02,  3.3085e-03,\n",
              "        -3.4758e-02,  4.9128e-02, -3.8034e-02, -2.2849e-02,  1.0662e-03,\n",
              "        -1.9450e-02, -4.6429e-02, -5.2695e-02,  1.6922e-02,  1.7022e-02,\n",
              "         2.8108e-02,  6.9621e-03,  5.3982e-02,  4.1198e-03,  4.9790e-02,\n",
              "        -2.7724e-02, -7.1062e-02,  2.9345e-02, -5.4753e-02, -4.0211e-02,\n",
              "        -3.1507e-03, -4.1477e-02,  5.8728e-03,  3.0169e-04, -5.6595e-03,\n",
              "        -1.1779e-03,  9.1712e-03,  8.5957e-03, -1.1113e-02, -6.9602e-04,\n",
              "        -3.8342e-02, -3.3050e-02,  6.6153e-03,  1.6726e-02, -5.5017e-02,\n",
              "        -4.1863e-02,  2.4672e-02, -2.6382e-02, -2.6349e-02,  4.7349e-02,\n",
              "         4.8152e-02, -1.8644e-02,  2.4631e-02, -1.2353e-02, -1.3686e-02,\n",
              "         2.6030e-02,  2.9189e-03, -7.6053e-02, -3.0614e-02,  8.8836e-02,\n",
              "        -1.8871e-02,  5.2655e-02, -4.4456e-04, -1.4489e-02, -3.4250e-02,\n",
              "        -2.1757e-02, -9.4445e-03, -7.4760e-03, -4.5764e-03,  1.5439e-02,\n",
              "         4.5843e-02,  1.0566e-02, -3.6724e-03,  2.5239e-02, -2.5684e-02,\n",
              "        -2.0954e-02,  2.5101e-02, -3.4521e-02, -2.3324e-02,  3.0507e-02,\n",
              "        -6.0655e-03, -1.2098e-02,  1.5072e-02, -3.2266e-02,  2.5835e-02,\n",
              "        -5.8681e-02, -5.9056e-02,  2.2656e-02, -1.7210e-02,  8.9109e-03,\n",
              "        -1.3286e-02,  1.8075e-02,  5.4659e-02,  7.9006e-03,  1.3282e-02,\n",
              "         1.9666e-02, -4.9128e-02,  3.9547e-02, -5.6965e-02, -7.2596e-03,\n",
              "         3.0111e-02,  1.8977e-02, -3.5253e-02, -1.6586e-02, -5.5027e-02,\n",
              "         1.7354e-02, -4.5608e-02, -4.2816e-02, -3.2584e-02, -1.6475e-02,\n",
              "        -1.8636e-02, -5.8197e-03, -3.0036e-02, -9.1348e-03,  3.3313e-02,\n",
              "         2.3690e-04, -5.3751e-02,  3.9149e-02,  1.5677e-02, -5.4835e-02,\n",
              "         1.5855e-02,  3.3193e-02,  2.7040e-02,  1.7746e-02, -7.0088e-03,\n",
              "         7.3871e-02,  2.4159e-02, -4.1400e-03,  4.2636e-02, -4.4606e-02,\n",
              "         2.2690e-02,  5.1542e-02,  6.6880e-02,  3.9975e-02, -7.1328e-03,\n",
              "        -2.4639e-02,  2.1820e-02,  3.2840e-02, -1.1777e-02,  4.4945e-02,\n",
              "        -2.2249e-02, -5.2730e-04, -2.1731e-02,  4.2982e-02, -1.0577e-02,\n",
              "        -8.4009e-02, -1.6172e-02,  3.7118e-02, -3.5803e-02,  4.2010e-03,\n",
              "         8.4776e-03, -2.5988e-03,  5.3057e-03,  2.8233e-02,  3.1690e-02,\n",
              "        -2.3973e-02, -2.0170e-02,  2.3850e-02, -8.2473e-02,  5.3678e-02,\n",
              "         1.8527e-02, -1.4210e-02, -6.9355e-02, -1.3623e-02,  3.5717e-02,\n",
              "         8.2476e-03,  3.4910e-02, -2.7122e-02,  3.8074e-02, -4.1341e-02,\n",
              "         3.9393e-02, -1.6894e-02, -6.1027e-02, -4.7219e-02, -2.5812e-02,\n",
              "         5.7053e-02, -2.4779e-02, -5.2005e-03, -5.2714e-02, -5.9200e-02,\n",
              "         4.9607e-02, -6.3065e-02, -3.9867e-02,  4.8460e-03,  9.1936e-02,\n",
              "        -8.5952e-03, -2.0181e-02, -1.8436e-02,  2.6748e-02, -1.6999e-02,\n",
              "        -1.4840e-03, -1.2537e-03,  4.1918e-02, -6.5533e-05,  1.3685e-02,\n",
              "         1.0012e-02, -1.3478e-02,  4.3827e-02,  3.3998e-02,  2.5824e-04,\n",
              "        -4.6239e-02, -4.3844e-03,  1.6494e-02,  4.7944e-02,  2.6713e-02,\n",
              "         1.4018e-02,  7.5560e-02, -3.7277e-02, -9.2111e-02, -1.2049e-02,\n",
              "        -4.4473e-02,  1.9883e-02,  1.4744e-02,  5.0223e-03, -1.6002e-02,\n",
              "        -1.4784e-02,  6.2918e-03, -1.2782e-02, -5.4679e-02, -1.6306e-03,\n",
              "         3.4364e-02, -8.5657e-03,  2.5682e-02,  3.7974e-02, -3.2339e-02,\n",
              "        -1.8166e-02, -9.0991e-03, -1.8904e-02, -5.4873e-02, -3.4294e-03,\n",
              "         1.3496e-02, -2.3740e-05,  3.5508e-02, -2.7521e-02, -6.4204e-02,\n",
              "         4.5435e-02,  1.1325e-01,  3.2574e-02,  2.3510e-02,  2.2437e-02,\n",
              "        -2.3940e-02, -2.1593e-02,  2.3552e-02, -6.2520e-02,  3.3548e-02,\n",
              "         6.3694e-02, -1.8662e-02, -8.6013e-02,  1.6066e-02,  3.7282e-02,\n",
              "         4.7882e-02,  6.8023e-02,  7.9031e-03, -3.1130e-02, -3.8062e-02,\n",
              "         4.4008e-02,  1.2794e-02, -4.9687e-02,  1.5622e-02, -1.8663e-02,\n",
              "         1.9677e-02,  1.7172e-02, -3.9101e-02,  1.4192e-02,  1.5179e-02,\n",
              "         2.2858e-02, -4.6097e-02, -6.8319e-02, -1.7198e-02,  1.8843e-02,\n",
              "        -3.6664e-02,  1.7459e-02, -7.6153e-03,  3.1528e-02,  1.0233e-01,\n",
              "         3.4934e-02,  3.5470e-02, -5.7510e-03,  4.1516e-02, -9.0391e-03,\n",
              "        -5.3469e-02, -2.5799e-02, -1.2246e-02, -1.5208e-02, -7.0262e-02,\n",
              "         4.6277e-02,  1.8386e-02,  1.7212e-02, -6.0774e-02,  1.5406e-02,\n",
              "        -3.5773e-02, -3.0975e-02,  3.3108e-02,  2.6138e-03, -3.2132e-03,\n",
              "         4.1931e-02, -4.8962e-03, -6.1211e-03,  8.6967e-04,  5.1749e-02,\n",
              "         4.8688e-02,  2.8222e-02, -4.4735e-02, -1.8424e-02, -5.1222e-02,\n",
              "         2.8670e-02,  3.2398e-02,  2.2563e-02, -2.4688e-02,  1.1700e-02,\n",
              "        -2.1802e-02, -7.8687e-02, -3.1139e-03,  1.2897e-02,  5.2851e-02,\n",
              "         7.9641e-03, -3.8615e-02,  2.5800e-02,  2.6620e-02, -1.2767e-02,\n",
              "        -1.8457e-02, -4.3473e-02,  3.3697e-02, -1.7330e-02, -3.2203e-03,\n",
              "        -6.5465e-33, -1.6001e-02, -2.5252e-02, -4.2531e-02,  6.0487e-02,\n",
              "         2.4670e-03, -4.6322e-03,  1.6967e-02, -6.6355e-03,  2.0887e-02,\n",
              "         3.8379e-02, -1.6098e-02,  1.2731e-02,  1.1134e-02,  6.0611e-04,\n",
              "         7.4491e-03, -1.4494e-02,  3.7838e-02, -3.7393e-02,  3.8937e-02,\n",
              "         5.4285e-02,  3.0575e-02,  5.0860e-02,  3.0518e-02, -1.5607e-01,\n",
              "        -2.5583e-02, -5.6205e-03, -3.8103e-02,  2.4005e-02, -3.2169e-02,\n",
              "         2.0262e-02, -3.0630e-02,  3.3671e-02,  3.9260e-02,  6.1847e-03,\n",
              "         1.3022e-02, -1.5659e-02, -9.2699e-02,  2.3970e-02, -9.2328e-02,\n",
              "         5.3220e-02,  1.8654e-02, -5.7118e-02,  8.2313e-02,  1.7661e-02,\n",
              "        -4.0538e-02,  1.0318e-02,  5.6987e-02, -3.0158e-02, -5.6517e-03,\n",
              "        -3.5517e-02,  2.3609e-02,  1.4753e-03, -1.4948e-02, -2.9958e-02,\n",
              "         3.4993e-02,  3.8957e-02, -7.6723e-03,  3.4832e-02, -1.1871e-01,\n",
              "        -2.7281e-04, -1.6030e-03, -1.1541e-03,  5.0794e-03,  5.0657e-02,\n",
              "        -5.2921e-02,  5.2885e-02,  7.3864e-02, -2.1522e-03,  1.5639e-02,\n",
              "        -6.3951e-02, -7.9023e-03,  1.1352e-02,  4.5364e-02,  2.8780e-02,\n",
              "         7.2898e-02, -6.4468e-02, -9.9913e-02,  4.6239e-02, -4.3469e-02,\n",
              "        -4.2182e-02, -2.3803e-02,  1.0646e-03, -3.2488e-02, -1.6469e-02,\n",
              "         1.8936e-02, -6.0059e-02, -4.2712e-02,  3.4280e-02,  2.3848e-03,\n",
              "         1.4440e-02, -2.9850e-02,  4.8391e-03, -1.1152e-02, -3.0208e-02,\n",
              "         1.6828e-03, -4.8987e-02,  5.3620e-02,  3.1228e-02,  1.2044e-02,\n",
              "        -3.8455e-02, -4.4104e-02, -5.3147e-02,  8.2596e-04,  2.1624e-02,\n",
              "        -8.3543e-03,  7.3237e-03,  8.1667e-02,  2.1962e-02, -3.1412e-02,\n",
              "         1.1895e-02,  9.1072e-03,  1.8337e-02,  6.1991e-02, -2.7996e-02,\n",
              "         1.2338e-02, -3.4642e-02,  1.1172e-03,  2.7351e-02,  5.9832e-02,\n",
              "         5.8636e-02,  8.1144e-03,  9.7038e-03, -2.5104e-02, -5.4894e-04,\n",
              "        -2.7834e-02, -1.9046e-02,  1.6924e-02, -2.2880e-02,  3.2198e-02,\n",
              "        -1.9011e-02,  1.0847e-02, -1.3057e-02,  3.1499e-07, -2.6681e-02,\n",
              "        -5.0434e-02, -3.0048e-03, -5.2638e-02,  6.9711e-03,  7.8349e-02,\n",
              "        -3.6252e-02,  2.9468e-02,  8.8661e-03, -1.8118e-02,  3.1925e-02,\n",
              "        -3.3239e-02,  2.9142e-02, -1.8725e-02, -3.0668e-02, -3.0032e-02,\n",
              "         2.3817e-02, -5.0389e-04, -1.5996e-02,  4.6268e-02,  3.7110e-02,\n",
              "         2.8868e-02,  2.1355e-02, -4.9067e-02,  4.1485e-03, -7.4993e-03,\n",
              "         2.7060e-02, -1.7140e-02,  1.8789e-02,  1.3649e-03,  4.6187e-02,\n",
              "         1.2665e-02,  8.1252e-03,  2.8744e-02, -2.1487e-02,  2.0919e-02,\n",
              "         1.6115e-02, -5.1905e-03, -3.8733e-02, -3.8555e-03, -2.1550e-02,\n",
              "         8.9149e-02,  3.0749e-02, -1.1985e-03,  1.9236e-02, -4.1225e-02,\n",
              "        -2.1651e-02, -7.9043e-03, -6.0658e-02,  1.3427e-02,  1.6364e-02,\n",
              "         1.4859e-02, -7.4707e-03, -6.2970e-03, -3.0885e-02, -2.0711e-03,\n",
              "        -1.2127e-02, -5.8127e-02,  4.8278e-02,  1.0868e-02, -4.4501e-02,\n",
              "        -1.9426e-02,  3.9355e-02, -1.1257e-02,  4.2453e-02, -3.2867e-02,\n",
              "        -1.1923e-02,  2.3568e-34,  1.1036e-03, -1.6594e-02, -1.8682e-02,\n",
              "         3.7336e-02, -1.6161e-02, -1.8887e-02, -4.1775e-02, -4.1025e-02,\n",
              "        -7.0724e-04, -2.7342e-02, -1.6943e-02], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Df showing everything"
      ],
      "metadata": {
        "id": "2wU3becr6s5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving embedding to file\n",
        "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "save_path = \"text_chunks_and_embeddings_df.csv\"\n",
        "text_chunks_and_embeddings_df.to_csv(save_path, index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:00:10.297736Z",
          "iopub.execute_input": "2024-10-17T18:00:10.298184Z",
          "iopub.status.idle": "2024-10-17T18:00:10.630975Z",
          "shell.execute_reply.started": "2024-10-17T18:00:10.298146Z",
          "shell.execute_reply": "2024-10-17T18:00:10.629797Z"
        },
        "trusted": true,
        "id": "cuzJjQle6s5r"
      },
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "code",
      "source": [
        "# Import saved file and view\n",
        "text_chunks_and_embeddings_df_load = pd.read_csv(save_path)\n",
        "text_chunks_and_embeddings_df_load.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:00:11.824034Z",
          "iopub.execute_input": "2024-10-17T18:00:11.824437Z",
          "iopub.status.idle": "2024-10-17T18:00:11.855197Z",
          "shell.execute_reply.started": "2024-10-17T18:00:11.824387Z",
          "shell.execute_reply": "2024-10-17T18:00:11.854085Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "id": "c-Afi8-W6s5s",
        "outputId": "5c529630-02f4-4089-9157-1a30fc9d0c74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   page_number                                     sentence_chunk  \\\n",
              "0            0  This article has been accepted for inclusion i...   \n",
              "1            1  This article has been accepted for inclusion i...   \n",
              "2            2  This article has been accepted for inclusion i...   \n",
              "3            3  This article has been accepted for inclusion i...   \n",
              "4            4  This article has been accepted for inclusion i...   \n",
              "\n",
              "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
              "0              7014              1001            1753.50   \n",
              "1              6134               926            1533.50   \n",
              "2              6107              1000            1526.75   \n",
              "3              4871               897            1217.75   \n",
              "4              4136               805            1034.00   \n",
              "\n",
              "                                           embedding  \\\n",
              "0  [ 2.21239850e-02  3.81273478e-02 -9.93154477e-...   \n",
              "1  [ 2.10550893e-02  1.89801585e-02  5.71878441e-...   \n",
              "2  [ 2.32288763e-02 -3.22096795e-02 -1.31214494e-...   \n",
              "3  [ 1.74471941e-02  1.00928722e-02 -8.88381561e-...   \n",
              "4  [-1.78029444e-02  8.12479015e-03 -1.64016578e-...   \n",
              "\n",
              "                                               words  \\\n",
              "0  ['This', 'article', 'has', 'been', 'accepted',...   \n",
              "1  ['This', 'article', 'has', 'been', 'accepted',...   \n",
              "2  ['This', 'article', 'has', 'been', 'accepted',...   \n",
              "3  ['This', 'article', 'has', 'been', 'accepted',...   \n",
              "4  ['This', 'article', 'has', 'been', 'accepted',...   \n",
              "\n",
              "                                     word_embeddings  \\\n",
              "0  [[-0.02399354  0.02436408 -0.01130732 ...  0.0...   \n",
              "1  [[-0.02399348  0.02436411 -0.01130732 ...  0.0...   \n",
              "2  [[-0.02399354  0.02436408 -0.01130732 ...  0.0...   \n",
              "3  [[-0.02399354  0.02436408 -0.01130732 ...  0.0...   \n",
              "4  [[-0.02399354  0.02436408 -0.01130732 ...  0.0...   \n",
              "\n",
              "                                  document_embedding  \n",
              "0  [ 1.83159053e-02  3.61173917e-02 -2.62305447e-...  \n",
              "1  [ 2.12759182e-02  3.14661646e-02 -2.46859062e-...  \n",
              "2  [ 1.69693984e-02  2.67011831e-02 -2.58474208e-...  \n",
              "3  [ 1.75494336e-02  2.44253294e-02 -2.38905113e-...  \n",
              "4  [ 1.73777745e-02  2.62514099e-02 -2.57682228e-...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-450101e1-d161-4684-b0bd-8da20f27b93f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>sentence_chunk</th>\n",
              "      <th>chunk_char_count</th>\n",
              "      <th>chunk_word_count</th>\n",
              "      <th>chunk_token_count</th>\n",
              "      <th>embedding</th>\n",
              "      <th>words</th>\n",
              "      <th>word_embeddings</th>\n",
              "      <th>document_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>7014</td>\n",
              "      <td>1001</td>\n",
              "      <td>1753.50</td>\n",
              "      <td>[ 2.21239850e-02  3.81273478e-02 -9.93154477e-...</td>\n",
              "      <td>['This', 'article', 'has', 'been', 'accepted',...</td>\n",
              "      <td>[[-0.02399354  0.02436408 -0.01130732 ...  0.0...</td>\n",
              "      <td>[ 1.83159053e-02  3.61173917e-02 -2.62305447e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6134</td>\n",
              "      <td>926</td>\n",
              "      <td>1533.50</td>\n",
              "      <td>[ 2.10550893e-02  1.89801585e-02  5.71878441e-...</td>\n",
              "      <td>['This', 'article', 'has', 'been', 'accepted',...</td>\n",
              "      <td>[[-0.02399348  0.02436411 -0.01130732 ...  0.0...</td>\n",
              "      <td>[ 2.12759182e-02  3.14661646e-02 -2.46859062e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6107</td>\n",
              "      <td>1000</td>\n",
              "      <td>1526.75</td>\n",
              "      <td>[ 2.32288763e-02 -3.22096795e-02 -1.31214494e-...</td>\n",
              "      <td>['This', 'article', 'has', 'been', 'accepted',...</td>\n",
              "      <td>[[-0.02399354  0.02436408 -0.01130732 ...  0.0...</td>\n",
              "      <td>[ 1.69693984e-02  2.67011831e-02 -2.58474208e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4871</td>\n",
              "      <td>897</td>\n",
              "      <td>1217.75</td>\n",
              "      <td>[ 1.74471941e-02  1.00928722e-02 -8.88381561e-...</td>\n",
              "      <td>['This', 'article', 'has', 'been', 'accepted',...</td>\n",
              "      <td>[[-0.02399354  0.02436408 -0.01130732 ...  0.0...</td>\n",
              "      <td>[ 1.75494336e-02  2.44253294e-02 -2.38905113e-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4136</td>\n",
              "      <td>805</td>\n",
              "      <td>1034.00</td>\n",
              "      <td>[-1.78029444e-02  8.12479015e-03 -1.64016578e-...</td>\n",
              "      <td>['This', 'article', 'has', 'been', 'accepted',...</td>\n",
              "      <td>[[-0.02399354  0.02436408 -0.01130732 ...  0.0...</td>\n",
              "      <td>[ 1.73777745e-02  2.62514099e-02 -2.57682228e-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-450101e1-d161-4684-b0bd-8da20f27b93f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-450101e1-d161-4684-b0bd-8da20f27b93f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-450101e1-d161-4684-b0bd-8da20f27b93f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f6bebe3e-7494-4c79-890a-220280b32d52\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f6bebe3e-7494-4c79-890a-220280b32d52')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f6bebe3e-7494-4c79-890a-220280b32d52 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "text_chunks_and_embeddings_df_load",
              "summary": "{\n  \"name\": \"text_chunks_and_embeddings_df_load\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classi\\ufb01cation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract\\u2014Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classi\\ufb01er training and severely decrease the classi\\ufb01cation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classi\\ufb01cation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as \\u201clabel noise handling via side information\\u201d (LNSI).Speci\\ufb01cally, the observed label matrix is decomposed as the sum of two parts, in which the \\ufb01rst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been suf\\ufb01ciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the \\u201cSummit of the Six Top Talents\\u201d Program under Grant DZXX-027, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by Jiangsu Province, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the \\u201c111\\u201d Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi\\u2019an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Arti\\ufb01cial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the \\ufb01gures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identi\\ufb01er 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classi\\ufb01cation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets con\\ufb01rm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms\\u2014Classi\\ufb01cation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classi\\ufb01er, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the suf\\ufb01- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classi\\ufb01cation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signi\\ufb01cantly mislead the classi\\ufb01er training and then severely decrease the classi\\ufb01cation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signi\\ufb01cance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]\\u2013[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classi\\ufb01cation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite dif\\ufb01cult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X \\u00a9 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z\\u2217and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning \\ufb01elds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and \\ufb01rst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speci\\ufb01cally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]\\u2013[11].Therefore, our proposed method is named as \\u201clabel noise handling via side information\\u201d (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z\\u2217on the example features (i.e., T = X Z\\u2217), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classi\\ufb01cation, which does not need the one-versus-one or one-versus-the-rest operations; 2) suf\\ufb01cient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classi\\ufb01er parameter optimization into a uni\\ufb01ed framework.Due to the above merits, a reliable classi\\ufb01er can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classi\\ufb01er.II.RELATED WORK This section brie\\ufb02y reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches \\ufb01rst detect and eliminate label noise and then conduct the standard supervised classi\\ufb01cation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble \\ufb01l- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the in\\ufb02uence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classi\\ufb01cation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speci\\ufb01c tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Algorithm 1 Algorithm for Solving LNSI Input: feature matrix X, observed label matrix Y; trade-off parameters: \\u03bb1, \\u03bb2, and \\u03bb3; Z = O, J = Z, E = O, B = O, M1 = O, M2 = O, M3 = O; \\u03bc = 10\\u22123, \\u03bcmax = 106, \\u03c1 = 1.2, \\u03f5 = 10\\u22126, iter_max = 1000; iter = 0; 1: Construct graph G and calculate the Laplacian matrix L via (3); 2: while not converge do 3: Update Z via (10), 4: Update E via (14), 5: Update J via (16), 6: Update B via (19), 7: Update the multipliers M1 := M1 + \\u03bc(Y \\u2212B \\u2212E), M2 := M2 + \\u03bc(B \\u2212X J), M3 := M3 + \\u03bc(Z \\u2212J), 8: Update the parameter \\u03bc by \\u03bc := min(\\u03c1\\u03bc, \\u03bcmax), 9: iter := iter + 1, 10: Check the convergence conditions: \\u2225Y \\u2212B\\u2212E\\u2225F \\u2264\\u03f5 and \\u2225B\\u2212X J\\u2225F \\u2264\\u03f5 and \\u2225Z\\u2212J\\u2225F \\u2264 \\u03f5; or iter > iter_max.11: end while Output: optimized Z\\u2217and E\\u2217. B. Computational Complexity This section studies the computational complexity of Algorithm 1.The graph construction in Line 1 of Algorithm 1 takes O(n2) complexity.Line 3 is accomplished by using the SVD, of which the complexity is O(min(d2c, dc2)).In Line 4, one should compute the \\u21132-norm of each row of a n \\u00d7 c matrix E, so the complexity is O(nc).Note that a d \\u00d7 d matrix should be inverted in Line 5, so the complexity of this step is O(d3).Therefore, the total complexity of our proposed algorithm is O(n2 +(min(d2c, dc2)+nc+d3)k) by assuming that Lines 2\\u20139 are iterated k times.Note that the complexity of Algorithm 1 is squared to the number of training examples n, so its complexity is acceptable.C. Generalization Bound In this section, we derive the generalization bound of LNSI.1) Preliminaries: Recall that our goal is to \\ufb01nd a suit- able project matrix Z by recovering the clean label matrix X Z, given the observed noisy label matrix Y and example features X. Similar to [10], (6) can be reformulated to the following expression with hard constraints, namely: min Z,E \\u0002 (i, j)\\u2208{1,...,n}\\u00d7{1,...,c} \\u2113((X Z + E)ij , Y ij ) s.t.\\u2225Z\\u2225\\u2217\\u2264Z\\u2217, \\u2225Z\\u22252 F \\u2264ZF, X Z \\u2208[\\u22121, 1]n\\u00d7c tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1. (21) Let \\u03b8 = (Z, E) be any feasible solution, and \\r = {(Z, E) | \\u2225Z\\u2225\\u2217 \\u2264 Z\\u2217, \\u2225Z\\u2225F \\u2264 \\u221aZF, X Z \\u2208 [\\u22121, 1]n\\u00d7c, tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1} be the feasible solution set.Also, let f\\u03b8(i, j) = Xi ZI j + Eij be the estimation function for Yij parameterized by \\u03b8 = (Z, E), and F\\r = { f\\u03b8 | \\u03b8 \\u2208\\r} be the set of feasible functions.I j is the jth column of identity matrix I \\u2208Rc\\u00d7c.We are interested in the following two \\u201c\\u2113-risk\\u201d quantities: 1) expected \\u2113-risk: R\\u2113( f ) = Ei, j [\\u2113( f (i, j), Yij )]; 2) empirical \\u2113-risk: \\u02c6R\\u2113( f ) = (1/nr) \\u0011 (i, j) \\u2113( f (i, j)Yij ), where nr is the number of observed entries.Thus, LNSI is to \\ufb01nd a proper \\u03b8\\u2217= (Z\\u2217, E\\u2217) that parameterizes f \\u2217= arg min f \\u2208F\\r \\u02c6R\\u2113( f ).2) Generalization Bound of LNSI: To bound the generaliza- tion error of LNSI, we \\ufb01rst link the quality of training labels to Rademacher complexity, which theoretically measures the complexity of a function class.We will show that high-quality labels of training examples will result in a lower model complexity and thus a smaller error bound.To begin with, we apply the following lemma to bound the expected \\u2113-risk.Lemma 3 (Bound on Expected \\u2113-Risk [39]): Let \\u2113be the loss function bounded by B with Lipschitz constant L\\u2113, and \\u03b4 be a constant where 0 < \\u03b4 < 1.With probability at least 1 \\u2212\\u03b4, we have max f \\u2208F |R\\u2113( f ) \\u2212\\u02c6R\\u2113( f )| \\u22642L\\u2113Rn(F) + B \\u0012 ln(1/\\u03b4) 2nr where Rn(F) := E[R(F)] is the Rademacher complexity of the function class F and R(F) := E\\u03c3 \\u0013 sup f \\u2208F 1 nr nr \\u0002 \\u03b1=1 \\u03c3\\u03b1 f (\\u03b1) \\u0014 is the empirical Rademacher complexity on the training examples.Note that \\u03c3\\u03b1 (\\u03b1 = 1, 2, . . . ,nr) are independent identically distributed (i.i.d.)Rademacher random variables.Given Lemma 3, we see that the key to derive the upper bound of a function f \\u2208F is to bound the complexity Rn(F\\r).More formally, the Rademacher complexity can be bounded in terms of the constraints in (21).Before diving into the details, we \\ufb01rst provide several useful theorems and lemmas.Lemma 4 (Complexity Bound [40]): Let S be a closed con- vex set and let F : S \\u2192R be \\u03b2-strongly convex with respect to \\u2225\\u00b7 \\u2225. In addition, we assume that F\\u22c6(O) = 0 with F\\u22c6being the Fenchel conjugate of function F. Further, let A = {A : \\u2225A\\u2225\\u22c6\\u2264A} and de\\ufb01ne W = {W \\u2208S : F(W) \\u2264Fmax}.Considering the class of linear functions F = {A \\u2192 W, A : W \\u2208W}, we have R(F) \\u2264A \\u0012 2Fmax \\u03b2nr (22) where W, A = tr(W\\u22a4A).Lemma 5 [41]: The function F : Rn\\u00d7c \\u2192R de\\ufb01ned as F(W) = (1/2)\\u2225W\\u22252 2,q for q = (ln(c)/(ln(c) \\u22121)) is (1/(3 ln(c)))-strongly convex with respect to \\u2225\\u00b7\\u22252,1 over Rn\\u00d7c.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1417,\n        \"min\": 1922,\n        \"max\": 7014,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          7014,\n          6134,\n          4805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 231,\n        \"min\": 253,\n        \"max\": 1001,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1001,\n          926,\n          905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 354.4185164950596,\n        \"min\": 480.5,\n        \"max\": 1753.5,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1753.5,\n          1533.5,\n          1201.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"[ 2.21239850e-02  3.81273478e-02 -9.93154477e-03  6.44065142e-02\\n  3.02477945e-02  4.15348969e-02  2.19483543e-02  2.18814891e-02\\n  1.90859195e-02 -1.88395344e-02 -3.53879109e-02  3.78173264e-03\\n  1.29918493e-02 -2.39541177e-02  5.46273813e-02 -9.04467236e-03\\n  1.83388721e-02 -1.59426033e-02  1.21632693e-02  1.43065816e-03\\n -4.99694096e-03 -4.10837308e-02 -1.62218586e-02  6.27142237e-03\\n -7.04150945e-02 -2.55587598e-04  7.28442706e-03 -2.66082864e-03\\n  6.30354136e-03 -1.04558198e-02  8.21296349e-02  5.90451760e-03\\n -7.91084766e-03  3.59727368e-02  2.40913755e-06 -7.10283825e-03\\n -1.46812433e-02 -5.40001132e-03 -5.11019351e-03  3.75314639e-03\\n -1.48983840e-02  1.91426016e-02 -2.50629559e-02 -1.18210195e-02\\n  1.58547331e-02 -6.58385605e-02  5.96776493e-02  1.08353682e-01\\n -2.56540370e-03  1.70195643e-02 -8.86159483e-03 -1.18391505e-02\\n -7.54189538e-03 -1.83826517e-02  2.03844756e-02 -7.21648484e-02\\n  6.73565343e-02 -7.85439536e-02 -1.06272571e-01 -3.18334182e-03\\n  2.27580629e-02  3.48998606e-02  2.04322003e-02  3.14163938e-02\\n  1.56055512e-02  4.63152491e-02  9.21303127e-03 -1.20248757e-02\\n -1.45155415e-02  5.95788285e-02  1.29572423e-02  2.17556208e-02\\n  3.59488614e-02 -1.24122445e-02  3.20064835e-02 -3.50186117e-02\\n  2.22297385e-02  7.22508645e-03  2.57048067e-02  2.11333577e-02\\n -4.26531322e-02  2.38546245e-02  2.34684162e-02 -1.43672631e-03\\n  1.49769709e-02  5.26280813e-02  8.32905062e-03 -2.64711231e-02\\n  1.74558554e-02 -1.55906081e-02  3.07463538e-02 -1.99814402e-02\\n  3.60251367e-02  3.31349559e-02  2.88516581e-02  1.60592180e-02\\n -2.13310681e-02 -5.05927205e-02  1.80202704e-02 -7.97369257e-02\\n  8.48283991e-03 -3.32840793e-02 -2.27017831e-02  2.63711065e-02\\n -8.41593556e-03 -1.94247887e-02  4.90178214e-03  8.50942507e-02\\n -5.21804951e-02 -2.10775416e-02  3.26968133e-02  1.98855940e-02\\n -1.69840306e-02  8.71918425e-02  1.92025788e-02  2.99161728e-02\\n -7.49577135e-02 -3.59308012e-02 -5.23379184e-02  1.51459230e-02\\n -5.41425385e-02  2.80906186e-02 -2.92199422e-02 -1.38515988e-02\\n  7.05875736e-03 -1.90978888e-02  5.40076979e-02  3.25268470e-02\\n -1.63328033e-02 -4.14506998e-04 -1.46237612e-02  2.16031764e-02\\n -1.30715575e-02 -2.27168463e-02  3.50143202e-02  7.46605843e-02\\n  1.41851874e-02 -3.16289291e-02 -9.65258479e-02 -1.59746166e-02\\n -2.97232158e-02  3.96878412e-03  1.22825161e-03 -2.10935622e-02\\n -2.24989820e-02 -3.24975848e-02  1.35780498e-02 -4.56886329e-02\\n  8.49641766e-03 -4.31251153e-03 -5.11621498e-02  5.83554246e-02\\n -4.83614653e-02 -1.86819918e-02  6.57639429e-02 -2.75466125e-03\\n  1.58054624e-02 -4.15337272e-03  4.28251103e-02  1.45055549e-02\\n  3.59744914e-02 -1.50911296e-02  4.13329080e-02  2.76894998e-02\\n  5.48515655e-02 -2.18039826e-02 -3.35365124e-02 -4.70657200e-02\\n  6.20571375e-02 -3.05307899e-02  2.74623535e-03  4.00637649e-02\\n  2.02801041e-02 -6.31678710e-03  3.36272083e-02 -3.35243829e-02\\n  5.13982913e-03  2.62982957e-02 -1.64130405e-02 -5.18423803e-02\\n -3.93847302e-02  1.76417883e-02  9.52450186e-03  4.77198837e-03\\n -9.11361277e-02 -2.73146536e-02  3.52999493e-02 -1.63628645e-02\\n -7.56920949e-02 -8.27878639e-02  1.47726657e-02 -3.71734723e-02\\n  1.69266555e-02  3.37706506e-02  1.03592813e-01 -5.17487200e-03\\n -9.70060122e-04  3.88635285e-02  5.47850840e-02  3.17289028e-04\\n -5.25135314e-03 -5.22951968e-02  1.49193369e-02 -7.12398114e-03\\n -6.90913014e-03 -6.98405057e-02 -4.07549506e-03  6.58764457e-03\\n -2.46124994e-03 -4.03979942e-02  6.12172624e-03 -1.73577797e-02\\n  5.41665666e-02 -3.48936729e-02 -8.15528072e-03  5.42558357e-02\\n  1.81918722e-02  2.11584829e-02 -4.29923162e-02 -5.11293346e-03\\n  3.30377035e-02 -4.15946692e-02 -6.73250332e-02  3.78366932e-02\\n  8.33311956e-03 -4.78944182e-02  4.84220572e-02  5.40262125e-02\\n  2.35570055e-02  9.23440233e-03 -1.48962308e-02 -4.19711918e-02\\n  4.50558364e-02 -2.78086718e-02 -7.07701594e-03 -7.39722028e-02\\n -2.39478257e-02  8.90808459e-03  6.61961511e-02  3.30849504e-03\\n -3.47582884e-02  4.91283387e-02 -3.80339660e-02 -2.28492264e-02\\n  1.06618553e-03 -1.94503739e-02 -4.64285575e-02 -5.26951998e-02\\n  1.69219822e-02  1.70216113e-02  2.81076636e-02  6.96212333e-03\\n  5.39823584e-02  4.11974313e-03  4.97900806e-02 -2.77236551e-02\\n -7.10621774e-02  2.93449350e-02 -5.47534563e-02 -4.02109288e-02\\n -3.15075391e-03 -4.14767191e-02  5.87275485e-03  3.01689521e-04\\n -5.65953134e-03 -1.17786054e-03  9.17122420e-03  8.59569199e-03\\n -1.11128232e-02 -6.96009956e-04 -3.83416526e-02 -3.30501683e-02\\n  6.61531370e-03  1.67261399e-02 -5.50168641e-02 -4.18626219e-02\\n  2.46719774e-02 -2.63819788e-02 -2.63488647e-02  4.73489985e-02\\n  4.81520258e-02 -1.86441615e-02  2.46313717e-02 -1.23528186e-02\\n -1.36861764e-02  2.60302238e-02  2.91889114e-03 -7.60535076e-02\\n -3.06137484e-02  8.88359100e-02 -1.88711379e-02  5.26550710e-02\\n -4.44560283e-04 -1.44885518e-02 -3.42500135e-02 -2.17565857e-02\\n -9.44458134e-03 -7.47601781e-03 -4.57637385e-03  1.54390354e-02\\n  4.58428338e-02  1.05657494e-02 -3.67240631e-03  2.52387859e-02\\n -2.56845001e-02 -2.09541880e-02  2.51007918e-02 -3.45209315e-02\\n -2.33236048e-02  3.05067692e-02 -6.06549717e-03 -1.20975776e-02\\n  1.50724594e-02 -3.22657675e-02  2.58353725e-02 -5.86812571e-02\\n -5.90562038e-02  2.26557720e-02 -1.72098894e-02  8.91088974e-03\\n -1.32863382e-02  1.80752277e-02  5.46591692e-02  7.90057238e-03\\n  1.32817589e-02  1.96655318e-02 -4.91281413e-02  3.95470746e-02\\n -5.69648631e-02 -7.25962268e-03  3.01112887e-02  1.89773887e-02\\n -3.52526233e-02 -1.65856387e-02 -5.50267883e-02  1.73542015e-02\\n -4.56079096e-02 -4.28156890e-02 -3.25840376e-02 -1.64750293e-02\\n -1.86358802e-02 -5.81973419e-03 -3.00365202e-02 -9.13485140e-03\\n  3.33131328e-02  2.36882843e-04 -5.37511781e-02  3.91492248e-02\\n  1.56774204e-02 -5.48352040e-02  1.58548374e-02  3.31928097e-02\\n  2.70396918e-02  1.77459605e-02 -7.00878445e-03  7.38714114e-02\\n  2.41586752e-02 -4.14005807e-03  4.26365025e-02 -4.46061753e-02\\n  2.26897430e-02  5.15418872e-02  6.68798462e-02  3.99752371e-02\\n -7.13280635e-03 -2.46389713e-02  2.18200833e-02  3.28394771e-02\\n -1.17770415e-02  4.49450016e-02 -2.22493671e-02 -5.27306052e-04\\n -2.17306223e-02  4.29817811e-02 -1.05768004e-02 -8.40090960e-02\\n -1.61722545e-02  3.71182151e-02 -3.58031951e-02  4.20100987e-03\\n  8.47765431e-03 -2.59885564e-03  5.30572794e-03  2.82327849e-02\\n  3.16903889e-02 -2.39732675e-02 -2.01698598e-02  2.38499008e-02\\n -8.24732482e-02  5.36778569e-02  1.85268968e-02 -1.42100668e-02\\n -6.93547875e-02 -1.36233214e-02  3.57165784e-02  8.24762881e-03\\n  3.49095464e-02 -2.71221455e-02  3.80744748e-02 -4.13412787e-02\\n  3.93928997e-02 -1.68937780e-02 -6.10272773e-02 -4.72188815e-02\\n -2.58115027e-02  5.70528284e-02 -2.47790795e-02 -5.20046288e-03\\n -5.27144484e-02 -5.91998883e-02  4.96069640e-02 -6.30648360e-02\\n -3.98671106e-02  4.84597683e-03  9.19358134e-02 -8.59519560e-03\\n -2.01808382e-02 -1.84358656e-02  2.67479289e-02 -1.69991087e-02\\n -1.48401607e-03 -1.25369919e-03  4.19182032e-02 -6.55532567e-05\\n  1.36850160e-02  1.00121936e-02 -1.34783350e-02  4.38269489e-02\\n  3.39978524e-02  2.58215296e-04 -4.62394021e-02 -4.38442733e-03\\n  1.64935496e-02  4.79437262e-02  2.67130155e-02  1.40184294e-02\\n  7.55599514e-02 -3.72769348e-02 -9.21111703e-02 -1.20489765e-02\\n -4.44725081e-02  1.98834762e-02  1.47435870e-02  5.02232695e-03\\n -1.60016790e-02 -1.47836208e-02  6.29175734e-03 -1.27818901e-02\\n -5.46793416e-02 -1.63065863e-03  3.43641452e-02 -8.56569875e-03\\n  2.56823935e-02  3.79740261e-02 -3.23387235e-02 -1.81656424e-02\\n -9.09916777e-03 -1.89040136e-02 -5.48730791e-02 -3.42944451e-03\\n  1.34957125e-02 -2.37450713e-05  3.55082862e-02 -2.75209900e-02\\n -6.42041042e-02  4.54346240e-02  1.13246411e-01  3.25743444e-02\\n  2.35100985e-02  2.24374756e-02 -2.39400119e-02 -2.15931386e-02\\n  2.35519800e-02 -6.25196323e-02  3.35483290e-02  6.36937618e-02\\n -1.86616927e-02 -8.60128403e-02  1.60662998e-02  3.72817479e-02\\n  4.78819311e-02  6.80231750e-02  7.90307112e-03 -3.11304573e-02\\n -3.80616449e-02  4.40077148e-02  1.27940066e-02 -4.96872030e-02\\n  1.56216258e-02 -1.86634064e-02  1.96770243e-02  1.71715897e-02\\n -3.91012691e-02  1.41924024e-02  1.51793435e-02  2.28579864e-02\\n -4.60972749e-02 -6.83191121e-02 -1.71978548e-02  1.88427847e-02\\n -3.66639458e-02  1.74589865e-02 -7.61533855e-03  3.15282792e-02\\n  1.02329157e-01  3.49345021e-02  3.54699083e-02 -5.75096672e-03\\n  4.15158793e-02 -9.03912634e-03 -5.34687862e-02 -2.57986281e-02\\n -1.22461822e-02 -1.52084930e-02 -7.02621266e-02  4.62766699e-02\\n  1.83859020e-02  1.72120612e-02 -6.07737452e-02  1.54056503e-02\\n -3.57728638e-02 -3.09745204e-02  3.31078358e-02  2.61383737e-03\\n -3.21328174e-03  4.19312306e-02 -4.89622168e-03 -6.12104591e-03\\n  8.69649579e-04  5.17493710e-02  4.86879013e-02  2.82217525e-02\\n -4.47348170e-02 -1.84235927e-02 -5.12220897e-02  2.86704097e-02\\n  3.23983207e-02  2.25628968e-02 -2.46884041e-02  1.17000164e-02\\n -2.18017418e-02 -7.86873549e-02 -3.11394059e-03  1.28971459e-02\\n  5.28506525e-02  7.96411186e-03 -3.86150554e-02  2.58003324e-02\\n  2.66196337e-02 -1.27674444e-02 -1.84565987e-02 -4.34729233e-02\\n  3.36968303e-02 -1.73303690e-02 -3.22025362e-03 -6.54651024e-33\\n -1.60008483e-02 -2.52516456e-02 -4.25307639e-02  6.04866073e-02\\n  2.46696337e-03 -4.63219872e-03  1.69671774e-02 -6.63553039e-03\\n  2.08867602e-02  3.83786038e-02 -1.60984322e-02  1.27311191e-02\\n  1.11338915e-02  6.06127724e-04  7.44905276e-03 -1.44941118e-02\\n  3.78375910e-02 -3.73926684e-02  3.89369540e-02  5.42852394e-02\\n  3.05752289e-02  5.08601032e-02  3.05182692e-02 -1.56071216e-01\\n -2.55827345e-02 -5.62050892e-03 -3.81025933e-02  2.40053479e-02\\n -3.21689434e-02  2.02620551e-02 -3.06302570e-02  3.36711220e-02\\n  3.92599925e-02  6.18473534e-03  1.30217168e-02 -1.56592894e-02\\n -9.26991925e-02  2.39697937e-02 -9.23283100e-02  5.32196946e-02\\n  1.86535474e-02 -5.71182929e-02  8.23127106e-02  1.76605452e-02\\n -4.05384265e-02  1.03175305e-02  5.69872074e-02 -3.01575754e-02\\n -5.65174315e-03 -3.55171561e-02  2.36092620e-02  1.47523184e-03\\n -1.49480784e-02 -2.99577639e-02  3.49926911e-02  3.89567353e-02\\n -7.67234806e-03  3.48314829e-02 -1.18709326e-01 -2.72839359e-04\\n -1.60304818e-03 -1.15412124e-03  5.07944077e-03  5.06573878e-02\\n -5.29214516e-02  5.28849214e-02  7.38637820e-02 -2.15217983e-03\\n  1.56386383e-02 -6.39506951e-02 -7.90230744e-03  1.13515463e-02\\n  4.53637652e-02  2.87798531e-02  7.28979260e-02 -6.44681528e-02\\n -9.99129489e-02  4.62394878e-02 -4.34693173e-02 -4.21818309e-02\\n -2.38027256e-02  1.06456829e-03 -3.24884467e-02 -1.64689720e-02\\n  1.89358443e-02 -6.00590445e-02 -4.27121855e-02  3.42800692e-02\\n  2.38478580e-03  1.44398687e-02 -2.98495293e-02  4.83908923e-03\\n -1.11521361e-02 -3.02082878e-02  1.68285181e-03 -4.89873365e-02\\n  5.36197945e-02  3.12284250e-02  1.20443478e-02 -3.84554602e-02\\n -4.41040173e-02 -5.31468056e-02  8.25968338e-04  2.16238741e-02\\n -8.35427735e-03  7.32368929e-03  8.16668347e-02  2.19619777e-02\\n -3.14116739e-02  1.18946638e-02  9.10718180e-03  1.83369759e-02\\n  6.19906895e-02 -2.79957168e-02  1.23383394e-02 -3.46424580e-02\\n  1.11726345e-03  2.73512192e-02  5.98321743e-02  5.86363226e-02\\n  8.11435282e-03  9.70379822e-03 -2.51040943e-02 -5.48960932e-04\\n -2.78343409e-02 -1.90455969e-02  1.69244520e-02 -2.28798594e-02\\n  3.21981460e-02 -1.90113708e-02  1.08470712e-02 -1.30565455e-02\\n  3.14991667e-07 -2.66809836e-02 -5.04343212e-02 -3.00477515e-03\\n -5.26378490e-02  6.97110780e-03  7.83493221e-02 -3.62523943e-02\\n  2.94678062e-02  8.86607543e-03 -1.81180686e-02  3.19248997e-02\\n -3.32394056e-02  2.91425157e-02 -1.87250227e-02 -3.06681469e-02\\n -3.00318375e-02  2.38168165e-02 -5.03875897e-04 -1.59957372e-02\\n  4.62676883e-02  3.71098258e-02  2.88677029e-02  2.13550832e-02\\n -4.90668491e-02  4.14854381e-03 -7.49932230e-03  2.70599294e-02\\n -1.71404034e-02  1.87887941e-02  1.36491214e-03  4.61873449e-02\\n  1.26653286e-02  8.12521856e-03  2.87443679e-02 -2.14867461e-02\\n  2.09189709e-02  1.61148161e-02 -5.19053685e-03 -3.87330055e-02\\n -3.85548268e-03 -2.15498190e-02  8.91489685e-02  3.07485852e-02\\n -1.19851367e-03  1.92357767e-02 -4.12252955e-02 -2.16509867e-02\\n -7.90429953e-03 -6.06577769e-02  1.34265628e-02  1.63638145e-02\\n  1.48593467e-02 -7.47065339e-03 -6.29693782e-03 -3.08844894e-02\\n -2.07106234e-03 -1.21269077e-02 -5.81270382e-02  4.82779332e-02\\n  1.08680762e-02 -4.45005856e-02 -1.94260813e-02  3.93552221e-02\\n -1.12565830e-02  4.24531475e-02 -3.28674205e-02 -1.19228829e-02\\n  2.35684872e-34  1.10358431e-03 -1.65937599e-02 -1.86821464e-02\\n  3.73362340e-02 -1.61608625e-02 -1.88870244e-02 -4.17751484e-02\\n -4.10245359e-02 -7.07218016e-04 -2.73415148e-02 -1.69429779e-02]\",\n          \"[ 2.10550893e-02  1.89801585e-02  5.71878441e-03  3.61207016e-02\\n  1.63449235e-02  3.11987139e-02 -1.20841619e-02  2.30167862e-02\\n  3.31267826e-02  1.00797135e-02 -3.28415036e-02  3.42223095e-03\\n  5.07854968e-02 -2.98436210e-02  4.62613106e-02 -3.84079292e-02\\n  1.14178229e-02 -3.70757189e-03  1.48023907e-02 -2.55221333e-02\\n -1.71051398e-02 -2.53099129e-02 -1.47182755e-02 -4.14543669e-04\\n -8.95351544e-02 -6.79462962e-03 -3.74950166e-03 -1.05760833e-02\\n -3.28498136e-04  2.57104915e-02  1.39725879e-02  1.99878681e-03\\n -1.90667138e-02 -1.58404130e-02  2.33557444e-06 -5.83466981e-03\\n -1.37021318e-02 -2.32117623e-02  4.67648283e-02  5.57171972e-03\\n  2.40222253e-02  1.66733481e-03 -2.50799325e-03 -1.49622252e-02\\n  3.13951187e-02 -4.75366041e-02  5.12210093e-02  1.26882464e-01\\n  1.34473536e-02 -5.59561187e-03 -2.55274605e-02 -2.81702206e-02\\n  2.44509690e-02  6.64423453e-04  4.34606932e-02 -7.55074397e-02\\n  6.12650923e-02 -4.06524278e-02 -4.50989231e-02  5.21418359e-03\\n -1.01586878e-02  5.06902449e-02  2.10652929e-02  2.26580966e-02\\n -3.56616161e-04  2.36960407e-02  4.87537906e-02 -1.77030116e-02\\n -5.15145110e-03  2.65531931e-02  2.41775922e-02  6.33520558e-02\\n  3.10066100e-02  3.01770982e-03  4.75547090e-02 -1.85832288e-02\\n -7.11838203e-03 -1.26520311e-02  2.02992149e-02  9.95948445e-03\\n -5.48179559e-02 -7.52569817e-04  2.27128975e-02 -1.73370633e-02\\n  6.56787772e-03  5.57598919e-02  1.17403474e-02 -3.72796180e-03\\n -3.91943082e-02  1.86832354e-03  3.32107358e-02 -1.49729196e-02\\n  3.36530171e-02  2.95902956e-02  5.46592521e-03  4.58259461e-03\\n -2.90288217e-02 -3.28444876e-02 -1.35652057e-03 -6.06179871e-02\\n  3.71856522e-03 -1.69415288e-02 -5.45698628e-02  2.45840158e-02\\n -7.59849185e-03 -1.50636565e-02 -8.47218186e-03  5.32102026e-02\\n -6.50225505e-02 -5.03194407e-02  2.36310083e-02  1.10258218e-02\\n -2.34587602e-02  8.78079608e-02  3.00543215e-02  2.67926455e-02\\n -7.23311529e-02 -1.17220301e-02 -5.30933179e-02  1.13221398e-02\\n -3.92442457e-02  2.67230682e-02 -1.51080098e-02 -4.77697589e-02\\n  1.79316681e-02 -8.59030639e-04  3.59218940e-02  2.91775428e-02\\n -3.31706330e-02  3.77702937e-02  1.36903091e-03  8.35342146e-03\\n  1.89626124e-03 -2.43077707e-02  3.42880972e-02  3.29875052e-02\\n  7.69419642e-03 -2.31320355e-02 -9.50938687e-02 -2.54366994e-02\\n -6.33284748e-02 -7.87501875e-03 -4.01410311e-02 -2.52976362e-02\\n  4.21809219e-03 -1.91590991e-02  1.91203095e-02 -4.71791923e-02\\n -6.49152696e-03 -3.01919281e-02 -8.48763958e-02  8.02073702e-02\\n -5.62297367e-02 -5.34940744e-04  5.98467849e-02 -2.08579022e-02\\n  4.19790410e-02 -2.13981401e-02  3.93503457e-02  1.88375879e-02\\n  2.61032544e-02 -1.50902979e-02  3.71263549e-02  1.84293669e-02\\n  4.00742255e-02 -1.26321176e-02 -5.97902201e-02 -6.44343123e-02\\n  7.20704123e-02 -5.84760159e-02 -2.17250101e-02  3.41655165e-02\\n  1.46639831e-02  1.43233510e-02  2.03447491e-02 -3.15894522e-02\\n  4.19319421e-03  1.06328940e-02 -2.42646132e-02 -5.86020872e-02\\n -5.03181666e-02  2.90372111e-02  3.53418998e-02  2.17625890e-02\\n -9.06326696e-02 -1.02755288e-03  8.53646770e-02 -1.25846323e-02\\n -5.49325161e-02 -9.24001709e-02  1.89238582e-02 -5.07488661e-02\\n  2.60304473e-02  1.35270944e-02  1.25795454e-01 -1.50260795e-03\\n  4.51929029e-03  2.21446138e-02  6.39135242e-02 -1.84141267e-02\\n  3.65030183e-03 -1.99334752e-02  2.24405248e-02 -1.09270550e-02\\n  1.66823678e-02 -6.97771832e-02 -1.85789969e-02  5.25896763e-03\\n -1.11887557e-02 -6.43005520e-02 -2.39531416e-02 -1.37542179e-02\\n  2.33191568e-02 -2.62643043e-02 -2.57731676e-02  3.58114243e-02\\n  3.34087424e-02  1.42173152e-02 -7.95787275e-02 -1.45705696e-02\\n  2.90386640e-02 -6.27638251e-02 -1.47985416e-02  2.12667901e-02\\n  2.48956978e-02  3.17415744e-02  4.52817269e-02  6.52571097e-02\\n  2.83715241e-02  5.03486395e-02 -1.36963266e-04 -6.20558336e-02\\n  4.83775735e-02 -1.23163173e-02  1.51452031e-02 -5.49965352e-02\\n -1.05187679e-02 -1.03535131e-02  6.87807724e-02  1.43374102e-02\\n -4.68065552e-02  4.32082228e-02 -1.07185207e-02 -5.39556965e-02\\n  3.09304874e-02 -1.54443132e-02 -3.41038369e-02 -4.32374962e-02\\n  1.79398973e-02  8.89223628e-03  1.23871127e-02  8.80784914e-03\\n  6.65723234e-02 -2.30019391e-02  6.34150952e-02 -5.62867522e-03\\n -8.36487114e-02  2.69037653e-02 -2.07156520e-02 -3.92409824e-02\\n  3.75115946e-02 -2.67354194e-02 -8.53903778e-03  1.74122918e-02\\n -3.27650234e-02  1.00899599e-02  3.52287618e-03 -2.35018739e-03\\n -1.93762034e-02 -1.06236502e-03 -6.26200512e-02 -3.21941599e-02\\n  2.29684729e-02  4.90316898e-02 -6.60090148e-02 -1.74961565e-03\\n  6.25856221e-02  4.86720819e-03 -2.94932853e-02  5.20597324e-02\\n  5.39600439e-02 -4.78354096e-02  2.78273113e-02 -7.35508138e-03\\n -7.36295292e-03  2.90202759e-02  2.22777687e-02 -1.16647348e-01\\n -4.32230160e-02  7.46734887e-02 -3.46202217e-02  1.37035223e-02\\n -8.42871610e-03 -6.60846781e-05 -2.76261494e-02 -3.48049700e-02\\n  2.77007057e-04 -2.66524535e-02 -4.39391052e-03 -1.40611697e-02\\n  3.59997302e-02 -1.59557082e-03 -5.07120136e-03  2.51615681e-02\\n -3.52887809e-02 -1.54184764e-02  2.18697544e-02 -3.13306488e-02\\n -1.94293074e-02  1.06538730e-02 -7.54684675e-03 -1.65617652e-02\\n -7.22304545e-03 -3.92016247e-02  2.70127244e-02 -5.24496026e-02\\n -7.70510286e-02 -8.09387956e-03 -1.79232247e-02  1.37755172e-02\\n -3.27942744e-02  1.44116506e-02  4.56037410e-02  2.31570774e-03\\n -8.11290555e-03  1.29549215e-02 -4.39356230e-02  2.47745868e-02\\n -1.28647899e-02  1.52695952e-02  4.30341549e-02  2.51338910e-02\\n -6.84472248e-02 -8.67999066e-03 -2.96621528e-02 -1.51238563e-02\\n -1.41085675e-02 -1.59868523e-02 -1.26079982e-02 -2.01590955e-02\\n  1.29859028e-02  2.53185388e-02  6.41601905e-03  5.19341137e-03\\n  6.11069053e-02 -2.45128106e-02 -1.25904996e-02  2.58439910e-02\\n -3.36897485e-02 -3.93188260e-02  2.65237056e-02  2.52613351e-02\\n  4.78105322e-02 -5.17255487e-03 -4.23825765e-03  4.41568941e-02\\n -2.15242784e-02 -2.71178447e-02 -1.01225311e-02 -5.65900765e-02\\n -1.33249387e-02  4.68344577e-02  6.63645715e-02  1.73682291e-02\\n -1.23711200e-02  1.91569887e-03  3.42697389e-02  5.50557449e-02\\n -2.34624576e-02  1.67622492e-02 -1.21611217e-02  2.99172904e-02\\n  1.40139251e-04  4.67966422e-02  1.08778467e-02 -8.52033868e-02\\n -2.38133222e-02  1.19318524e-02 -3.27914394e-02  2.53525190e-02\\n -3.47539812e-04 -7.08718598e-03  4.51942421e-02  2.33369414e-02\\n  2.51474790e-02  1.64955538e-02 -9.72430874e-03  2.45660543e-02\\n -4.50734645e-02  4.90039922e-02  2.81872228e-02 -2.21484657e-02\\n -7.09706098e-02 -2.02374198e-02  3.41517068e-02  8.35860707e-03\\n  1.83284432e-02 -4.46018996e-03  3.18413153e-02 -4.75112200e-02\\n  2.12556645e-02 -1.04751857e-02 -3.75003777e-02 -5.07965945e-02\\n -2.57231183e-02  5.26337996e-02 -2.29738355e-02 -8.45240150e-03\\n -5.74463420e-02 -4.37313840e-02  5.91497608e-02  1.07909460e-02\\n -3.00474465e-02 -1.06473705e-02  7.36638606e-02  8.01240653e-03\\n -4.00637090e-02 -1.92075353e-02  4.62723821e-02 -1.58027317e-02\\n -2.36656312e-02 -3.32798325e-02  5.18313833e-02  9.83862765e-03\\n  2.74150092e-02  1.04317926e-02 -2.88804919e-02 -3.15335440e-03\\n -6.31727395e-04 -1.35425152e-02 -5.81718124e-02  3.05105653e-02\\n  2.07314510e-02  5.00274710e-02  4.52587605e-02  3.76195200e-02\\n  3.64865698e-02 -7.30005503e-02 -7.07912967e-02 -2.05901098e-02\\n -3.02602220e-02  5.02636358e-02  2.83411238e-02 -8.70072469e-03\\n -9.04226303e-03 -1.66041534e-02 -1.16284462e-02 -2.23443564e-03\\n -4.19639349e-02  5.06043993e-03  2.38951892e-02 -1.48712322e-02\\n  3.35972086e-02  2.13164184e-02 -4.90135513e-02 -1.77328978e-02\\n -3.97091061e-02  1.18422266e-02 -5.32879047e-02 -2.47810129e-02\\n  1.51711609e-02 -2.60349154e-03  2.70625595e-02 -2.31936499e-02\\n -6.14969917e-02  1.29095754e-02  9.83934700e-02  4.58643921e-02\\n  4.18215580e-02  8.02183151e-03 -5.94617836e-02 -1.12223206e-02\\n  9.18268692e-03 -6.06421418e-02  2.59701051e-02  2.51836739e-02\\n -1.07894707e-02 -7.08993152e-02 -3.63337924e-03  5.10058664e-02\\n  5.69999442e-02  9.25690308e-02  3.10551263e-02 -1.20254382e-02\\n -4.56252545e-02  8.64332821e-03 -3.32425348e-04 -6.01316504e-02\\n  9.21596773e-03 -9.70432162e-03  4.14767973e-02  4.04620776e-03\\n -2.31341179e-02  2.17653625e-02 -4.50751046e-03  2.59834435e-02\\n -3.29296440e-02 -5.98579496e-02 -1.15644159e-02 -2.37335060e-02\\n -3.30731198e-02  1.60827823e-02  1.02382544e-02  1.20227877e-02\\n  7.97529593e-02  1.68122537e-02  2.00964138e-02 -3.11609022e-02\\n  3.05634942e-02  1.06220208e-02 -7.57162347e-02 -1.54893734e-02\\n -8.92875157e-03  7.03877304e-03 -7.91718960e-02  2.95857992e-02\\n  1.58436764e-02  3.75731885e-02 -2.88633667e-02  1.80778131e-02\\n -3.23485844e-02 -3.74564081e-02  2.91445535e-02  1.80075057e-02\\n -3.03092748e-02  1.80427786e-02 -1.00802584e-02  3.67544312e-03\\n  4.75678826e-03  6.62693754e-02 -1.18051646e-02  3.06721739e-02\\n -6.08768016e-02 -1.74490940e-02 -4.43661287e-02  1.05364025e-02\\n  1.15400022e-02  3.71641517e-02 -3.09709627e-02  1.75388679e-02\\n  3.47353220e-02 -7.00471997e-02  9.15592536e-03  4.06794390e-03\\n  3.42130065e-02  1.86223872e-02 -2.98176426e-02  4.08786722e-02\\n  5.43165952e-02  6.52609719e-03 -2.59359926e-02 -7.17616528e-02\\n  9.22450125e-02 -3.13151628e-02  3.16555351e-02 -6.20345177e-33\\n -3.97616550e-02 -4.32634391e-02 -4.77800556e-02  1.93742476e-02\\n -1.66403651e-02 -3.21341269e-02  1.89297404e-02  3.54606286e-02\\n  3.91195863e-02  4.04801033e-02 -1.61202240e-03  8.35716259e-03\\n  9.62217711e-03 -6.56917598e-03  2.05796883e-02  3.61313447e-02\\n  4.92361784e-02 -6.74197031e-03  4.76930328e-02  2.67273840e-02\\n  3.49291638e-02  5.36606945e-02  2.41849180e-02 -1.34477630e-01\\n -1.03883259e-03 -1.54035294e-03 -3.86975072e-02  6.76503778e-03\\n -9.26031359e-03  1.55651271e-02 -8.96859821e-03  1.75948236e-02\\n  4.60903458e-02 -4.73510846e-03 -2.69889496e-02 -3.16790938e-02\\n -7.17270747e-02  1.35300178e-02 -5.62191121e-02  4.59308401e-02\\n -2.49934793e-02 -4.66031693e-02  7.53040388e-02  1.06864898e-02\\n -1.89684872e-02  1.81733072e-02  5.91524579e-02 -5.14261127e-02\\n -9.83419456e-03  7.88325910e-03  1.81071181e-02 -3.76527291e-03\\n -1.83195211e-02 -2.78006587e-02  2.32995730e-02  7.27591664e-02\\n -3.62907094e-03  4.40579988e-02 -8.61914456e-02  1.11711808e-02\\n -1.53791150e-02  7.66110723e-04 -3.78105277e-03  3.30510326e-02\\n -4.98042703e-02  4.64404747e-02  9.33773443e-02 -7.22056255e-03\\n -1.64992223e-03 -6.27262518e-02 -4.77778055e-02  4.27918360e-02\\n  2.67219245e-02  2.90619433e-02  2.08942555e-02 -7.48944506e-02\\n -6.54795915e-02  6.42420799e-02 -7.35235363e-02 -1.00560449e-02\\n -3.65290395e-03  1.26981540e-02 -1.56423599e-02 -9.73427296e-03\\n -1.28238192e-02 -3.85917202e-02 -5.87402843e-02  3.04584149e-02\\n -4.04084381e-03  2.49313693e-02 -5.67005314e-02  1.33919269e-02\\n -1.89131424e-02 -3.15722898e-02  3.21894651e-03 -4.41263430e-02\\n  2.45475750e-02  8.99827555e-02  2.20796280e-02 -2.66138166e-02\\n -1.19491247e-02 -2.76923049e-02 -1.28650535e-02  7.12957419e-03\\n -2.25821454e-02 -1.69143192e-02  6.81783184e-02  1.11499662e-02\\n -1.71439964e-02  1.42560815e-02  3.63804288e-02 -1.08883176e-02\\n  3.53411213e-02 -5.83006181e-02  3.36391218e-02 -2.69964896e-02\\n -2.30455282e-03  5.23405634e-02  4.33231033e-02  4.90481779e-02\\n  1.33651327e-02 -4.64103669e-02 -2.59182081e-02 -1.68336127e-02\\n -1.89030226e-02 -1.74713600e-02  2.60974318e-02 -1.99088696e-02\\n  5.44507168e-02  1.45052513e-02 -1.11594331e-03 -9.60233342e-03\\n  3.04299846e-07 -1.00857727e-02 -1.55406557e-02  1.85955670e-02\\n -3.17204408e-02 -1.62142765e-04  5.42464964e-02 -3.50641795e-02\\n  2.57112663e-02  2.79086339e-03 -1.64898746e-02  3.62174623e-02\\n -6.17642701e-02  1.56645067e-02 -1.73774213e-02 -1.72441639e-02\\n -2.23114286e-02  6.26773247e-03  1.05404733e-02 -2.50976980e-02\\n  2.42613479e-02  2.05559433e-02  9.80118103e-03  9.92734078e-03\\n -3.56878899e-02  1.13434866e-02  8.55076034e-03  3.89316976e-02\\n -4.32687923e-02 -1.54835414e-02 -2.61044782e-03  3.95319499e-02\\n  4.66551306e-03  2.99613420e-02  1.16096577e-02  4.86277323e-03\\n  1.74418688e-02  3.39161456e-02 -2.82859839e-02 -2.31307577e-02\\n  1.88201899e-03  1.31329307e-02  9.05935541e-02  2.17796825e-02\\n  3.65023268e-03  3.51014845e-02 -1.33884549e-02 -2.65007769e-03\\n  2.08782628e-02 -6.96348697e-02  4.43855897e-02  3.24460045e-02\\n  1.60273723e-02  2.95764813e-03  7.47646773e-05 -1.58960652e-02\\n  1.84539519e-02  1.45285120e-02 -4.08691652e-02  7.89841861e-02\\n  2.82303784e-02 -5.00131473e-02  1.40842153e-02  5.80927506e-02\\n -2.54775900e-02  3.22881266e-02 -2.27088910e-02 -1.20208962e-02\\n  2.09936403e-34  1.76231228e-02 -3.00002657e-02  1.28547885e-02\\n  1.13205817e-02 -4.71676476e-02 -2.91008372e-02 -8.25969204e-02\\n -5.26107214e-02 -2.30194032e-02 -3.66974548e-02 -2.09685657e-02]\",\n          \"[-5.02650300e-03 -1.63045265e-02 -1.31100230e-02  4.33680005e-02\\n -4.92987130e-03 -3.49951386e-02  1.65077168e-02  1.37604671e-02\\n  3.36855389e-02  1.13667790e-02 -4.35674284e-03  4.90542268e-03\\n  4.99444120e-02  1.67168397e-02  1.27788205e-02  2.21423116e-02\\n -1.33845061e-02 -1.90101936e-02  3.51010300e-02 -4.00180779e-02\\n -1.04247127e-02 -1.67737547e-02 -1.79546811e-02  7.27909943e-03\\n -6.19723946e-02  2.90275761e-03 -1.81817356e-02 -4.75392473e-04\\n -5.95924305e-03 -2.10406240e-02 -1.60495355e-03  6.27872255e-03\\n  5.52013405e-02 -1.36806548e-03  2.03610011e-06 -1.95495971e-02\\n  3.64414901e-02 -2.74337195e-02  5.28488159e-02  4.27613296e-02\\n  2.03688089e-02  8.74717440e-03  5.79887209e-03 -1.17650479e-02\\n -2.48321947e-02 -4.72220741e-02  2.04897169e-02  7.82550052e-02\\n  3.08863837e-02  4.38715294e-02 -1.40678138e-02 -3.22036073e-02\\n  2.32335776e-02  1.67954527e-02  1.14904977e-02 -7.91249946e-02\\n  5.03912680e-02 -3.29025313e-02 -3.15775871e-02 -7.40215480e-02\\n  1.34744225e-02  1.10284947e-01  1.61819905e-02 -2.41398904e-02\\n  4.56023216e-02  1.91699229e-02  2.88358442e-02 -1.67704206e-02\\n -2.59557683e-02  2.95680594e-02 -5.57287689e-03  3.15533280e-02\\n  9.24281310e-03  6.35805400e-03  2.75607826e-03 -8.61617830e-03\\n  2.21220106e-02 -6.14521466e-02  3.56268249e-02 -2.99768839e-02\\n -5.37714325e-02 -1.44077502e-02  9.62754770e-04 -3.71162705e-02\\n  1.37411850e-02  9.27525610e-02 -5.75172389e-03 -2.85466686e-02\\n  2.78969873e-02 -3.28483097e-02 -2.75465231e-02 -2.44959798e-02\\n  5.11158071e-02  4.07042541e-02 -5.23622967e-02 -1.97942872e-02\\n -2.58114864e-03 -6.73707947e-02 -7.29596838e-02 -6.45026285e-03\\n  3.96099389e-02 -2.80221067e-02  2.91773956e-02  6.25106925e-03\\n -6.27069846e-02  8.84441063e-02 -4.37287241e-02  2.94731054e-02\\n -4.85000759e-02  2.78154183e-02  7.13761058e-03  2.37795128e-03\\n -6.35227263e-02  6.68619350e-02  5.19412346e-02  8.35098792e-03\\n -2.75774691e-02 -2.74821240e-02 -2.13864427e-02 -2.34688912e-02\\n -5.50815230e-03  5.40469773e-02  7.88343102e-02  8.37527274e-04\\n -4.01798002e-02  1.96144953e-02  1.31005645e-02  1.86969191e-02\\n -5.46793081e-02  6.23367541e-02  4.23825085e-02  1.43853622e-02\\n  4.63623833e-03  2.75742523e-02  5.58997446e-04  1.09079480e-01\\n -4.56170272e-03  1.77542381e-02 -1.02544673e-01  3.90172713e-02\\n -5.18423412e-03 -1.62668731e-02  2.98979897e-02 -5.07199131e-02\\n  3.63426395e-02 -3.76277752e-02 -2.22063083e-02 -4.05947417e-02\\n -3.81223066e-03 -5.40593080e-02  7.39217456e-03  5.34682833e-02\\n -3.20467092e-02  1.11346750e-03  1.10776901e-01 -2.15262137e-02\\n -1.09319333e-02  3.23990285e-02  4.80807945e-02  1.87092740e-02\\n -7.92732090e-03 -2.85938829e-02 -2.87770563e-05  1.75603423e-02\\n  4.60636197e-03  7.97169376e-03 -5.74569330e-02 -5.73838614e-02\\n -7.79129611e-03  4.24176715e-02 -1.23197772e-02 -3.72578070e-04\\n  2.12073121e-02 -4.64455970e-02  5.26848696e-02  9.27272737e-02\\n  4.46279533e-02 -1.36348850e-03 -3.50575615e-03  2.28006784e-02\\n  1.03465449e-02 -4.06493945e-03  2.77979020e-02  5.40212542e-02\\n -4.38513570e-02 -3.25832926e-02 -2.03794912e-02  2.57235691e-02\\n -3.97239402e-02 -6.46620616e-02 -8.93722608e-05 -4.74465042e-02\\n -1.22287981e-02 -1.42379925e-02  4.80975471e-02 -3.97960506e-02\\n -4.09353226e-02  4.97020036e-02  2.06555817e-02  1.10609280e-02\\n -1.90973468e-02 -2.60458514e-03  3.29016224e-02  9.74788051e-03\\n -2.92438976e-02 -3.18574570e-02 -4.10965644e-02 -2.22354364e-02\\n -3.37729640e-02 -8.36217310e-03 -2.59122369e-03  2.97181066e-02\\n  9.79668275e-03 -3.73520851e-02 -1.66206509e-02  4.11350355e-02\\n  2.62783607e-03 -3.66879883e-03 -2.39171386e-02 -2.75231320e-02\\n  3.32529023e-02 -2.82130502e-02 -5.58216125e-02 -1.68292802e-02\\n -3.21670622e-02  1.12615693e-02  1.84925776e-02  7.12051913e-02\\n  1.75425746e-02  4.19323146e-02  6.63588867e-02 -4.07905243e-02\\n  1.71857458e-02 -3.73353213e-02 -4.93357070e-02 -4.75574024e-02\\n  2.52243690e-02 -1.42559810e-02  3.10694892e-02 -4.39821854e-02\\n -1.02730086e-02  6.78353906e-02 -2.02846061e-03 -3.70264240e-02\\n  5.07751852e-02 -2.64259838e-02 -4.77991402e-02 -2.16283780e-02\\n -4.86246543e-03 -4.10388708e-02 -1.89552661e-02 -8.27519316e-03\\n  1.89811066e-02 -1.35328947e-02  3.46252508e-02  9.23128333e-03\\n  1.12329870e-02  9.68936738e-03 -5.10754846e-02 -2.61946525e-02\\n -2.08158186e-03 -6.13383204e-02 -2.02137958e-02  2.27728486e-03\\n  5.04878946e-02  7.39012286e-02  1.03999581e-02  7.81034026e-03\\n -3.67905013e-02  1.65387765e-02 -1.42758433e-02  2.51340847e-02\\n -3.18930857e-02  1.68860778e-02  3.38679999e-02 -3.83110382e-02\\n  1.00748844e-01 -1.74808148e-02 -5.67117296e-02 -2.93414545e-04\\n  2.83654574e-02 -6.69135153e-02 -1.20936679e-02 -2.54427232e-02\\n -3.16555165e-02  6.04922920e-02  1.82029717e-02 -7.10605904e-02\\n -6.11719862e-02  8.78562108e-02 -6.44140393e-02  3.17308903e-02\\n  2.16577705e-02 -2.79205218e-02 -2.94613224e-02 -5.77985086e-02\\n -1.88140031e-02  3.73069197e-02 -2.12553572e-02  3.02123129e-02\\n  3.56285423e-02  5.81496023e-02 -4.80643148e-03  2.26455666e-02\\n -3.28073800e-02  1.21884607e-02 -7.01550115e-03 -5.86306155e-02\\n  2.97600757e-02  3.10836881e-02  5.20293182e-03  1.30642718e-02\\n  1.82039049e-02 -1.94258746e-02  1.63498789e-03 -6.17429540e-02\\n -4.49103117e-02  3.90434116e-02 -2.03683693e-02 -8.76382925e-03\\n  7.92844407e-03 -1.77045986e-02  4.22687531e-02 -1.43178254e-02\\n  1.84052810e-02 -2.79208142e-02 -7.32878968e-02 -2.60759937e-03\\n -7.99800232e-02  3.72268865e-03  2.54688226e-02 -2.54774038e-02\\n -3.48838381e-02  1.98384635e-02  3.51533741e-02  2.21301727e-02\\n  5.72798867e-03  3.11061484e-03  3.53186280e-02  2.39787549e-02\\n -1.06955497e-02  1.18464939e-02 -7.54129607e-03  4.91281645e-03\\n  1.25273066e-02 -2.58180816e-02 -4.00774665e-02 -1.09494831e-02\\n  2.75417529e-02  2.82497518e-02  7.86571950e-03  3.15210484e-02\\n -2.64260508e-02  1.69581231e-02 -2.47243159e-02  6.02537915e-02\\n  1.52206747e-02  9.94152296e-03  3.63663002e-03  2.68190131e-02\\n  3.36210206e-02 -1.84716061e-02  3.79068665e-02  4.39042374e-02\\n -1.00654483e-01  1.06939964e-03  3.20874713e-02  6.89372327e-03\\n -3.56933242e-03  3.24770287e-02 -4.09167400e-03  2.35849936e-02\\n  3.15119177e-02  3.14412266e-02  8.10014736e-03 -7.87148327e-02\\n -3.47753838e-02 -4.88627562e-03  1.01125939e-03  2.27968134e-02\\n -3.99874663e-03  6.30389201e-03  9.21630953e-03 -1.85778961e-02\\n  1.03306305e-02 -6.05335133e-03 -2.10090075e-02 -2.72874553e-02\\n -2.88013704e-02  1.12993959e-02  2.05001025e-03 -6.86195195e-02\\n -6.71750382e-02 -6.79582506e-02 -3.08770817e-02  1.51825706e-02\\n -2.07352284e-02  3.75482230e-03  2.16188524e-02 -3.85059156e-02\\n  3.69790290e-03 -9.55864321e-03 -1.72178205e-02 -2.55725719e-02\\n -3.10452618e-02 -8.27065296e-03  1.87800098e-02 -6.32077968e-03\\n -3.27332579e-02 -1.94835011e-02  5.96688241e-02  4.45814915e-02\\n  9.31648351e-03 -5.70418350e-02  1.88165326e-02  6.92663565e-02\\n  1.86110977e-02 -6.40688986e-02  5.28625213e-02 -4.83384170e-02\\n  5.64896092e-02  6.56382553e-03  8.55504274e-02  1.62183810e-02\\n -1.29597550e-02  8.18856526e-04  4.64648269e-02  3.99082527e-02\\n -7.24321371e-03  3.97744477e-02 -6.58186898e-02  7.27337785e-03\\n  1.14133244e-03  2.40641721e-02  3.33199240e-02 -1.16446689e-02\\n  8.96966271e-03 -2.96781901e-02 -4.49618585e-02 -3.55669744e-02\\n  3.39149795e-02  2.25346396e-03  2.63778511e-02  3.02993786e-02\\n -3.18737961e-02  5.05602024e-02 -3.95209119e-02  4.50587552e-03\\n -2.07252949e-02  2.23710947e-02  2.59856693e-02 -2.66035646e-02\\n  9.03548002e-02  7.47564659e-02 -2.90162731e-02  1.37752211e-02\\n -3.61123271e-02 -4.18010466e-02 -2.84322686e-02 -1.38457436e-02\\n  2.99200509e-02  1.36877615e-02  1.39532785e-04 -5.27466759e-02\\n -3.43236811e-02 -1.24118133e-02  1.31492699e-02  4.34152633e-02\\n  5.43314517e-02  1.41727021e-02  1.83264278e-02  5.26886899e-03\\n  3.71423811e-02 -3.98526192e-02  4.38967645e-02  6.34267228e-03\\n -1.22539746e-02 -3.42415646e-02 -3.79617587e-02 -1.24087129e-02\\n  6.56640157e-02  8.34976360e-02  3.68557163e-02 -5.71986958e-02\\n -4.60254401e-02  3.67322043e-02  7.78329372e-03  8.02659150e-03\\n -2.08190642e-02  8.83329660e-03 -5.82692912e-04  8.94164965e-02\\n  2.27698106e-02  5.42157190e-03  8.67346823e-02 -2.82600876e-02\\n -2.66551971e-02  3.50537077e-02  1.36176953e-02  7.84636661e-03\\n -2.98852138e-02  2.93436889e-02 -7.41368672e-03  5.49763851e-02\\n  5.40525541e-02 -7.11374916e-03  1.48725521e-04  2.21125390e-02\\n  8.15050583e-03 -3.44700217e-02 -5.24259023e-02 -2.98891328e-02\\n -1.94307361e-02 -4.65467349e-02 -8.60579498e-03  1.22706974e-02\\n  6.47916943e-02  1.78974010e-02 -3.76546010e-03  1.53581714e-02\\n -1.08335102e-02 -2.30177473e-02  7.42838606e-02 -9.96507239e-03\\n  3.97315361e-02  5.18509820e-02 -1.58685390e-02  1.25480648e-02\\n  1.37244025e-02  1.22133289e-02  3.03020310e-02 -3.60305086e-02\\n  2.18285862e-02  1.25431819e-02  3.86894681e-03  3.31762768e-02\\n -1.52920710e-03  3.32675837e-02 -5.95464222e-02 -3.30562564e-03\\n  1.60720199e-02 -2.88753975e-02  3.61381695e-02 -2.04594210e-02\\n  4.16915119e-02 -1.37414597e-02  2.04974823e-02  6.02054643e-03\\n -1.90799162e-02  3.95728536e-02  3.47258747e-02 -2.96686813e-02\\n -3.48065645e-02 -3.14850062e-02  1.12888794e-02 -5.68590071e-33\\n -1.67306140e-02 -9.38637778e-02 -4.29282971e-02  6.29248470e-02\\n -2.11301614e-02  8.82522855e-03  2.47774869e-02 -1.63794253e-02\\n  1.54733495e-03 -3.09393164e-02  3.08033335e-03 -2.29943823e-02\\n  1.03297774e-02 -5.09993825e-03 -3.93798426e-02 -3.55011560e-02\\n -6.07458875e-03 -3.99557501e-02  1.24604227e-02 -5.47390953e-02\\n  8.78669973e-03  7.55866095e-02  4.24702372e-03 -7.62498602e-02\\n -7.87913278e-02  4.43841666e-02 -1.72223169e-02  1.55514143e-02\\n -4.67012227e-02 -1.86088495e-02  1.53890289e-02  2.39602607e-02\\n -1.30381668e-02 -1.83633752e-02  1.18389232e-02  8.61477386e-03\\n -5.27794622e-02 -4.64922823e-02 -1.07869068e-02  5.83573990e-02\\n -4.05454598e-02 -3.43377814e-02  6.12191856e-02  3.99205983e-02\\n -9.24234465e-02 -1.19990334e-02  5.29215671e-02  2.59640440e-02\\n -5.60065359e-03 -1.43786119e-02 -3.84456804e-03 -1.46385441e-02\\n  3.10907001e-03  3.65709909e-03  5.24655469e-02  3.10400501e-02\\n -4.20712940e-02  1.17827933e-02 -6.65510595e-02  9.94447432e-03\\n  1.03791375e-02  3.79463620e-02  2.43467689e-02  3.45929936e-02\\n  2.52195355e-03  1.60819534e-02  9.89828780e-02 -4.07777317e-02\\n -1.96477380e-02 -5.76713979e-02  8.99881497e-03 -5.10615576e-03\\n  3.45978439e-02 -3.57036777e-02  1.74144246e-02 -7.42156506e-02\\n -7.68209323e-02  1.65534485e-02 -4.62021381e-02  3.86832398e-03\\n -2.05261540e-02  9.13857855e-03  1.86924953e-02 -1.68427248e-02\\n -1.47324344e-02 -7.11621046e-02 -4.07226942e-02  6.35616574e-03\\n -1.73617434e-02 -1.92896686e-02 -9.46418568e-03 -3.47999893e-02\\n -2.87268460e-02 -9.12435446e-03  6.85637072e-02 -4.82475981e-02\\n  3.99932750e-02  4.17459272e-02  1.07232993e-02  6.87543396e-03\\n -2.47262064e-02  8.08636192e-03 -2.51098759e-02 -2.76380219e-02\\n  3.99984419e-02  3.16511951e-02  1.94380525e-02  1.80274304e-02\\n  2.85509583e-02  6.38158806e-03  4.79359366e-03 -2.65731793e-02\\n  1.76152978e-02  1.16506750e-02  4.24177349e-02 -8.04124549e-02\\n  1.65631133e-03  1.11457266e-01  1.49206426e-02  4.25606892e-02\\n  2.84698978e-03  1.85867827e-02  3.70982848e-02  2.67802775e-02\\n -8.30292925e-02  8.97822529e-03 -2.34887395e-02  3.70226502e-02\\n -5.74626913e-03 -2.16810089e-02 -3.17147076e-02  1.12252217e-03\\n  2.83076190e-07 -4.98757921e-02  1.99220292e-02  1.69941888e-03\\n  6.64287386e-03  7.53050623e-03  1.66779440e-02 -4.17935811e-02\\n -2.33475808e-02  8.51469673e-03 -2.09399201e-02  1.88568011e-02\\n -3.31888311e-02 -1.60999931e-02 -2.68999338e-02 -4.22972664e-02\\n -3.33689265e-02  3.36070657e-02  2.57256497e-02 -4.30287531e-04\\n  3.88811938e-02  3.42530571e-02 -3.19535770e-02  4.91065048e-02\\n -3.97773497e-02 -2.72317138e-02  6.25792379e-03  9.12534725e-03\\n -4.02994687e-03  2.39866860e-02 -4.83539738e-02  4.21564355e-02\\n -3.36128883e-02 -1.93151832e-02 -3.24515137e-03 -1.30933737e-02\\n  3.54845598e-02  3.90090570e-02  1.30847730e-02 -1.06283929e-02\\n -3.72805633e-02  6.46295473e-02  6.86313659e-02  2.66691856e-03\\n  2.89330054e-02  7.01903133e-03 -5.17417677e-03 -4.40275185e-02\\n -1.39506273e-02  3.73636093e-03 -2.66982689e-02  2.93009784e-02\\n -4.79575731e-02  3.98344435e-02 -4.40889746e-02 -3.97811793e-02\\n -4.06089574e-02  2.63695568e-02 -2.69993651e-03  3.17324363e-02\\n  3.43339406e-02 -3.06906621e-03 -1.02966372e-02  1.12607041e-02\\n  1.95876211e-02  1.20797291e-01  1.00848870e-02 -2.11252719e-02\\n  2.61132190e-34 -1.88326035e-02 -2.39085928e-02  3.60764191e-02\\n  4.67810445e-02 -4.16606925e-02 -4.54816297e-02 -1.12112060e-01\\n -1.04470057e-02 -1.25573082e-02 -3.05244476e-02 -5.45277772e-03]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"words\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"['This', 'article', 'has', 'been', 'accepted', 'for', 'inclusion', 'in', 'a', 'future', 'issue', 'of', 'this', 'journal.Content', 'is', 'final', 'as', 'presented,', 'with', 'the', 'exception', 'of', 'pagination.IEEE', 'TRANSACTIONS', 'ON', 'NEURAL', 'NETWORKS', 'AND', 'LEARNING', 'SYSTEMS', '1', 'Harnessing', 'Side', 'Information', 'for', 'Classi\\ufb01cation', 'Under', 'Label', 'Noise', 'Yang', 'Wei', ',', 'Chen', 'Gong', ',', 'Member,', 'IEEE,', 'Shuo', 'Chen', ',', 'Tongliang', 'Liu', ',', 'Member,', 'IEEE,', 'Jian', 'Yang', ',', 'Member,', 'IEEE,', 'and', 'Dacheng', 'Tao', ',', 'Fellow,', 'IEEE', 'Abstract\\u2014Practical', 'data', 'sets', 'often', 'contain', 'the', 'label', 'noise', 'caused', 'by', 'various', 'human', 'factors', 'or', 'measurement', 'errors,', 'which', 'means', 'that', 'a', 'fraction', 'of', 'training', 'examples', 'might', 'be', 'mistakenly', 'labeled.Such', 'noisy', 'labels', 'will', 'mislead', 'the', 'classi\\ufb01er', 'training', 'and', 'severely', 'decrease', 'the', 'classi\\ufb01cation', 'performance.Existing', 'approaches', 'to', 'handle', 'this', 'problem', 'are', 'usually', 'developed', 'through', 'various', 'surrogate', 'loss', 'functions', 'under', 'the', 'framework', 'of', 'empiri-', 'cal', 'risk', 'minimization.However,', 'they', 'are', 'only', 'suitable', 'for', 'binary', 'classi\\ufb01cation', 'and', 'also', 'require', 'strong', 'prior', 'knowledge.Therefore,', 'this', 'article', 'treats', 'the', 'example', 'features', 'as', 'side', 'information', 'and', 'formulates', 'the', 'noisy', 'label', 'removal', 'problem', 'as', 'a', 'matrix', 'recovery', 'problem.We', 'denote', 'our', 'proposed', 'method', 'as', '\\u201clabel', 'noise', 'handling', 'via', 'side', 'information\\u201d', '(LNSI).Speci\\ufb01cally,', 'the', 'observed', 'label', 'matrix', 'is', 'decomposed', 'as', 'the', 'sum', 'of', 'two', 'parts,', 'in', 'which', 'the', '\\ufb01rst', 'part', 'reveals', 'the', 'true', 'labels', 'and', 'can', 'be', 'obtained', 'by', 'conducting', 'a', 'low-rank', 'mapping', 'on', 'the', 'side', 'information;', 'and', 'the', 'second', 'part', 'captures', 'the', 'incorrect', 'labels', 'and', 'is', 'modeled', 'by', 'a', 'row-sparse', 'matrix.The', 'merits', 'of', 'such', 'formulation', 'lie', 'in', 'three', 'aspects:', '1)', 'the', 'strong', 'recovery', 'ability', 'of', 'this', 'strategy', 'has', 'been', 'suf\\ufb01ciently', 'demonstrated', 'by', 'intensive', 'theoretical', 'works', 'on', 'side', 'information;', '2)', 'multi-class', 'situations', 'can', 'be', 'directly', 'handled', 'Manuscript', 'received', 'August', '13,', '2018;', 'revised', 'January', '17,', '2019', 'and', 'April', '11,', '2019;', 'accepted', 'August', '26,', '2019.This', 'work', 'was', 'supported', 'by', 'NSF', 'of', 'China', 'under', 'Grant', '61602246,', 'Grant', '61973162,', 'and', 'Grant', 'U1713208,', 'in', 'part', 'by', 'NSF', 'of', 'Jiangsu', 'Province', 'under', 'Grant', 'BK20171430,', 'in', 'part', 'by', 'the', 'Fundamental', 'Research', 'Funds', 'for', 'the', 'Central', 'Universities', 'under', 'Grant', '30918011319,', 'in', 'part', 'by', 'the', 'Open', 'Project', 'of', 'the', 'State', 'Key', 'Laboratory', 'of', 'Integrated', 'Services', 'Networks', 'through', 'Xidian', 'University', 'under', 'Grant', 'ISN19-', '03,', 'in', 'part', 'by', 'the', '\\u201cSummit', 'of', 'the', 'Six', 'Top', 'Talents\\u201d', 'Program', 'under', 'Grant', 'DZXX-027,', 'in', 'part', 'by', 'the', '\\u201cYoung', 'Elite', 'Scientists', 'Sponsorship', 'Program\\u201d', 'by', 'Jiangsu', 'Province,', 'in', 'part', 'by', 'the', '\\u201cYoung', 'Elite', 'Scientists', 'Sponsorship', 'Program\\u201d', 'by', 'CAST', 'under', 'Grant', '2018QNRC001,', 'in', 'part', 'by', 'the', 'Program', 'for', 'Changjiang', 'Scholars,', 'in', 'part', 'by', 'the', '\\u201c111\\u201d', 'Program', 'AH92005,', 'in', 'part', 'by', 'the', 'ARC', 'FL-170100117,', 'in', 'part', 'by', 'DP180103424,', 'and', 'in', 'part', 'by', 'DE190101473.', '(Corresponding', 'author:', 'Chen', 'Gong.)Y.', 'Wei', 'and', 'C.', 'Gong', 'are', 'with', 'the', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nanjing', '210094,', 'China,', 'and', 'also', 'with', 'the', 'State', 'Key', 'Laboratory', 'of', 'Integrated', 'Services', 'Networks,', 'Xidian', 'University,', 'Xi\\u2019an,', 'China', '(e-mail:', 'csywei@njust.edu.cn;', 'chen.gong@njust.edu.cn).S.', 'Chen', 'and', 'J.', 'Yang', 'are', 'with', 'the', 'PCA', 'Laboratory,', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nanjing', '210094,', 'China,', 'with', 'the', 'Key', 'Laboratory', 'of', 'Intelligent', 'Perception', 'and', 'Systems', 'for', 'High-Dimensional', 'Information', 'of', 'Ministry', 'of', 'Education,', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nanjing', '210094,', 'China,', 'and', 'also', 'with', 'the', 'Jiangsu', 'Key', 'Laboratory', 'of', 'Image', 'and', 'Video', 'Understanding', 'for', 'Social', 'Security,', 'School', 'of', 'Computer', 'Science', 'and', 'Engineering,', 'Nanjing', 'University', 'of', 'Science', 'and', 'Technology,', 'Nan-', 'jing', '210094,', 'China', '(e-mail:', 'shuochen@njust.edu.cn;', 'csjyang@njust.edu.cn).T.', 'Liu', 'and', 'D.', 'Tao', 'are', 'with', 'the', 'UBTECH', 'Sydney', 'Arti\\ufb01cial', 'Intelligence', 'Centre,', 'School', 'of', 'Computer', 'Science,', 'Faculty', 'of', 'Engineer-', 'ing,', 'The', 'University', 'of', 'Sydney,', 'Darlington,', 'NSW', '2008,', 'Australia', '(e-mail:', 'tongliang.liu@sydney.edu.au;', 'dacheng.tao@sydney.edu.au).Color', 'versions', 'of', 'one', 'or', 'more', 'of', 'the', '\\ufb01gures', 'in', 'this', 'article', 'are', 'available', 'online', 'at', 'http://ieeexplore.ieee.org.Digital', 'Object', 'Identi\\ufb01er', '10.1109/TNNLS.2019.2938782', 'with', 'the', 'aid', 'of', 'learned', 'projection', 'matrix;', 'and', '3)', 'only', 'very', 'weak', 'assumptions', 'are', 'required', 'for', 'model', 'design,', 'making', 'LNSI', 'applicable', 'to', 'a', 'wide', 'range', 'of', 'practical', 'problems.Moreover,', 'we', 'theoretically', 'derive', 'the', 'generalization', 'bound', 'of', 'LNSI', 'and', 'show', 'that', 'the', 'expected', 'classi\\ufb01cation', 'error', 'of', 'LNSI', 'is', 'upper', 'bounded.The', 'experimental', 'results', 'on', 'a', 'variety', 'of', 'data', 'sets', 'including', 'UCI', 'benchmark', 'data', 'sets', 'and', 'practical', 'data', 'sets', 'con\\ufb01rm', 'the', 'superiority', 'of', 'LNSI', 'to', 'state-of-the-art', 'approaches', 'on', 'label', 'noise', 'handling.Index', 'Terms\\u2014Classi\\ufb01cation,', 'generalization', 'bound,', 'label', 'noise,', 'matrix', 'recovery,', 'side', 'information.I.', 'INTRODUCTION', 'T', 'RADITIONALLY,', 'a', 'reliable', 'supervised', 'classi\\ufb01er,', 'such', 'as', 'support', 'vector', 'machines', '(SVMs)', 'or', 'convolutional', 'neural', 'networks', '(CNNs),', 'is', 'usually', 'trained', 'based', 'on', 'the', 'suf\\ufb01-', 'cient', 'correctly', 'labeled', 'data.Unfortunately,', 'the', 'real-world', 'data', 'sets', 'often', 'contain', 'the', 'noise', 'in', 'label', 'space,', 'which', 'means', 'that', 'a', 'fraction', 'of', 'training', 'examples', 'are', 'erroneously', 'labeled', '[1].For', 'instance,', 'as', 'the', 'numerous', 'examples', 'in', 'many', 'applications', '(e.g.,', 'image', 'classi\\ufb01cation', 'and', 'document', 'categorization)', 'are', 'manu-', 'ally', 'annotated,', 'the', 'labeling', 'errors', 'are', 'inevitably', 'introduced', 'due', 'to', 'the', 'human', 'fatigue.Disease', 'diagnosis,', 'in', 'which', 'the', 'decision', 'is', 'strongly', 'dependent', 'on', 'the', 'experience', 'and', 'expertise', 'of', 'the', 'doctors,', 'is', 'also', 'very', 'likely', 'to', 'include', 'labeling', 'errors.These', 'noisy', 'labels', 'will', 'signi\\ufb01cantly', 'mislead', 'the', 'classi\\ufb01er', 'training', 'and', 'then', 'severely', 'decrease', 'the', 'classi\\ufb01cation', 'performance', '[2].Hence,', 'designing', 'algorithms', 'that', 'account', 'for', 'the', 'data', 'with', 'noisy', 'labels', 'is', 'of', 'great', 'signi\\ufb01cance', 'and', 'has', 'become', 'a', 'critical', 'issue', 'in', 'the', 'machine', 'learning', 'community.Several', 'approaches', 'have', 'been', 'proposed', 'to', 'deal', 'with', 'the', 'learning', 'problem', 'with', 'label', 'noise', 'to', 'prevent', 'the', 'performance', 'decrease,', 'and', 'most', 'of', 'them', 'are', 'based', 'on', 'the', 'minimization', 'of', 'empirical', 'risk', 'via', 'a', 'conditional', 'probability', 'model', '[3]\\u2013[6].For', 'example,', 'Gao', 'et', 'al.', '[3]', 'and', 'Patrini', 'et', 'al.', '[6]', 'analyze', 'the', 'empirical', 'risk', 'minimization', 'in', 'the', 'presence', 'of', 'label', 'noise', 'by', 'decomposing', 'the', 'loss', 'function', 'into', 'a', 'label-independent', 'part', 'and', 'a', 'label-dependent', 'part.Manwani', 'and', 'Sastry', '[5]', 'study', 'the', 'noise', 'tolerance', 'properties', 'of', 'risk', 'minimization', 'under', 'differ-', 'ent', 'loss', 'functions', 'and', 'provide', 'insightful', 'theoretical', 'results.However,', 'they', 'are', 'only', 'applicable', 'to', 'binary', 'classi\\ufb01cation', 'and', 'the', 'extension', 'to', 'multi-class', 'is', 'nontrivial', '[7].Moreover,', 'these', 'methods', 'require', 'the', 'estimation', 'of', 'class', 'prior,', 'which', 'is', 'actually', 'quite', 'dif\\ufb01cult', 'in', 'the', 'presence', 'of', 'corrupted', 'observed', 'data.On', 'the', 'other', 'hand,', 'side', 'information', 'is', 'often', 'utilized', 'as', 'additional', 'knowledge', 'to', 'boost', 'the', 'performance', 'of', 'the', 'certain', '2162-237X', '\\u00a9', '2019', 'IEEE.Personal', 'use', 'is', 'permitted,', 'but', 'republication/redistribution', 'requires', 'IEEE', 'permission.See', 'http://www.ieee.org/publications_standards/publications/rights/index.html', 'for', 'more', 'information.Authorized', 'licensed', 'use', 'limited', 'to:', 'NANJING', 'UNIVERSITY', 'OF', 'SCIENCE', 'AND', 'TECHNOLOGY.Downloaded', 'on', 'July', '20,2020', 'at', '14:14:03', 'UTC', 'from', 'IEEE', 'Xplore.', 'Restrictions', 'apply.']\",\n          \"['This', 'article', 'has', 'been', 'accepted', 'for', 'inclusion', 'in', 'a', 'future', 'issue', 'of', 'this', 'journal.Content', 'is', 'final', 'as', 'presented,', 'with', 'the', 'exception', 'of', 'pagination.2', 'IEEE', 'TRANSACTIONS', 'ON', 'NEURAL', 'NETWORKS', 'AND', 'LEARNING', 'SYSTEMS', 'Fig.1.Motivation', 'illustration.', '(a)', 'Four', 'examples', 'from', 'two', 'classes,', 'among', 'which', 'the', 'label', 'of', 'the', '3rd', 'example', 'is', 'incorrect.', '(b)', 'Y', 'is', 'the', 'corrupted', 'label', 'matrix,', 'of', 'which', 'the', 'rows', 'represent', 'the', 'label', 'vectors', 'of', 'four', 'examples', 'displayed', 'in', '(a).By', 'taking', 'the', 'example', 'feature', 'matrix', 'X', 'as', 'side', 'information,', 'the', 'observed', 'label', 'matrix', 'Y', 'can', 'be', 'ideally', 'decomposed', 'as', 'the', 'sum', 'of', 'a', 'low-rank', 'recovered', 'label', 'matrix', 'T', '=', 'X', 'Z\\u2217and', 'a', 'row-sparse', 'matrix', 'E.', 'Note', 'that', 'the', 'nonzero', 'row', 'in', 'E', 'exactly', 'corresponds', 'to', 'the', '3rd', 'example', 'with', 'noisy', 'label.task,', 'which', 'has', 'been', 'widely', 'used', 'in', 'many', 'machine', 'learning', '\\ufb01elds', 'such', 'as', 'clustering', '[8]', 'and', 'multi-label', 'learning', '[9].For', 'example,', 'Zhao', 'et', 'al.', '[8]', 'propose', 'the', 'matrix', 'completion-based', 'approach', 'for', 'multi-view', 'clustering', 'and', '\\ufb01rst', 'introduce', 'the', 'side', 'information', 'to', 'aid', 'the', 'clustering', 'process.Xu', 'et', 'al.', '[9]', 'explore', 'the', 'side', 'information', 'to', 'reduce', 'the', 'requirement', 'on', 'the', 'number', 'of', 'observed', 'entries', 'for', 'matrix', 'completion', 'and', 'apply', 'the', 'method', 'to', 'transductive', 'incomplete', 'multi-label', 'learning.From', 'the', 'review,', 'we', 'know', 'that', 'the', 'side', 'information', 'has', 'not', 'been', 'utilized', 'for', 'removing', 'noisy', 'labels.Therefore,', 'this', 'article', 'provides', 'a', 'new', 'paradigm', 'for', 'dealing', 'with', 'the', 'label', 'noise', 'problem', 'from', 'the', 'viewpoint', 'of', 'side', 'information.Speci\\ufb01cally,', 'we', 'formulate', 'label', 'correction', 'as', 'a', 'label', 'matrix', 'recovery', 'problem', 'and', 'treat', 'the', 'example', 'features', 'as', 'side', 'infor-', 'mation', 'to', 'aid', 'the', 'recovery', 'process', '[9]\\u2013[11].Therefore,', 'our', 'proposed', 'method', 'is', 'named', 'as', '\\u201clabel', 'noise', 'handling', 'via', 'side', 'information\\u201d', '(LNSI).The', 'paradigm', 'of', 'this', 'article', 'is', 'shown', 'in', 'Fig.1,', 'which', 'intuitively', 'explains', 'how', 'to', 'transform', 'the', 'noisy', 'label', 'removing', 'problem', 'to', 'the', 'label', 'matrix', 'recovery', 'task', 'by', 'exploiting', 'the', 'side', 'information.Fig.1(a)', 'shows', 'the', 'case', 'where', 'four', 'examples', 'belong', 'to', 'two', 'classes,', 'in', 'which', 'the', '3rd', 'example', 'has', 'been', 'mistakenly', 'labeled', 'as', 'positive', 'and', 'the', 'noisy', 'label', 'is', 'constituted.As', 'illustrated', 'in', 'Fig.1(b),', 'given', 'the', 'observed', 'label', 'matrix', 'Y', 'and', 'corresponding', 'feature', 'matrix', 'X,', 'the', 'true', 'labels', 'T', 'can', 'be', 'obtained', 'by', 'conducting', 'a', 'low-rank', 'mapping', 'Z\\u2217on', 'the', 'example', 'features', '(i.e.,', 'T', '=', 'X', 'Z\\u2217),', 'and', 'the', 'incorrect', 'labels', 'are', 'captured', 'by', 'a', 'row-sparse', 'matrix', 'E.', 'The', 'merits', 'of', 'our', 'paradigm', 'lie', 'in', 'three', 'aspects:', '1)', 'LNSI', 'is', 'inherently', 'suitable', 'for', 'multi-class', 'classi\\ufb01cation,', 'which', 'does', 'not', 'need', 'the', 'one-versus-one', 'or', 'one-versus-the-rest', 'operations;', '2)', 'suf\\ufb01cient', 'theoretical', 'results', 'have', 'demonstrated', 'that', 'the', 'real', 'label', 'matrix', 'can', 'be', 'exactly', 'recovered', 'under', 'mild', 'conditions', '[12],', 'so', 'LNSI', 'is', 'guaranteed', 'to', 'obtain', 'satisfactory', 'performance;', 'and', '3)', 'LNSI', 'seamlessly', 'integrates', 'the', 'label', 'noise', 'removal', 'and', 'classi\\ufb01er', 'parameter', 'optimization', 'into', 'a', 'uni\\ufb01ed', 'framework.Due', 'to', 'the', 'above', 'merits,', 'a', 'reliable', 'classi\\ufb01er', 'can', 'be', 'learned', 'to', 'accurately', 'classify', 'the', 'unseen', 'test', 'examples', 'with', 'different', 'levels', 'of', 'training', 'label', 'noise.Furthermore,', 'the', 'experimental', 'results', 'on', 'both', 'benchmark', 'data', 'sets', 'and', 'practical', 'data', 'sets', 'verify', 'the', 'superiority', 'of', 'the', 'learned', 'classi\\ufb01er.II.RELATED', 'WORK', 'This', 'section', 'brie\\ufb02y', 'reviews', 'the', 'representative', 'prior', 'works', 'on', 'label', 'noise', 'handling', 'and', 'side', 'information', 'utilization,', 'as', 'they', 'are', 'related', 'to', 'the', 'proposed', 'LNSI.A.', 'Label', 'Noise', 'Handling', 'Practically,', 'the', 'labels', 'of', 'training', 'examples', 'are', 'often', 'not', 'reliable', 'due', 'to', 'various', 'limitations', 'in', 'data', 'acquisition', 'and', 'data', 'processing,', 'and', 'the', 'noisy', 'labels', 'often', 'occur', 'that', 'hinders', 'the', 'machine', 'learning', 'model', 'to', 'achieve', 'sound', 'performance.One', 'straightforward', 'idea', 'to', 'address', 'this', 'problem', 'is', 'to', 'improve', 'the', 'quality', 'of', 'training', 'data.Since', 'the', 'training', 'data', 'are', 'associated', 'with', 'noisy', 'labels,', 'the', 'early-stage', 'approaches', '\\ufb01rst', 'detect', 'and', 'eliminate', 'label', 'noise', 'and', 'then', 'conduct', 'the', 'standard', 'supervised', 'classi\\ufb01cation', 'algorithm.To', 'implement', 'noise', 'detection,', 'some', 'works', '[13]', 'explore', 'the', 'neighborhood', 'relationship', 'while', 'some', 'approaches', 'rely', 'on', 'ensemble', '\\ufb01l-', 'ters', '[14].Nevertheless,', 'the', 'performances', 'of', 'these', 'approaches', 'are', 'very', 'sensitive', 'to', 'the', 'quality', 'of', 'noise', 'detection,', 'which', 'makes', 'them', 'unreliable', 'for', 'practical', 'use.To', 'avoid', 'the', 'explicit', 'noise', 'detection', 'step,', 'plenty', 'of', 'efforts', 'have', 'been', 'made', 'recently', 'to', 'develop', 'the', 'algorithms', 'that', 'are', 'inherently', 'effective', 'and', 'robust', 'to', 'the', 'noisy', 'labels.Patrini', 'et', 'al.', '[6]', 'decompose', 'the', 'conventional', 'loss', 'function', 'into', 'a', 'label-independent', 'part', 'and', 'a', 'label-dependent', 'part,', 'in', 'which', 'only', 'the', 'latter', 'is', 'affected', 'by', 'label', 'noise.Conse-', 'quently,', 'various', 'surrogate', 'loss', 'functions', 'can', 'be', 'designed.Similarly,', 'Gao', 'et', 'al.', '[3]', 'tackle', 'the', 'second', 'part', 'by', 'deploying', 'the', 'labeled', 'instance', 'centroid', 'to', 'reduce', 'the', 'in\\ufb02uence', 'caused', 'by', 'label', 'noise.Natarajan', 'et', 'al.', '[4]', 'provide', 'an', 'unbiased', 'estimator', 'of', 'loss', 'function', 'to', 'deal', 'with', 'the', 'symmetric', 'noise,', 'while', 'Van', 'Rooyen', 'et', 'al.', '[15]', 'modify', 'the', 'traditional', 'hinge', 'loss', 'and', 'prove', 'its', 'robustness', 'to', 'label', 'noise.Although', 'these', 'algorithms', 'can', 'reduce', 'the', 'adverse', 'impact', 'of', 'label', 'noise', 'to', 'some', 'degree,', 'they', 'can', 'only', 'handle', 'canonical', 'binary', 'classi\\ufb01cation', 'and', 'lack', 'the', 'theoretical', 'guarantee', 'of', 'exact', 'recovery', 'on', 'accurate', 'labels.Recently,', 'some', 'methods', 'try', 'to', 'extend', 'deep', 'learning', 'models', 'to', 'the', 'case', 'of', 'noisy', 'labels.For', 'example,', 'Khetan', 'et', 'al.', '[16]', 'propose', 'a', 'new', 'supervised', 'learning', 'algorithm', 'which', 'can', 'jointly', 'model', 'labels', 'and', 'worker', 'quality', 'from', 'noisy', 'data.Patrini', 'et', 'al.', '[17]', 'present', 'a', 'loss', 'correction', 'approach', 'to', 'train', 'deep', 'models', 'that', 'are', 'robust', 'to', 'label', 'noise.Han', 'et', 'al.', '[18]', 'train', 'two', 'deep', 'neural', 'networks', 'simultaneously', 'and', 'let', 'them', 'teach', 'each', 'other', 'given', 'every', 'minibatch', 'for', 'combating', 'with', 'noisy', 'labels.Meanwhile,', 'Han', 'et', 'al.', '[19]', 'also', 'estimate', 'the', 'noise', 'transition', 'matrix', 'with', 'the', 'assistance', 'of', 'human', 'cognition', 'and', 'then', 'derive', 'a', 'structure-aware', 'probabilistic', 'model', 'for', 'label', 'noise', 'handling.However,', 'these', 'deep', 'learning-based', 'methods', 'are', 'only', 'suitable', 'for', 'speci\\ufb01c', 'tasks', 'related', 'to', 'image', 'analysis', 'or', 'natural', 'language', 'processing', '[17].Moreover,', 'the', 'perfor-', 'mance', 'of', 'these', 'methods', 'generally', 'lacks', 'theoretical', 'guarantees.Authorized', 'licensed', 'use', 'limited', 'to:', 'NANJING', 'UNIVERSITY', 'OF', 'SCIENCE', 'AND', 'TECHNOLOGY.Downloaded', 'on', 'July', '20,2020', 'at', '14:14:03', 'UTC', 'from', 'IEEE', 'Xplore.', 'Restrictions', 'apply.']\",\n          \"['This', 'article', 'has', 'been', 'accepted', 'for', 'inclusion', 'in', 'a', 'future', 'issue', 'of', 'this', 'journal.Content', 'is', 'final', 'as', 'presented,', 'with', 'the', 'exception', 'of', 'pagination.6', 'IEEE', 'TRANSACTIONS', 'ON', 'NEURAL', 'NETWORKS', 'AND', 'LEARNING', 'SYSTEMS', 'Algorithm', '1', 'Algorithm', 'for', 'Solving', 'LNSI', 'Input:', 'feature', 'matrix', 'X,', 'observed', 'label', 'matrix', 'Y;', 'trade-off', 'parameters:', '\\u03bb1,', '\\u03bb2,', 'and', '\\u03bb3;', 'Z', '=', 'O,', 'J', '=', 'Z,', 'E', '=', 'O,', 'B', '=', 'O,', 'M1', '=', 'O,', 'M2', '=', 'O,', 'M3', '=', 'O;', '\\u03bc', '=', '10\\u22123,', '\\u03bcmax', '=', '106,', '\\u03c1', '=', '1.2,', '\\u03f5', '=', '10\\u22126,', 'iter_max', '=', '1000;', 'iter', '=', '0;', '1:', 'Construct', 'graph', 'G', 'and', 'calculate', 'the', 'Laplacian', 'matrix', 'L', 'via', '(3);', '2:', 'while', 'not', 'converge', 'do', '3:', 'Update', 'Z', 'via', '(10),', '4:', 'Update', 'E', 'via', '(14),', '5:', 'Update', 'J', 'via', '(16),', '6:', 'Update', 'B', 'via', '(19),', '7:', 'Update', 'the', 'multipliers', 'M1', ':=', 'M1', '+', '\\u03bc(Y', '\\u2212B', '\\u2212E),', 'M2', ':=', 'M2', '+', '\\u03bc(B', '\\u2212X', 'J),', 'M3', ':=', 'M3', '+', '\\u03bc(Z', '\\u2212J),', '8:', 'Update', 'the', 'parameter', '\\u03bc', 'by', '\\u03bc', ':=', 'min(\\u03c1\\u03bc,', '\\u03bcmax),', '9:', 'iter', ':=', 'iter', '+', '1,', '10:', 'Check', 'the', 'convergence', 'conditions:', '\\u2225Y', '\\u2212B\\u2212E\\u2225F', '\\u2264\\u03f5', 'and', '\\u2225B\\u2212X', 'J\\u2225F', '\\u2264\\u03f5', 'and', '\\u2225Z\\u2212J\\u2225F', '\\u2264', '\\u03f5;', 'or', 'iter', '>', 'iter_max.11:', 'end', 'while', 'Output:', 'optimized', 'Z\\u2217and', 'E\\u2217.', 'B.', 'Computational', 'Complexity', 'This', 'section', 'studies', 'the', 'computational', 'complexity', 'of', 'Algorithm', '1.The', 'graph', 'construction', 'in', 'Line', '1', 'of', 'Algorithm', '1', 'takes', 'O(n2)', 'complexity.Line', '3', 'is', 'accomplished', 'by', 'using', 'the', 'SVD,', 'of', 'which', 'the', 'complexity', 'is', 'O(min(d2c,', 'dc2)).In', 'Line', '4,', 'one', 'should', 'compute', 'the', '\\u21132-norm', 'of', 'each', 'row', 'of', 'a', 'n', '\\u00d7', 'c', 'matrix', 'E,', 'so', 'the', 'complexity', 'is', 'O(nc).Note', 'that', 'a', 'd', '\\u00d7', 'd', 'matrix', 'should', 'be', 'inverted', 'in', 'Line', '5,', 'so', 'the', 'complexity', 'of', 'this', 'step', 'is', 'O(d3).Therefore,', 'the', 'total', 'complexity', 'of', 'our', 'proposed', 'algorithm', 'is', 'O(n2', '+(min(d2c,', 'dc2)+nc+d3)k)', 'by', 'assuming', 'that', 'Lines', '2\\u20139', 'are', 'iterated', 'k', 'times.Note', 'that', 'the', 'complexity', 'of', 'Algorithm', '1', 'is', 'squared', 'to', 'the', 'number', 'of', 'training', 'examples', 'n,', 'so', 'its', 'complexity', 'is', 'acceptable.C.', 'Generalization', 'Bound', 'In', 'this', 'section,', 'we', 'derive', 'the', 'generalization', 'bound', 'of', 'LNSI.1)', 'Preliminaries:', 'Recall', 'that', 'our', 'goal', 'is', 'to', '\\ufb01nd', 'a', 'suit-', 'able', 'project', 'matrix', 'Z', 'by', 'recovering', 'the', 'clean', 'label', 'matrix', 'X', 'Z,', 'given', 'the', 'observed', 'noisy', 'label', 'matrix', 'Y', 'and', 'example', 'features', 'X.', 'Similar', 'to', '[10],', '(6)', 'can', 'be', 'reformulated', 'to', 'the', 'following', 'expression', 'with', 'hard', 'constraints,', 'namely:', 'min', 'Z,E', '\\\\x02', '(i,', 'j)\\u2208{1,...,n}\\u00d7{1,...,c}', '\\u2113((X', 'Z', '+', 'E)ij', ',', 'Y', 'ij', ')', 's.t.\\u2225Z\\u2225\\u2217\\u2264Z\\u2217,', '\\u2225Z\\u22252', 'F', '\\u2264ZF,', 'X', 'Z', '\\u2208[\\u22121,', '1]n\\u00d7c', 'tr((X', 'Z)\\u22a4L(X', 'Z))', '\\u2264Ztr,', '\\u2225E\\u22252,1', '\\u2264E2,1.', '(21)', 'Let', '\\u03b8', '=', '(Z,', 'E)', 'be', 'any', 'feasible', 'solution,', 'and', '=', '{(Z,', 'E)', '|', '\\u2225Z\\u2225\\u2217', '\\u2264', 'Z\\u2217,', '\\u2225Z\\u2225F', '\\u2264', '\\u221aZF,', 'X', 'Z', '\\u2208', '[\\u22121,', '1]n\\u00d7c,', 'tr((X', 'Z)\\u22a4L(X', 'Z))', '\\u2264Ztr,', '\\u2225E\\u22252,1', '\\u2264E2,1}', 'be', 'the', 'feasible', 'solution', 'set.Also,', 'let', 'f\\u03b8(i,', 'j)', '=', 'Xi', 'ZI', 'j', '+', 'Eij', 'be', 'the', 'estimation', 'function', 'for', 'Yij', 'parameterized', 'by', '\\u03b8', '=', '(Z,', 'E),', 'and', 'F', '=', '{', 'f\\u03b8', '|', '\\u03b8', '\\u2208', '}', 'be', 'the', 'set', 'of', 'feasible', 'functions.I', 'j', 'is', 'the', 'jth', 'column', 'of', 'identity', 'matrix', 'I', '\\u2208Rc\\u00d7c.We', 'are', 'interested', 'in', 'the', 'following', 'two', '\\u201c\\u2113-risk\\u201d', 'quantities:', '1)', 'expected', '\\u2113-risk:', 'R\\u2113(', 'f', ')', '=', 'Ei,', 'j', '[\\u2113(', 'f', '(i,', 'j),', 'Yij', ')];', '2)', 'empirical', '\\u2113-risk:', '\\u02c6R\\u2113(', 'f', ')', '=', '(1/nr)', '\\\\x11', '(i,', 'j)', '\\u2113(', 'f', '(i,', 'j)Yij', '),', 'where', 'nr', 'is', 'the', 'number', 'of', 'observed', 'entries.Thus,', 'LNSI', 'is', 'to', '\\ufb01nd', 'a', 'proper', '\\u03b8\\u2217=', '(Z\\u2217,', 'E\\u2217)', 'that', 'parameterizes', 'f', '\\u2217=', 'arg', 'min', 'f', '\\u2208F', '\\u02c6R\\u2113(', 'f', ').2)', 'Generalization', 'Bound', 'of', 'LNSI:', 'To', 'bound', 'the', 'generaliza-', 'tion', 'error', 'of', 'LNSI,', 'we', '\\ufb01rst', 'link', 'the', 'quality', 'of', 'training', 'labels', 'to', 'Rademacher', 'complexity,', 'which', 'theoretically', 'measures', 'the', 'complexity', 'of', 'a', 'function', 'class.We', 'will', 'show', 'that', 'high-quality', 'labels', 'of', 'training', 'examples', 'will', 'result', 'in', 'a', 'lower', 'model', 'complexity', 'and', 'thus', 'a', 'smaller', 'error', 'bound.To', 'begin', 'with,', 'we', 'apply', 'the', 'following', 'lemma', 'to', 'bound', 'the', 'expected', '\\u2113-risk.Lemma', '3', '(Bound', 'on', 'Expected', '\\u2113-Risk', '[39]):', 'Let', '\\u2113be', 'the', 'loss', 'function', 'bounded', 'by', 'B', 'with', 'Lipschitz', 'constant', 'L\\u2113,', 'and', '\\u03b4', 'be', 'a', 'constant', 'where', '0', '<', '\\u03b4', '<', '1.With', 'probability', 'at', 'least', '1', '\\u2212\\u03b4,', 'we', 'have', 'max', 'f', '\\u2208F', '|R\\u2113(', 'f', ')', '\\u2212\\u02c6R\\u2113(', 'f', ')|', '\\u22642L\\u2113Rn(F)', '+', 'B', '\\\\x12', 'ln(1/\\u03b4)', '2nr', 'where', 'Rn(F)', ':=', 'E[R(F)]', 'is', 'the', 'Rademacher', 'complexity', 'of', 'the', 'function', 'class', 'F', 'and', 'R(F)', ':=', 'E\\u03c3', '\\\\x13', 'sup', 'f', '\\u2208F', '1', 'nr', 'nr', '\\\\x02', '\\u03b1=1', '\\u03c3\\u03b1', 'f', '(\\u03b1)', '\\\\x14', 'is', 'the', 'empirical', 'Rademacher', 'complexity', 'on', 'the', 'training', 'examples.Note', 'that', '\\u03c3\\u03b1', '(\\u03b1', '=', '1,', '2,', '.', '.', '.', ',nr)', 'are', 'independent', 'identically', 'distributed', '(i.i.d.)Rademacher', 'random', 'variables.Given', 'Lemma', '3,', 'we', 'see', 'that', 'the', 'key', 'to', 'derive', 'the', 'upper', 'bound', 'of', 'a', 'function', 'f', '\\u2208F', 'is', 'to', 'bound', 'the', 'complexity', 'Rn(F', ').More', 'formally,', 'the', 'Rademacher', 'complexity', 'can', 'be', 'bounded', 'in', 'terms', 'of', 'the', 'constraints', 'in', '(21).Before', 'diving', 'into', 'the', 'details,', 'we', '\\ufb01rst', 'provide', 'several', 'useful', 'theorems', 'and', 'lemmas.Lemma', '4', '(Complexity', 'Bound', '[40]):', 'Let', 'S', 'be', 'a', 'closed', 'con-', 'vex', 'set', 'and', 'let', 'F', ':', 'S', '\\u2192R', 'be', '\\u03b2-strongly', 'convex', 'with', 'respect', 'to', '\\u2225\\u00b7', '\\u2225.', 'In', 'addition,', 'we', 'assume', 'that', 'F\\u22c6(O)', '=', '0', 'with', 'F\\u22c6being', 'the', 'Fenchel', 'conjugate', 'of', 'function', 'F.', 'Further,', 'let', 'A', '=', '{A', ':', '\\u2225A\\u2225\\u22c6\\u2264A}', 'and', 'de\\ufb01ne', 'W', '=', '{W', '\\u2208S', ':', 'F(W)', '\\u2264Fmax}.Considering', 'the', 'class', 'of', 'linear', 'functions', 'F', '=', '{A', '\\u2192', 'W,', 'A', ':', 'W', '\\u2208W},', 'we', 'have', 'R(F)', '\\u2264A', '\\\\x12', '2Fmax', '\\u03b2nr', '(22)', 'where', 'W,', 'A', '=', 'tr(W\\u22a4A).Lemma', '5', '[41]:', 'The', 'function', 'F', ':', 'Rn\\u00d7c', '\\u2192R', 'de\\ufb01ned', 'as', 'F(W)', '=', '(1/2)\\u2225W\\u22252', '2,q', 'for', 'q', '=', '(ln(c)/(ln(c)', '\\u22121))', 'is', '(1/(3', 'ln(c)))-strongly', 'convex', 'with', 'respect', 'to', '\\u2225\\u00b7\\u22252,1', 'over', 'Rn\\u00d7c.Authorized', 'licensed', 'use', 'limited', 'to:', 'NANJING', 'UNIVERSITY', 'OF', 'SCIENCE', 'AND', 'TECHNOLOGY.Downloaded', 'on', 'July', '20,2020', 'at', '14:14:03', 'UTC', 'from', 'IEEE', 'Xplore.', 'Restrictions', 'apply.']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_embeddings\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"[[-0.02399354  0.02436408 -0.01130732 ...  0.04851779 -0.03059575\\n  -0.06718535]\\n [ 0.02872143  0.06235189 -0.00240356 ... -0.02024206 -0.06902064\\n   0.01966141]\\n [-0.02656304  0.04583179 -0.00301339 ...  0.03154208 -0.06035382\\n  -0.00432453]\\n ...\\n [-0.02972006 -0.00399144  0.00966241 ...  0.01339654 -0.02797396\\n   0.00976396]\\n [ 0.05336843  0.08734006 -0.00663722 ...  0.05458362  0.01032661\\n   0.00404617]\\n [ 0.00064343  0.02722497 -0.03459514 ... -0.03032272 -0.03990483\\n  -0.01313225]]\",\n          \"[[-0.02399348  0.02436411 -0.01130732 ...  0.04851775 -0.03059577\\n  -0.06718538]\\n [ 0.02872143  0.06235189 -0.00240356 ... -0.02024206 -0.06902064\\n   0.01966141]\\n [-0.02656302  0.04583187 -0.0030134  ...  0.03154212 -0.06035384\\n  -0.00432454]\\n ...\\n [-0.02972006 -0.00399144  0.00966241 ...  0.01339654 -0.02797396\\n   0.00976396]\\n [ 0.05336846  0.08734012 -0.00663721 ...  0.05458366  0.01032657\\n   0.00404619]\\n [ 0.00064347  0.02722497 -0.0345951  ... -0.03032274 -0.03990486\\n  -0.01313223]]\",\n          \"[[-0.02399354  0.02436408 -0.01130733 ...  0.04851773 -0.03059573\\n  -0.06718538]\\n [ 0.0287214   0.06235184 -0.00240359 ... -0.02024205 -0.06902064\\n   0.0196614 ]\\n [-0.02656302  0.04583187 -0.0030134  ...  0.03154212 -0.06035384\\n  -0.00432454]\\n ...\\n [-0.02972012 -0.00399148  0.00966242 ...  0.01339652 -0.02797394\\n   0.00976393]\\n [ 0.05336844  0.08734009 -0.00663721 ...  0.05458366  0.01032656\\n   0.00404617]\\n [ 0.00064347  0.02722497 -0.0345951  ... -0.03032274 -0.03990486\\n  -0.01313223]]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_embedding\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"[ 1.83159053e-02  3.61173917e-02 -2.62305447e-02 -1.17869190e-02\\n -1.47397605e-02  1.84890033e-02 -3.23374256e-02  3.03042123e-03\\n -2.47615758e-02  1.07302534e-02  6.97436984e-02 -2.06049280e-02\\n  3.21826779e-02  1.40334612e-02  1.78309855e-02 -2.46992059e-02\\n -9.24112113e-03  2.42777015e-02 -4.70184604e-02 -2.79626679e-02\\n -3.11986386e-02  1.07005153e-02  1.65418755e-02  1.39283327e-03\\n  6.18360383e-03 -3.44821228e-02  2.58867592e-02 -2.06543301e-03\\n -1.62086090e-03 -1.71752859e-02  2.13368151e-02 -1.38592719e-02\\n -3.33465795e-04 -1.67093207e-02  2.17386128e-06  6.37939535e-03\\n -1.06943091e-03  3.17177284e-03  2.44237140e-03  2.64480662e-02\\n  1.57022026e-03  2.89111790e-02 -2.59971043e-02  1.34045273e-02\\n -9.47787427e-03 -2.01975920e-02  3.76353993e-02  1.24798246e-02\\n -2.71412164e-02  2.27674823e-03 -5.86789573e-03  7.36613980e-04\\n -2.82624811e-02 -1.94127867e-02  4.86917939e-02  2.12227460e-02\\n -2.66266111e-03 -2.86979227e-02  1.88775304e-02  8.52387253e-03\\n -5.20165092e-03  3.12888872e-03  9.77749220e-03  2.45580134e-03\\n  3.19334181e-02  2.13117337e-02  1.78215146e-02  7.37650288e-03\\n -6.42157425e-03 -1.58702479e-02 -1.71599515e-03 -5.32108958e-03\\n  8.66359587e-03  9.60519533e-02 -1.42634964e-02  1.39330528e-03\\n -1.06631588e-02  3.38186757e-02 -9.92093343e-03 -1.64927126e-03\\n -2.29944385e-02  1.31066514e-02  2.22339401e-02  4.76376208e-02\\n -1.44168854e-02  2.99126077e-02  5.60256926e-03  7.09852763e-04\\n -4.09038918e-02 -9.65559467e-03 -3.27014928e-02 -3.44321770e-02\\n -1.18731765e-02  7.96429351e-03 -1.12203210e-02 -2.22303743e-02\\n -9.13712438e-03  8.27659798e-03  1.43756253e-02 -1.23274024e-02\\n -1.27714302e-02  3.05260694e-02 -2.84214623e-02  5.08889232e-03\\n  6.20777382e-03  3.63307975e-03  1.73988611e-02 -2.08134714e-03\\n -1.31347262e-02  4.31467159e-02 -3.27901328e-02  1.64549314e-02\\n -5.74642912e-03  5.06628472e-02  2.39559502e-02 -3.36432868e-03\\n -3.02162603e-02  9.92538643e-03 -3.68193294e-03  3.06876118e-02\\n  6.31157976e-03  1.40406603e-02 -1.21164873e-02  1.12883364e-02\\n  1.62543132e-02  3.76815328e-03 -3.22258545e-02  4.46860278e-03\\n  1.31281957e-02 -7.06639774e-03 -1.25937020e-02  3.05549637e-03\\n -6.98559105e-03 -6.51377993e-03  1.19822766e-02 -1.44470319e-02\\n -1.22184491e-02 -1.20292494e-02  1.51475543e-02 -8.99056169e-03\\n -1.62668862e-02 -4.32108595e-02 -2.57698875e-02 -2.20449112e-02\\n -2.40978732e-02 -8.28206930e-03  4.24869492e-02 -3.30518150e-02\\n -1.00010347e-03 -1.71387579e-02  9.99249435e-03  2.03991780e-02\\n -1.62264238e-02 -1.37405271e-02  2.22442827e-02  2.03938321e-02\\n  1.01287966e-02 -2.52430458e-02 -7.37838238e-03  2.68116663e-02\\n  1.73693145e-02  8.56646682e-03 -1.76308932e-02 -2.16952943e-02\\n -1.74336356e-02  3.51921347e-03  8.54631630e-03 -1.08893533e-02\\n  4.49681403e-03 -2.29209706e-02 -4.23636876e-03 -7.78414955e-03\\n -2.95407855e-02  3.00954760e-02  3.81620462e-02  3.38767416e-02\\n  1.27460041e-02 -1.57574754e-02 -4.76513251e-02 -1.22941869e-02\\n -1.41181732e-02 -4.32884007e-02 -9.59498582e-04 -1.66430948e-02\\n  6.04942301e-04  1.62513802e-02  2.59976540e-02  3.13395272e-02\\n -2.66155980e-02 -1.42804916e-03  4.66265986e-03 -1.58750460e-03\\n -3.40235825e-02  2.41022938e-02  1.42969640e-02  9.44832826e-03\\n  4.08575391e-02  2.44217655e-03  6.98946049e-03 -1.08305444e-04\\n -2.43725423e-02 -1.15976422e-02  1.57963516e-02 -7.50015240e-03\\n -2.85119811e-03  2.38010234e-03  7.76886634e-03 -5.81146263e-03\\n -3.12044324e-02  3.88577740e-02 -6.18005303e-03  6.60619232e-03\\n  4.03884557e-03  2.42171020e-03 -3.67057345e-03  4.35314241e-03\\n  1.17136586e-02  1.38673268e-03 -2.81687974e-02  4.78551081e-02\\n -1.85355648e-02  1.42589610e-03  1.32815941e-02 -3.03173679e-03\\n  1.15703703e-02  4.76793827e-03 -7.34246525e-03 -1.42871945e-02\\n  2.55748629e-02  2.80116204e-03  6.90800209e-03  1.55178265e-02\\n  6.92419658e-03 -2.90578875e-03  1.95509102e-02  8.14493408e-03\\n  4.31048332e-02  1.30719080e-02  1.10902128e-02 -9.87626121e-02\\n -1.71673075e-02 -6.98758677e-04  7.17230725e-03  8.22258688e-03\\n  3.17576377e-02 -1.18494553e-02  2.44471760e-02 -2.55002718e-03\\n -1.66948602e-02 -2.21102741e-02 -1.76938729e-02 -8.05037339e-04\\n  3.71246839e-02 -1.16929950e-02 -5.20167857e-03  6.81102690e-03\\n -1.97375255e-02  8.50052949e-03 -4.15193752e-02 -3.86185915e-02\\n  1.27792550e-03 -2.44296106e-02 -2.22649675e-02 -3.33316935e-03\\n  5.70708700e-04  2.18855366e-03  5.22065152e-03 -2.45865621e-02\\n -4.44627101e-03  6.75168355e-03  1.78002304e-02 -1.28825853e-02\\n  3.01877106e-02  1.04034980e-02 -1.88808904e-03 -1.83249822e-03\\n -3.37928506e-02  5.49539019e-03 -2.08667657e-02 -1.67331031e-02\\n -1.27599747e-02 -6.52861965e-03 -1.53006613e-02  2.01837971e-02\\n  1.64409158e-02  3.05783464e-02  7.52115816e-03 -5.40213050e-02\\n -6.79075331e-03 -1.46891618e-02 -2.49038187e-02 -1.50608576e-02\\n  2.44776109e-02 -3.35990555e-02  7.59121317e-03  2.02552668e-02\\n -2.38718390e-04  2.32375884e-02  1.80134481e-02  4.11718913e-02\\n  1.82361089e-02  1.10589664e-02 -2.62735107e-03 -8.08662340e-03\\n -1.65148845e-02 -1.60030925e-02 -1.11386910e-03 -1.53110742e-02\\n  8.68700994e-03  2.03816682e-02  9.25926669e-03  2.20713976e-02\\n  6.53635308e-03 -2.24706332e-03  7.80798324e-03  1.73867045e-02\\n -3.29922200e-02 -1.82627349e-02  4.02743760e-03  2.43530823e-02\\n  8.25279188e-03 -1.03574567e-02 -2.83108055e-03 -7.09119389e-03\\n  6.30956688e-03  1.64927570e-02 -2.24999941e-02  6.24560747e-03\\n  3.44140022e-04  4.37702170e-04  3.94775078e-03  1.02671143e-03\\n  2.80782088e-02 -1.76765850e-02 -4.35403641e-03  2.29903725e-02\\n -1.40993301e-02 -1.70267859e-02  1.39477635e-02  2.61913932e-02\\n  1.54988815e-02 -2.76035548e-02 -4.17999387e-03 -8.71455275e-03\\n -2.88993527e-02 -2.67990529e-02 -3.35753136e-02  9.69913814e-03\\n -6.72041047e-02  7.62479714e-06 -1.97596308e-05  5.26180911e-03\\n -6.25654788e-03 -4.19935285e-02 -1.00162570e-02  1.82914865e-02\\n -1.62159894e-02 -1.25894332e-03 -1.24302818e-02 -1.50700908e-02\\n -2.49837730e-02  2.62662521e-02  3.83729547e-02  4.79579241e-02\\n  7.90909672e-04  1.03950007e-02  3.01413026e-02  1.80709358e-02\\n -7.85869040e-03 -3.54711285e-03 -2.51410163e-03  2.21672882e-03\\n -2.89266609e-02  3.38023193e-02  4.48683098e-03  3.62172382e-03\\n  2.35066260e-02  2.55436784e-02 -1.38661992e-02 -8.92599385e-03\\n -1.69120462e-04 -6.15153753e-02 -2.11234707e-02  5.09721231e-03\\n  1.15426203e-02 -2.23640089e-02  1.17749618e-03 -4.73991324e-03\\n -3.73308415e-02  1.02973780e-02  2.65334563e-02 -2.44381337e-02\\n -2.80192187e-03  3.44256327e-02 -1.67065417e-03  6.08233637e-03\\n  2.27475067e-03  1.12854169e-02  9.43582456e-03  1.50798572e-02\\n  6.25533230e-03  7.03782608e-02  6.51238132e-03  1.26351728e-02\\n -1.87572661e-02 -1.83574562e-02 -1.39323178e-02  1.96547625e-02\\n -8.39146510e-03 -1.72905635e-02 -1.11910342e-02  1.81577118e-03\\n  1.12823068e-05  1.58679158e-02 -1.05983508e-02 -3.19416697e-02\\n -1.18405873e-02 -1.74418167e-03  1.50756304e-02 -2.33606922e-02\\n  6.94117072e-03 -1.54523740e-02  1.00381094e-02 -4.25461287e-03\\n -1.65613787e-02  9.83837951e-03  2.23134742e-02 -1.38901931e-03\\n  1.55294276e-02 -1.49547628e-02  2.89618656e-02  2.25279748e-02\\n -3.52453373e-02  1.02400700e-03 -1.66423828e-05 -8.64036359e-03\\n  1.34338124e-02  4.53234441e-03 -1.98890825e-02 -1.01265089e-03\\n -3.60666079e-03  7.25788915e-03  5.13176244e-02  2.34446123e-03\\n  4.81372111e-04  3.33406742e-03 -1.02223173e-02  2.82544896e-02\\n -3.25947459e-02  2.56793797e-03 -2.53309086e-03  1.21457733e-02\\n  1.09340393e-02 -2.95325802e-02  1.45981636e-02  8.16242832e-03\\n  2.05185131e-02  4.61028725e-03  7.59958912e-04  3.12435361e-02\\n  4.55518071e-02  2.73809107e-02 -2.16129977e-02  1.36220059e-02\\n -3.24244039e-02 -2.27163144e-02 -1.36344303e-02 -1.06185981e-02\\n  1.54021861e-02 -2.53502085e-02 -3.49570326e-02  6.05348852e-03\\n -1.03809726e-02  2.00035998e-02  9.76642060e-03  1.88826467e-02\\n -2.73724463e-02 -5.22547076e-03  2.04458097e-02  9.40751720e-04\\n -1.59997615e-02  9.81123977e-03 -1.31784783e-02  7.22159495e-03\\n -4.69115793e-03  2.42349565e-03  5.41282092e-03 -3.13875274e-03\\n  4.10369356e-03  2.02180170e-02 -2.75056858e-03 -1.19458986e-02\\n -7.95291527e-03 -1.16926896e-02  1.14066556e-02  2.07125629e-02\\n -4.66723907e-02 -1.02551518e-02  3.52128301e-02  1.60426030e-02\\n  1.01181332e-03  4.55499172e-03 -4.23922225e-04  3.32107103e-02\\n -4.24525694e-03 -1.50642565e-02  9.87022240e-03 -1.69440133e-03\\n  4.32109822e-02 -1.77173439e-02 -8.92956069e-03 -6.18261111e-03\\n -4.19244592e-02 -4.46313662e-02 -2.39782962e-03  3.76217164e-04\\n -3.10847433e-03 -3.23532698e-02 -5.90076504e-03 -1.48676474e-02\\n -8.56638520e-03 -7.54065407e-03  1.50151218e-02 -3.04397673e-03\\n -2.40745844e-02  2.58876704e-02  5.26180808e-03 -8.37653919e-03\\n -1.28995566e-02  2.76657387e-02 -4.62710782e-02 -9.33082717e-03\\n  4.17288724e-02  1.42114779e-03 -6.49156031e-03  3.86338260e-02\\n -1.11943751e-02 -1.71362791e-03 -1.26278380e-02 -8.38030308e-04\\n  1.27312574e-02 -3.80824005e-03  1.91255944e-02 -7.08641307e-03\\n  3.42677333e-02  2.01471543e-02  2.67918303e-03  4.65895972e-03\\n  2.73915214e-02  2.91733030e-03  1.46322313e-02 -1.91204524e-04\\n -5.40934552e-02 -5.24479939e-02 -2.82182084e-02 -9.09102944e-33\\n  1.24465556e-03  1.30831310e-03 -1.21065641e-02  7.65117768e-03\\n -3.51104577e-02 -1.16179622e-02 -2.62630694e-03  2.43728728e-02\\n -2.86732196e-02 -3.97831854e-03 -2.17292169e-02  5.84127028e-03\\n  2.79092595e-02  2.26505188e-02  2.28926020e-02 -2.33370055e-02\\n  1.69903848e-02  1.70373026e-04 -9.43466544e-03 -1.77436056e-02\\n  6.14784048e-03  1.39878412e-02  4.89330385e-02  3.56561986e-02\\n  1.04442064e-02 -3.20598546e-03 -2.53787306e-02  1.10583462e-02\\n  5.81834208e-02 -7.88437367e-03 -7.27722938e-03 -1.59552386e-02\\n -4.35082967e-03 -5.07680911e-04  1.70337732e-02 -3.66733768e-03\\n -1.28453531e-03 -1.26038058e-02 -2.93400678e-03 -2.72704297e-02\\n -2.15890560e-02 -2.52743971e-02  1.81614319e-02 -2.14524088e-02\\n  1.07042168e-02 -4.17802039e-02  2.67537931e-03 -2.48982956e-02\\n -7.73415043e-03 -2.94134132e-06 -3.12372624e-02  1.04881256e-02\\n -2.11912070e-02  3.05085533e-02 -1.79127908e-02 -5.98940003e-02\\n -5.87065405e-03  6.16505630e-03 -2.29769612e-02 -2.06549285e-02\\n  1.21459558e-02  1.17583035e-02 -1.00325601e-02 -3.38374882e-02\\n  1.17021033e-02  4.39977851e-03 -5.55720517e-03  1.54013923e-02\\n -5.26006515e-03  6.19161840e-03 -1.81755980e-02  2.04443026e-02\\n -6.47851721e-03 -9.05541387e-03  5.97759330e-03 -4.42212243e-02\\n -3.31789462e-02  1.16600085e-02  4.66004851e-02  4.89768343e-03\\n  1.58487381e-02 -3.76239880e-03 -5.78234465e-03 -2.56021352e-03\\n  7.30001719e-03 -8.60830181e-03  6.60879100e-03 -4.88722796e-03\\n -5.21654269e-03 -1.57079117e-02  4.97491122e-03  1.82737527e-03\\n -3.01513181e-02  3.70489512e-03 -2.80298906e-02 -2.65552361e-02\\n  1.51397127e-02  1.04873360e-02 -1.08694121e-02  1.17505186e-02\\n  1.19330896e-02  2.48320842e-02 -2.75045163e-02  3.46669849e-02\\n -2.64403767e-04 -4.87948514e-05 -3.26674401e-03  9.06796839e-03\\n -3.42257640e-02 -4.43065631e-04 -6.70977121e-03 -1.44167269e-02\\n  2.61343749e-02  9.73856578e-03  9.28007800e-04  1.38689616e-02\\n  1.45729140e-02 -2.84458350e-02 -1.46524677e-02  1.25083915e-02\\n  1.75526771e-02  2.50873350e-02 -3.10208330e-02  5.53491015e-03\\n -3.07862303e-02  3.26273801e-02 -1.73383144e-03 -2.11621667e-02\\n -1.65549537e-02 -2.44053009e-02  9.88357073e-03 -2.91212190e-02\\n  2.86584061e-07 -2.66933662e-02  2.44903295e-03  6.47079957e-03\\n  1.59799955e-02  2.23555500e-03  1.52650879e-03  3.69179238e-03\\n  4.01630385e-02 -1.02570812e-02  9.68908922e-03  1.14560740e-02\\n -1.44595405e-02  1.68683697e-02  1.30947231e-02  3.29233674e-02\\n -5.76228844e-03  1.94241140e-02 -1.84329983e-02 -2.05451409e-02\\n -4.13596300e-02 -3.83540227e-02  3.26165465e-02  3.19304261e-02\\n -8.13742175e-03  1.81499190e-04 -6.44095008e-03  1.53581546e-02\\n -2.30821195e-02  8.23751106e-03 -1.44782976e-02  3.24620887e-02\\n  1.15490174e-03 -1.73910979e-02  4.32066772e-03 -8.26980960e-04\\n -1.30049705e-02  1.86216731e-02 -4.10130095e-02  1.30626060e-02\\n  5.02742931e-02 -3.03072402e-02 -2.43072432e-03 -6.87221415e-03\\n  3.91440417e-03  6.58599632e-03  1.18783980e-02 -1.89212172e-02\\n  4.82416943e-03 -5.06593475e-02 -1.90272206e-02 -5.03077127e-03\\n -9.01395825e-04  2.58986304e-02  4.94584669e-02  3.78899176e-03\\n -1.64483073e-02  4.19719158e-02 -4.26190859e-03  2.90581722e-02\\n  2.60212202e-02 -9.74184305e-03  9.45471914e-03  1.74063601e-03\\n  4.57256426e-02  2.97652878e-02  1.06769338e-02 -1.32194301e-03\\n  1.12078300e-34  1.88878457e-02 -1.77928447e-03  1.47753370e-02\\n  1.58897764e-02  2.43116826e-03 -2.23677267e-02  7.35592573e-03\\n -1.87820301e-02  1.03329134e-02 -2.93105769e-02 -4.25223161e-03]\",\n          \"[ 2.12759182e-02  3.14661646e-02 -2.46859062e-02 -1.55498927e-02\\n -2.43853903e-02  1.87668490e-02 -2.36431066e-02  3.53954411e-03\\n -2.71009082e-02  7.24899271e-03  6.03997060e-02 -1.39886941e-02\\n  2.91721808e-02  2.06645642e-02  1.72736708e-02 -2.72885880e-02\\n -1.04334186e-02  1.97146570e-02 -4.64753180e-02 -3.40201603e-02\\n -3.40033179e-02  3.04681871e-03  1.92284110e-02  2.75994167e-03\\n  4.83418647e-03 -3.89718464e-02  2.69553172e-02 -1.51453011e-03\\n  7.01623455e-03 -1.70443440e-02  2.25687519e-02 -3.54217654e-03\\n  4.29872781e-03 -2.51617124e-02  2.20202712e-06  1.08352738e-02\\n -7.23804810e-03 -5.75883262e-04  1.00794051e-02  2.80406879e-02\\n -4.12812877e-03  2.80177086e-02 -3.03709773e-02  1.48475004e-02\\n -1.33573460e-03 -1.89498844e-02  4.30586399e-02  1.41274402e-02\\n -2.53568218e-02  1.83492007e-03 -8.09637223e-03 -1.34936522e-02\\n -3.29778211e-02 -1.92129225e-02  5.94003351e-02  1.31000406e-02\\n  1.75341628e-03 -2.96451953e-02  1.66192086e-02  1.39752662e-02\\n -1.22810351e-02  3.51579572e-03  1.28475823e-02  3.72828419e-03\\n  2.79820250e-02  2.63345204e-02  8.42657141e-03  1.23952786e-02\\n -1.00171293e-02 -2.26417791e-02  1.22772184e-02 -1.37197365e-02\\n  1.74959084e-02  1.08325011e-01 -2.23500673e-02  5.93178657e-03\\n -1.60470539e-03  3.62159350e-02 -1.11803204e-02 -7.88844110e-03\\n -2.35409735e-02  5.98345492e-03  2.58294207e-02  5.10659026e-02\\n -1.11678592e-02  3.23973918e-02  6.99788857e-03 -2.85634330e-03\\n -2.19134322e-02 -5.52556465e-03 -1.39050339e-02 -2.86713481e-02\\n -7.21690525e-03  6.95478912e-03 -2.73639070e-02 -1.94716144e-02\\n -1.12783278e-02 -9.36717187e-03  9.26893236e-03 -1.62950014e-02\\n -1.16228463e-02  3.60059435e-02 -3.23296589e-02  7.70347590e-03\\n  3.79550234e-03 -5.68162805e-03  1.02236966e-02 -4.70490780e-03\\n -1.63062757e-02  4.57607196e-02 -3.99783304e-02  9.19379069e-03\\n -7.18712621e-03  4.86011514e-02  3.38375198e-02 -1.01990543e-03\\n -3.32090567e-02  1.73897360e-02 -7.88602679e-03  3.76217774e-02\\n -6.58676148e-03  1.20703291e-02 -1.15617877e-02  3.85188610e-03\\n  3.13869202e-02  4.04705729e-03 -3.89101363e-02 -4.80618975e-03\\n  1.77745319e-02  8.95183188e-03 -1.51205574e-02  9.28154180e-03\\n -7.77804423e-03 -7.07356850e-03  1.36016478e-02 -9.08342210e-03\\n -1.78476275e-02 -1.11932851e-02  8.93623286e-03 -7.18338665e-03\\n -1.62304041e-02 -4.11560990e-02 -2.20765707e-02 -2.72198920e-02\\n -2.27798313e-02 -1.74272545e-02  4.78804641e-02 -5.29690923e-02\\n -8.42164821e-03 -2.82347890e-02  1.17028847e-02  2.40861301e-02\\n -1.76063867e-02 -2.05935560e-02  3.06986047e-02  2.72701615e-02\\n  8.56089697e-03 -3.73671093e-02 -4.82972998e-03  2.94637145e-02\\n  2.26605654e-02 -9.25922441e-04 -1.97092945e-02 -2.10677921e-02\\n -1.79338594e-02  1.31086304e-02  5.55810513e-03 -3.16985070e-03\\n -4.19372569e-03 -1.67795615e-02 -7.24205680e-04 -1.30875119e-02\\n -1.96973237e-02  2.36621583e-02  4.38045821e-02  3.88822801e-02\\n  4.36140590e-03 -2.45497094e-02 -4.57223712e-02 -2.75235673e-02\\n -2.02437790e-02 -4.19465777e-02  9.44047143e-03 -1.82976778e-02\\n -2.08946202e-03  8.69240383e-03  4.18160180e-02  2.37342209e-02\\n -3.25883140e-02  5.66151735e-03 -1.64730537e-03 -8.10144148e-03\\n -3.77049947e-02  2.77342414e-02  1.01911931e-02  5.79317427e-03\\n  3.86484813e-02  4.69130903e-03  4.95730485e-03  6.08195797e-03\\n -2.16569502e-02 -1.10342103e-02  3.36266613e-02 -9.28434759e-03\\n -3.54763544e-03  1.00958713e-02  2.90763805e-03 -3.27036117e-03\\n -3.85995636e-02  4.02612680e-02 -7.77903301e-03  1.59094339e-03\\n -4.30208848e-03  4.79717086e-03 -5.71101630e-03  2.19229149e-02\\n  1.21708911e-02  4.14714689e-04 -2.61114467e-02  5.10165089e-02\\n -1.55079837e-02 -4.58147067e-03  1.45794646e-02 -5.09644624e-03\\n  2.90778952e-03  1.18227409e-03  5.42321324e-04 -1.32113611e-02\\n  2.48595449e-02  8.05034596e-03  9.13268997e-03  1.83365840e-02\\n  1.67895862e-02 -5.62781184e-04  1.90855312e-02  8.23355824e-03\\n  4.54978475e-02  1.51276210e-02  3.91249282e-03 -1.07952569e-01\\n -1.00136546e-02  1.11415445e-02  4.00005108e-03  6.80771334e-03\\n  1.53058936e-02 -9.18656078e-03  2.14708619e-02  3.71848727e-04\\n -2.02872313e-02 -2.01310686e-02 -2.13065292e-02 -7.23987911e-04\\n  3.64586948e-02 -1.22507212e-02 -8.49809914e-03  7.34680521e-03\\n -2.08298101e-02 -8.73435196e-04 -5.51478661e-02 -4.00747364e-02\\n  7.39382997e-03 -1.50021306e-02 -2.61972497e-02 -6.67554453e-03\\n -2.12603635e-03 -6.37897346e-03  1.68167168e-03 -1.73837011e-02\\n  1.59348522e-03  3.82430307e-03  2.77644888e-02 -1.38177419e-02\\n  2.72085009e-02  4.08762829e-03 -3.42440480e-03 -7.01717977e-03\\n -2.96124570e-02 -1.85482631e-03 -1.80136108e-02 -2.30133142e-02\\n -1.39622320e-02 -5.58661833e-03 -1.65555117e-02  2.20881974e-02\\n  2.56593416e-02  3.06393520e-02  1.17354868e-02 -5.54948620e-02\\n -1.62211563e-02 -1.86288897e-02 -2.59193417e-02 -2.36566032e-02\\n  2.89998223e-02 -3.13152163e-02  4.43457814e-03  2.09224527e-02\\n -8.88857663e-04  2.44634108e-02  2.65277027e-02  4.38545108e-02\\n  6.49736597e-03  1.68869281e-02  1.20428473e-03 -1.01932720e-02\\n -2.33867002e-02 -2.61582039e-02 -1.20141061e-02 -1.45329080e-02\\n  2.91646587e-03  4.69594741e-02  1.05955052e-02  2.23615022e-02\\n  9.13360875e-03  5.32712100e-03  7.59346688e-03  1.96397000e-02\\n -3.89187849e-02 -2.03653688e-02  9.97378529e-03  2.80886446e-02\\n  8.64499445e-03 -3.33099331e-03 -5.61101606e-03 -5.13703665e-03\\n  1.09849843e-02  2.12940398e-02 -1.53897463e-02  1.34909883e-02\\n -3.65514045e-03 -2.83418403e-03  1.10777936e-04  2.72951939e-03\\n  3.03166925e-02 -1.38542997e-02  7.92507225e-04  2.27976399e-02\\n -1.28929016e-02 -2.48108909e-02  1.07354193e-02  2.80760974e-02\\n  2.02604797e-02 -3.03236413e-02 -1.84981810e-03  6.44173289e-03\\n -2.27632929e-02 -3.59715384e-02 -3.08398995e-02  1.93033057e-02\\n -7.28018328e-02 -8.89871397e-03  2.98964594e-03  6.18115491e-03\\n -4.21030743e-03 -4.69625566e-02 -1.10899824e-02  2.29612654e-02\\n -1.15904229e-02 -3.44885657e-03 -1.57074034e-02 -1.47805939e-02\\n -2.98842067e-02  2.88148172e-02  3.09675107e-02  4.91706507e-02\\n  3.90540188e-03  9.93861676e-03  2.76017994e-02  1.65365937e-02\\n -2.89525424e-03  3.84370831e-03 -6.98129352e-03 -9.19196367e-04\\n -2.50714207e-02  2.97850831e-02  7.85762957e-03  5.93226562e-03\\n  3.02876230e-02  2.70997716e-02 -1.02243235e-02 -9.79837151e-03\\n  4.13297617e-03 -5.69319576e-02 -2.09339634e-02  3.22275400e-03\\n  1.88225844e-02 -2.69103527e-02 -4.93091598e-03 -2.46197901e-03\\n -3.61694800e-02  3.01684866e-03  3.11909520e-02 -2.15111041e-02\\n  7.42929096e-03  3.80746981e-02  6.90437399e-03  9.68607848e-03\\n  1.28891285e-02  1.71208190e-02  1.08884791e-02  2.22868994e-02\\n  9.49999377e-03  6.17786307e-02  7.19470377e-03  1.39839321e-02\\n -1.40403656e-02 -1.62172696e-02 -3.13091298e-02  1.23577331e-02\\n -2.01182469e-02  7.64751200e-05 -8.59169949e-03  1.32795609e-02\\n  1.06524862e-02  4.01494319e-03 -5.69740708e-03 -3.38009555e-02\\n -1.68234659e-02  3.67378682e-03  4.32524918e-03 -2.79798042e-02\\n  1.06861347e-02 -1.40444567e-02  8.92547612e-03  1.28669159e-03\\n -2.06553698e-02  1.44013628e-02  2.45272027e-02  4.40003891e-03\\n  3.05194743e-02 -1.99378086e-02  2.64815098e-02  2.95422686e-02\\n -3.46621280e-02  7.13176881e-03  4.65317638e-03 -1.63150138e-02\\n  1.48370370e-02  9.07538594e-03 -2.75906721e-02 -9.24801904e-03\\n -5.64913786e-03  1.23155747e-02  4.14981784e-02  8.66825918e-03\\n  8.77648108e-03 -4.45451793e-03 -1.78937898e-02  3.51288506e-02\\n -3.65223580e-02  1.56129359e-03 -3.69727277e-03  1.54096113e-02\\n  1.36301580e-02 -3.37444516e-02  1.63603223e-02  6.19587796e-03\\n  1.89908065e-02  1.59349263e-03  1.02789155e-02  2.80789242e-02\\n  4.98556914e-02  2.91439429e-02 -3.05968830e-02  1.69742274e-02\\n -3.26840275e-02 -2.75080222e-02 -1.84675524e-02 -5.17014627e-03\\n  1.67770424e-02 -2.40209968e-02 -4.13276420e-02 -2.59742449e-03\\n -1.47350885e-02  1.88207280e-02  2.30093574e-02  2.29792613e-02\\n -2.36806818e-02 -3.70863565e-03  2.04592816e-02 -1.05850773e-03\\n -2.51608291e-02  2.41088159e-02 -1.48869210e-02  9.34450022e-03\\n -1.28046080e-03  7.66446756e-03  9.21786246e-03 -3.36527505e-03\\n  7.49930110e-03  2.36037328e-02 -3.32099150e-03 -2.54569686e-02\\n -5.96718090e-03 -1.26082159e-02  6.47821485e-03  2.34425471e-02\\n -4.84844695e-02 -1.05388051e-02  6.52315584e-02  2.83711397e-02\\n  7.06961939e-03  1.50448410e-03 -8.72881522e-03  2.63677513e-02\\n -3.62823229e-03 -4.63642453e-03  1.03101995e-02  1.99268983e-03\\n  5.11185315e-02 -1.37311489e-02 -9.07091385e-03  3.07579432e-03\\n -3.77127743e-02 -3.85523837e-02 -6.28188589e-03 -5.33455282e-03\\n -3.25604341e-03 -2.60240154e-02 -7.70423861e-03 -1.22982167e-02\\n -1.09161885e-02 -1.16943646e-02  1.26018493e-02 -3.30759372e-03\\n -3.72116960e-02  3.04603745e-02  3.56194352e-03 -1.80956245e-03\\n -1.01670841e-02  2.00890381e-02 -3.92886988e-02 -7.63513994e-03\\n  5.30631498e-02  3.89779407e-03 -7.74355294e-03  5.66131282e-02\\n -8.43650544e-03 -1.81762555e-02 -1.10029433e-02  2.51453435e-03\\n  1.19274385e-02 -1.32949426e-02  2.21425510e-02 -2.66382652e-03\\n  3.49736586e-02  1.63841734e-02  5.35484624e-03  7.88566315e-03\\n  2.66225305e-02  1.15852270e-02  5.92019389e-03 -4.68715641e-03\\n -5.77896879e-02 -5.94699985e-02 -2.95952734e-02 -9.21406601e-33\\n -4.41949803e-03 -7.32826531e-04 -1.64220078e-02  1.77361686e-02\\n -3.95702246e-02 -2.46235230e-02 -1.23191394e-02  3.11523401e-02\\n -2.68031870e-02 -1.42725748e-03 -2.00607578e-02  2.96044569e-03\\n  2.85925469e-02  1.29506829e-02  2.93678572e-02 -2.73465193e-02\\n  1.17451237e-02  1.38435952e-03 -1.39184554e-02 -1.06342497e-02\\n  1.45668293e-02  2.22198009e-02  6.84516332e-02  2.31473735e-02\\n  1.89402372e-02 -3.34518583e-03 -3.02719357e-02  9.32961507e-03\\n  6.59954260e-02 -4.34556744e-03 -5.22582501e-03 -1.93366054e-02\\n -1.86277444e-03 -1.88067568e-03  1.96218128e-02 -7.20739426e-03\\n -7.66203782e-03 -1.02832249e-02  3.97325336e-03 -3.48464930e-02\\n -3.47318685e-02 -3.86792507e-02  2.29738475e-02 -2.50100307e-02\\n  8.64066766e-03 -4.57456230e-02 -2.80186945e-04 -2.59473736e-02\\n -7.41851695e-03  8.74665232e-03 -3.39802849e-02  4.06264018e-03\\n -2.43438062e-02  4.52562043e-02 -2.66959372e-02 -5.46074125e-02\\n -7.81711667e-03 -2.69742641e-03 -3.81110690e-02 -2.42588600e-02\\n  6.54631895e-03  1.14905191e-02 -1.26596715e-03 -3.36117926e-02\\n  2.29276997e-03 -4.17670066e-03 -2.96117742e-03  3.83607948e-03\\n  5.62722556e-03  8.68218167e-03 -2.45181240e-02  5.87531105e-03\\n -5.15310444e-03 -1.06559282e-02 -1.34085527e-04 -3.92027609e-02\\n -2.44207698e-02  5.56450078e-03  4.48140196e-02 -9.34680819e-04\\n  2.51252178e-02 -3.09563610e-03 -8.07380852e-03 -1.04437342e-04\\n  5.65239134e-03 -1.17259305e-02  2.92899915e-03 -1.25044811e-02\\n -3.37158515e-03 -1.49579915e-02  1.37244773e-02 -7.08003569e-03\\n -3.97321747e-02  1.36230999e-02 -2.46260404e-02 -2.99949332e-02\\n  1.54986017e-02  1.13701409e-02 -1.37402880e-02  6.60321439e-03\\n  2.30569579e-02  2.97406772e-02 -3.01369454e-02  4.72332054e-02\\n  7.20958218e-03  7.43710490e-04 -7.97668283e-03  7.39422765e-03\\n -3.76274627e-02  5.08081535e-04 -8.92580185e-03 -1.02941080e-02\\n  1.99737092e-02 -8.95721661e-04 -4.70887366e-03  4.66522527e-03\\n  1.16090902e-02 -2.19952280e-02 -7.93218153e-03  2.02941897e-02\\n  2.24906214e-02  2.51351611e-02 -3.67024058e-02  4.97428723e-03\\n -2.50182903e-02  4.30434785e-02  1.73168379e-03 -2.16446658e-02\\n -1.51769580e-02 -1.40270060e-02  2.03092105e-02 -2.46306208e-02\\n  2.91539549e-07 -2.77390517e-02 -1.03106886e-02  5.41071474e-04\\n  1.29555239e-02  1.09482393e-02  6.26408458e-03 -3.89696288e-03\\n  4.39352431e-02 -1.32154940e-03 -3.30932987e-03  1.01818198e-02\\n -2.50778956e-02  2.44858867e-02  1.60503398e-02  3.68240354e-02\\n  3.93121152e-03  8.87665943e-03 -1.01983151e-02 -1.98017052e-02\\n -3.55354871e-02 -3.71325246e-02  2.67804601e-02  3.25903747e-02\\n -7.13052522e-03 -3.66709214e-03 -1.22307856e-03  2.17948331e-02\\n -3.30412946e-02  1.30980520e-02 -1.38810210e-02  3.60271077e-02\\n -1.84548216e-02 -1.69978169e-02 -7.04549590e-03 -3.82300352e-03\\n -1.47538717e-02  1.42038119e-02 -3.60511869e-02  1.62159032e-02\\n  5.24584394e-02 -3.63984233e-02  1.48334636e-03 -1.10536208e-02\\n -3.47556799e-03  6.73172428e-03 -7.75423780e-03 -2.46499755e-02\\n  1.48968352e-02 -5.36237194e-02 -1.75907303e-02 -1.64091487e-02\\n -1.96010767e-04  3.08912561e-02  4.88919655e-02  2.62103743e-03\\n -1.52241147e-02  4.94863490e-02 -8.38411701e-03  2.76535076e-02\\n  3.10838112e-02 -2.26137003e-02  9.81586455e-03  3.76922000e-03\\n  5.54724229e-02  2.54307528e-02  6.63156557e-03  2.45721663e-03\\n  1.15899513e-34  1.40155684e-02 -2.79220089e-03  1.76598400e-02\\n  9.85670150e-05 -5.97551752e-03 -2.62577014e-02  4.11447188e-03\\n -1.81779134e-02  1.80062242e-02 -2.41793421e-02  9.06849432e-04]\",\n          \"[ 2.05753164e-02  2.53250149e-02 -2.50502305e-02 -7.94454936e-03\\n -2.41158070e-02  1.76135333e-02 -3.33096466e-02  5.86341594e-03\\n -2.04282358e-02  1.30023246e-02  6.76420455e-02 -1.26517474e-02\\n  2.59723932e-02  2.50268114e-02  1.01834465e-02 -3.07564098e-02\\n -5.08326765e-03  1.73224495e-02 -4.79509506e-02 -2.73366969e-02\\n -3.12801350e-02  9.31772109e-03  1.24576213e-02 -4.94086466e-03\\n  1.28272013e-02 -3.13696815e-02  2.32636859e-02 -1.05729701e-03\\n  1.74760463e-03 -1.31692783e-02  1.47531919e-02 -1.40986454e-02\\n  1.04630415e-03 -1.29918811e-02  2.17440702e-06  2.70924778e-03\\n -4.36961305e-03  2.24545100e-03 -4.57343218e-03  3.04763256e-02\\n  2.92206411e-03  2.13040193e-02 -2.81878583e-02  1.59573822e-02\\n -6.19204779e-03 -1.58953752e-02  3.81545055e-02  4.53054501e-03\\n -2.68327791e-02  2.15249434e-03 -6.71601628e-03 -1.69038881e-03\\n -2.53564639e-02 -2.07085062e-02  4.36146562e-02  7.97938987e-03\\n -1.95977609e-03 -2.46552952e-02  1.68299376e-02  4.55650108e-03\\n -4.49707674e-03  5.56351551e-03  1.16692455e-02  4.91790923e-03\\n  2.82391085e-02  2.02685631e-02  3.14066860e-02  5.10617311e-03\\n -4.69960152e-03 -1.53925822e-02  7.39693511e-03 -6.03984531e-03\\n  7.06500446e-03  9.55665029e-02 -1.52566516e-02  4.56267931e-03\\n -7.59778835e-03  3.09163609e-02 -1.00914779e-02 -4.37280974e-03\\n -2.68892563e-02  1.75052136e-02  2.13452795e-02  4.47424961e-02\\n -1.56016136e-02  2.68380059e-02  2.87097375e-03 -2.77316262e-03\\n -2.81326115e-02 -6.90339956e-03 -2.91794913e-02 -3.20738187e-02\\n -9.33706497e-03  7.38423854e-03 -2.13801251e-02 -2.23132186e-02\\n -1.15257117e-02 -2.15030215e-03  9.58973537e-03 -1.53712897e-02\\n -3.00253099e-03  3.02476499e-02 -1.28077532e-02  8.25762763e-03\\n  1.03473155e-02  3.59755741e-03  8.01369245e-03 -1.07317419e-02\\n -1.60153698e-02  4.61656094e-02 -3.31687206e-02  1.12651599e-02\\n -5.46390263e-03  4.55270974e-02  3.01342499e-02 -2.87271020e-04\\n -3.06175130e-02  1.65752792e-02 -5.47849880e-03  3.38364711e-02\\n -1.76547280e-02  1.15827402e-02 -1.35449451e-02  2.47546957e-03\\n  1.49227193e-02  1.01314727e-02 -3.65387069e-02 -2.32792275e-03\\n  1.23582309e-02 -7.20506308e-03 -8.96468626e-03 -3.84758002e-04\\n -5.42805466e-03 -1.78491937e-03  1.13139871e-02 -8.04891437e-03\\n -1.75798157e-02 -1.48191650e-02  1.62195323e-02 -1.29646608e-02\\n -7.79824522e-03 -4.40329887e-02 -2.31599989e-02 -2.82179026e-02\\n -2.23431133e-02 -1.47702071e-02  3.78440685e-02 -3.65458674e-02\\n -6.90361507e-03 -2.03911282e-02  1.01655795e-02  2.47362821e-02\\n -2.23676315e-02 -1.42338578e-02  3.15205877e-02  2.24623288e-02\\n  1.00008000e-02 -2.71723503e-02 -3.53489970e-03  2.15964570e-02\\n  1.91059378e-02  4.81870109e-03 -8.94429860e-03 -1.98567525e-02\\n -1.66853049e-02  8.82318512e-03  1.62599818e-02 -6.41474092e-03\\n -1.29367289e-02 -1.76035643e-02 -8.75721296e-03 -6.05392314e-03\\n -2.55523960e-02  2.46448602e-02  2.69259009e-02  3.92899475e-02\\n  1.14270374e-02 -1.55129845e-02 -4.70544347e-02 -6.83183304e-03\\n -1.29108514e-02 -4.00511295e-02  1.71482855e-03 -7.82729857e-03\\n -4.28864629e-04  1.53788750e-02  2.97288984e-02  2.24072797e-02\\n -2.77916580e-02  1.07623835e-03  3.84674988e-03 -3.77322086e-03\\n -2.44363410e-02  2.09713234e-02  1.19283418e-02  3.94836696e-03\\n  3.22482134e-02 -2.66976387e-03  8.82117209e-03  1.18601129e-04\\n -2.68676929e-02 -1.20315010e-02  2.80641036e-02 -7.55882004e-03\\n -4.47329492e-03 -6.18373655e-05  5.25510961e-03 -6.75500433e-03\\n -2.40344544e-02  3.59823869e-02  5.60926723e-03  8.67582166e-03\\n  1.17136315e-03  3.68908097e-03 -2.85511125e-04  1.26628804e-02\\n  1.11101552e-02  4.47271218e-03 -2.58217167e-02  3.86342167e-02\\n -1.65582290e-02  9.92337369e-03  6.02318409e-03 -8.87364242e-03\\n  5.57067966e-03 -1.27094478e-03 -6.84442369e-03 -9.45562846e-03\\n  2.53287896e-02  4.70666822e-03  8.10851533e-03  2.07833266e-02\\n  1.62036486e-02 -7.54120590e-03  1.32047112e-02  6.20572458e-03\\n  3.93439635e-02  1.22302060e-02  3.75725450e-03 -1.03057522e-01\\n -1.43583927e-02  1.25503227e-02  3.48227888e-03  7.50635609e-03\\n  3.28102001e-02 -9.46530469e-03  2.54726103e-02  2.87844824e-04\\n -1.65334151e-02 -2.18280076e-02 -2.10947318e-02  2.28646221e-03\\n  3.21635235e-02 -1.44813556e-02  1.68583189e-03  1.01044880e-02\\n -2.65119681e-02  6.12337018e-03 -3.43042916e-02 -3.14816420e-02\\n  1.30882575e-03 -2.30252034e-02 -2.30153984e-02 -1.16009718e-03\\n  5.25651729e-03 -9.09690709e-05  5.64128318e-04 -2.36979264e-02\\n -6.26248997e-04  5.82099205e-03  2.63635062e-02 -1.25934918e-02\\n  3.33267811e-02  7.36182945e-03 -1.77096278e-03 -3.92692289e-03\\n -2.96510789e-02  8.29891841e-04 -1.80764676e-02 -1.17301071e-02\\n -2.64025189e-03 -1.01837706e-02 -1.78334244e-02  1.55963521e-02\\n  2.03945338e-02  2.89442445e-02  9.20561761e-03 -4.29634016e-02\\n -1.16074994e-02 -7.35022942e-03 -2.91952253e-02 -1.68363833e-02\\n  2.38201143e-02 -2.82437390e-02  1.27178159e-02  1.34029348e-02\\n  3.19959846e-03  1.83978725e-02  2.25664324e-02  3.72522239e-02\\n  6.55786516e-03  2.05483770e-02  9.49179130e-04 -1.07167347e-02\\n -1.05843254e-02 -2.83505082e-02  1.86444859e-03 -1.74838050e-02\\n  5.61742728e-03  2.82594090e-02  4.55929671e-03  2.01832308e-02\\n  2.15175463e-03  1.19062288e-03  6.08982603e-03  1.48168351e-02\\n -3.15595544e-02 -1.52937725e-02  2.51405351e-03  3.18648168e-02\\n  2.33802400e-03 -6.28954473e-03 -2.05196603e-03 -3.39472060e-03\\n  9.76622301e-03  1.90482561e-02 -2.07792112e-02  1.29955058e-02\\n  4.27443893e-03  4.65843913e-03  4.69315298e-03  8.06918505e-04\\n  2.47541172e-02 -1.38879668e-02 -1.11353773e-03  1.44685900e-02\\n -1.55019624e-02 -1.50960897e-02  8.42670225e-03  2.24426643e-02\\n  6.49663398e-03 -2.49912511e-02 -2.06810590e-04  2.28161383e-03\\n -2.62725189e-02 -3.01501454e-02 -2.25869923e-02  1.41612576e-02\\n -5.94995570e-02 -8.20698870e-03 -4.22882578e-03  6.80837884e-03\\n -5.92156210e-03 -4.42046377e-02 -3.11022068e-03  1.27519686e-02\\n -2.42750372e-02  3.18950695e-03 -8.99184628e-03 -1.14160887e-02\\n -2.13195292e-02  2.72321229e-02  2.43580149e-02  4.79698584e-02\\n  1.57633237e-03  1.02520624e-02  2.47367835e-02  1.85468796e-02\\n -4.57562989e-03  1.32752383e-03 -6.29642310e-03  8.40690759e-03\\n -2.60533237e-02  2.76323698e-02  6.60951093e-03  5.30629829e-03\\n  2.53520970e-02  1.69215266e-02 -2.61483952e-03 -6.71131491e-03\\n  5.46937285e-03 -5.97617528e-02 -1.42509176e-02  4.13004223e-03\\n  1.29411329e-02 -2.72093628e-02 -2.93715802e-03 -5.18021604e-03\\n -3.84695268e-02  3.60825754e-03  2.55715587e-02 -1.90034087e-02\\n -8.50447714e-03  4.16462500e-02  1.67909150e-03  1.06985413e-02\\n  4.98036543e-03  1.98002745e-02  7.17260482e-03  1.96261350e-02\\n  5.26236166e-03  5.72595444e-02  1.06345536e-02  1.05960565e-02\\n -1.69090387e-02 -1.95007819e-02 -2.04973675e-02  1.64692266e-02\\n -1.11566013e-02 -7.89535138e-03 -2.42190147e-03  8.65168799e-03\\n  8.34619437e-03  7.60743785e-03 -4.97936525e-03 -2.35478726e-02\\n -1.35957651e-02 -7.73007372e-03  3.00457874e-03 -1.92640566e-02\\n  1.21936429e-02 -9.71640085e-03  8.71386255e-03  3.14575122e-03\\n -1.37281416e-02  1.17141768e-02  1.80123753e-02 -6.66588294e-03\\n  2.04618287e-02 -1.61215855e-02  2.69881441e-02  2.10167702e-02\\n -3.95844326e-02  7.90834126e-03 -6.95245836e-03 -2.23846097e-02\\n  5.47112149e-03  1.59621202e-02 -1.70674605e-02 -1.15036646e-02\\n -2.35951200e-04  4.00277591e-03  5.11311344e-02  1.90127364e-03\\n -4.69950960e-03  5.25273134e-03 -7.74599087e-03  2.21447619e-02\\n -4.23891362e-02  7.16187345e-03  3.30663227e-03  7.78291542e-03\\n  1.17972884e-02 -2.61916775e-02  2.04059488e-02  7.80607975e-03\\n  1.86501568e-02 -2.44370912e-03  8.22888668e-03  2.05580226e-02\\n  4.91028706e-02  3.02739000e-02 -2.67842186e-02  1.67950423e-02\\n -2.52353415e-02 -2.34121977e-02 -1.49935251e-02 -1.90585255e-02\\n  1.89108599e-02 -2.26827811e-02 -3.37478641e-02  1.00238433e-03\\n -1.15836900e-02  1.53575970e-02  6.90472348e-03  1.59376187e-02\\n -2.46351168e-02 -6.10545690e-03  2.11095438e-02  3.45558206e-03\\n -1.68054254e-02  9.55403835e-03 -1.29725687e-02  4.17340333e-03\\n  2.85763908e-03  3.22386370e-03  6.28679047e-03 -4.97393638e-03\\n -9.24316446e-04  2.09124292e-02 -4.47707386e-03 -1.24214255e-02\\n -5.90546842e-03 -7.82709271e-03  1.19079326e-02  1.68293722e-02\\n -3.61920702e-02 -1.28526327e-02  3.64662296e-02  1.89649856e-02\\n  4.63348730e-03  5.31295636e-03 -9.54935594e-03  2.62056754e-02\\n -8.73748394e-03 -1.08076415e-02  9.45932962e-03  2.58001866e-04\\n  4.00741179e-02 -1.65857503e-02 -7.70053663e-03 -9.39319313e-04\\n -3.76799650e-02 -4.00485082e-02  8.09101953e-03  4.05088110e-03\\n -3.66777202e-03 -2.41950128e-02 -9.92837803e-03 -1.25023574e-02\\n -3.76216228e-03 -4.23309664e-03  9.30724359e-03 -6.59433371e-03\\n -2.42237338e-02  2.37082110e-02  8.44670989e-03 -3.40038454e-03\\n -8.45708025e-03  2.31053003e-02 -4.30338052e-02 -7.76702337e-03\\n  4.17902361e-02  7.92741106e-03 -1.25692073e-02  4.58499783e-02\\n -1.22328254e-02  4.88906401e-03 -6.13297517e-03  6.59261958e-03\\n  8.93494870e-03 -7.54106123e-03  2.51923277e-02 -8.07218900e-03\\n  3.91844821e-02  1.29781599e-02  3.85798762e-03  5.35014013e-03\\n  2.33702930e-02  2.35148120e-03  4.50595866e-03 -3.30996091e-03\\n -5.75047083e-02 -5.02152716e-02 -2.61246690e-02 -9.08110303e-33\\n -4.15554468e-03 -1.43868388e-03 -1.49772376e-02  1.24920710e-02\\n -3.88860137e-02 -2.16389684e-02 -8.06490555e-03  2.93958207e-02\\n -2.61369787e-02 -6.98825346e-03 -1.96128788e-02  2.86261040e-03\\n  2.63184500e-02  1.80576468e-02  2.34309593e-02 -1.80470480e-02\\n  1.45407620e-02  3.07538884e-03 -1.26949663e-02 -2.14562851e-02\\n  6.56297240e-03  1.67781010e-02  4.53650920e-02  3.40958732e-02\\n  8.40484552e-03 -5.39982481e-03 -2.22298583e-02  1.59023374e-02\\n  5.59190976e-02 -7.48112892e-03 -8.96124428e-03 -1.29033227e-02\\n -4.23878116e-03 -7.93407012e-03  1.78874184e-02  2.65625964e-03\\n -4.47995617e-03 -1.39635894e-02 -3.85419529e-03 -2.44355726e-02\\n -3.02175874e-02 -3.32465669e-02  1.85206969e-02 -2.41291010e-02\\n  6.50623050e-03 -4.24197760e-02  6.11238054e-04 -2.00167131e-02\\n -6.11113033e-03  5.68396489e-03 -2.88443033e-02  6.26242506e-03\\n -2.52263777e-02  2.94663569e-02 -1.58513227e-02 -4.41704135e-02\\n -4.38353105e-03 -7.71329733e-03 -2.83538247e-02 -1.74012267e-02\\n  1.13115684e-02  1.70406255e-02 -1.37456609e-02 -3.13645118e-02\\n  8.73514642e-03 -6.23020100e-04 -1.19381452e-03  4.08673860e-03\\n -1.18846826e-02  1.64729290e-02 -1.87799075e-02  1.89631350e-02\\n -2.04068168e-03 -6.72838002e-03  5.09032562e-03 -3.80918523e-02\\n -2.69219093e-02  6.80783845e-03  3.60880594e-02  8.83781858e-03\\n  1.98170419e-02 -1.12926653e-03 -3.68097308e-03 -4.73110523e-03\\n  1.27985031e-02 -7.53202267e-03  2.06638925e-03 -7.49733238e-03\\n -7.98854965e-03 -1.56549805e-02  6.64446451e-03 -4.63287638e-03\\n -2.82805086e-02  3.79167121e-03 -2.14677565e-02 -2.86093051e-02\\n  1.72047238e-02  1.09676562e-02 -1.18554454e-02  3.56363925e-03\\n  1.87418562e-02  3.05636837e-02 -3.27335607e-02  3.89830121e-02\\n  5.21729438e-03  7.90548395e-03 -5.80062324e-03  1.14268342e-02\\n -3.65542251e-02 -4.24729221e-03 -8.28804045e-03 -9.18729780e-03\\n  2.18852749e-02  7.82529913e-03 -2.86439524e-03  6.62666590e-03\\n  1.08469155e-02 -2.62644806e-02 -1.60617438e-02  1.34998847e-02\\n  1.29580479e-02  3.71681112e-02 -3.69644171e-02  8.58778368e-03\\n -2.68853044e-02  3.19607551e-02 -5.72157333e-03 -1.35049604e-02\\n -1.32003203e-02 -2.41845904e-02  4.59353840e-03 -2.37853331e-02\\n  2.88209159e-07 -3.28755930e-02  5.97258027e-03  2.66476651e-03\\n  1.69436050e-02  7.07147186e-03  5.55060492e-03 -4.76350738e-03\\n  4.14118452e-02 -1.45273279e-02  7.80837732e-03  1.24906309e-02\\n -1.64471199e-02  1.89501929e-02  1.11005678e-02  3.43546185e-02\\n -8.42664996e-03  1.33675890e-02 -1.08726216e-02 -1.49786715e-02\\n -3.77891450e-02 -3.06918871e-02  2.20430030e-02  3.44840531e-02\\n -4.26428217e-03 -1.77098887e-04 -1.47950682e-02  1.13667047e-02\\n -2.88687983e-02  2.02588041e-02 -2.13983260e-02  3.26936061e-02\\n -1.99093062e-03 -1.56497859e-02 -4.84833370e-03  3.99674361e-03\\n -1.47817161e-02  1.90957192e-02 -2.65959709e-02  1.75590875e-02\\n  5.50005516e-02 -3.16518438e-02 -1.29040641e-02 -8.06496841e-03\\n  7.23092540e-03  6.70646015e-03  4.22482086e-03 -2.57197690e-02\\n  9.31821870e-03 -5.34059667e-02 -1.91746527e-02 -1.05065909e-02\\n  6.75381201e-03  2.36980398e-02  4.84478425e-02  1.48178647e-03\\n -1.92999945e-02  4.20269838e-02 -3.93318976e-03  2.55282646e-02\\n  2.44320588e-02 -1.25277269e-02  1.16405849e-02  8.75171185e-04\\n  5.53073638e-02  2.04100095e-02  9.70325415e-03 -1.65828395e-03\\n  1.15786313e-34  1.74611442e-02 -3.68777750e-03  1.75174347e-02\\n  7.89770758e-04  4.33066356e-03 -2.18238755e-02  3.76428281e-03\\n -1.64803646e-02  1.92652971e-02 -2.25062210e-02 -4.92751586e-03]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "execution_count": 36
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. RAG - Search and Answer"
      ],
      "metadata": {
        "id": "tDBzGU-G6s5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity search\n",
        "Similarity search or semantic search or vector search is the idea of searching on *semantic*.\n",
        "\n",
        "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
        "\n",
        "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
        "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n"
      ],
      "metadata": {
        "id": "JMMQYkbX6s5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T14:43:30.576094Z",
          "iopub.execute_input": "2024-10-17T14:43:30.576465Z",
          "iopub.status.idle": "2024-10-17T14:43:30.580694Z",
          "shell.execute_reply.started": "2024-10-17T14:43:30.57643Z",
          "shell.execute_reply": "2024-10-17T14:43:30.579743Z"
        },
        "trusted": true,
        "id": "KtRlNQha6s5t"
      },
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "text_chunks_and_embedding_df = pd.read_csv(save_path)\n",
        "#convert embedding to array (it got converted to string when it saved)\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "\n",
        "#converting embedding into torch tensor\n",
        "embeddings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"embedding\"].tolist(), axis=0), dtype=torch.float32).to(device)\n",
        "# Convert texts and embedding df to list of dicts\n",
        "pages_and_chunks = text = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "text_chunks_and_embeddings_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T14:43:31.0295Z",
          "iopub.execute_input": "2024-10-17T14:43:31.029767Z",
          "iopub.status.idle": "2024-10-17T14:43:31.078834Z",
          "shell.execute_reply.started": "2024-10-17T14:43:31.029738Z",
          "shell.execute_reply": "2024-10-17T14:43:31.078036Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2PrZ3wNK6s5t",
        "outputId": "f3cffdff-da56-4ed3-8262-3faa7ae9420b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    page_number                                     sentence_chunk  \\\n",
              "0             0  This article has been accepted for inclusion i...   \n",
              "1             1  This article has been accepted for inclusion i...   \n",
              "2             2  This article has been accepted for inclusion i...   \n",
              "3             3  This article has been accepted for inclusion i...   \n",
              "4             4  This article has been accepted for inclusion i...   \n",
              "5             5  This article has been accepted for inclusion i...   \n",
              "6             6  This article has been accepted for inclusion i...   \n",
              "7             7  This article has been accepted for inclusion i...   \n",
              "8             8  This article has been accepted for inclusion i...   \n",
              "9             9  This article has been accepted for inclusion i...   \n",
              "10           10  This article has been accepted for inclusion i...   \n",
              "11           11  This article has been accepted for inclusion i...   \n",
              "12           12  This article has been accepted for inclusion i...   \n",
              "13           13  This article has been accepted for inclusion i...   \n",
              "14           13  Process.Syst.,2015, pp.10–18. [16] A. Khetan, ...   \n",
              "15           13  10, pp.2261–2274, Oct. 2015. [29] Y. Huang, D....   \n",
              "16           14  This article has been accepted for inclusion i...   \n",
              "\n",
              "    chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
              "0               7014              1001            1753.50   \n",
              "1               6134               926            1533.50   \n",
              "2               6107              1000            1526.75   \n",
              "3               4871               897            1217.75   \n",
              "4               4136               805            1034.00   \n",
              "5               4805               905            1201.25   \n",
              "6               4160               842            1040.00   \n",
              "7               4333               814            1083.25   \n",
              "8               5685               929            1421.25   \n",
              "9               3349               510             837.25   \n",
              "10              4449               703            1112.25   \n",
              "11              4195               660            1048.75   \n",
              "12              3607               627             901.75   \n",
              "13              3357               523             839.25   \n",
              "14              2280               298             570.00   \n",
              "15              1922               253             480.50   \n",
              "16              6392               909            1598.00   \n",
              "\n",
              "                                            embedding  \\\n",
              "0   [0.022123985, 0.038127348, -0.009931545, 0.064...   \n",
              "1   [0.02105509, 0.018980158, 0.0057187844, 0.0361...   \n",
              "2   [0.023228876, -0.03220968, -0.013121449, 0.025...   \n",
              "3   [0.017447194, 0.010092872, -0.00088838156, 0.0...   \n",
              "4   [-0.017802944, 0.00812479, -0.016401658, 0.038...   \n",
              "5   [-0.005026503, -0.016304526, -0.013110023, 0.0...   \n",
              "6   [-0.030130707, 0.0073698177, -0.006446064, 0.0...   \n",
              "7   [-0.06060846, 0.030870685, -0.023983289, 0.024...   \n",
              "8   [0.010136956, 0.0011052894, -0.008268423, 0.03...   \n",
              "9   [0.0014660318, 0.0242222, -0.009057149, 0.0415...   \n",
              "10  [-0.003505797, 0.03479154, -9.8598524e-05, 0.0...   \n",
              "11  [-0.013576578, 0.047201198, 0.00043018148, 0.0...   \n",
              "12  [0.004537486, 0.0271196, -0.023208935, 0.04752...   \n",
              "13  [0.023264721, 0.035725858, -0.00034581448, 0.0...   \n",
              "14  [0.037654877, 0.0320328, 0.0070856614, 0.03966...   \n",
              "15  [-0.014664694, -0.015721906, -0.017424468, 0.0...   \n",
              "16  [0.01783076, 0.01293898, -0.0071832705, 0.0251...   \n",
              "\n",
              "                                                words  \\\n",
              "0   [This, article, has, been, accepted, for, incl...   \n",
              "1   [This, article, has, been, accepted, for, incl...   \n",
              "2   [This, article, has, been, accepted, for, incl...   \n",
              "3   [This, article, has, been, accepted, for, incl...   \n",
              "4   [This, article, has, been, accepted, for, incl...   \n",
              "5   [This, article, has, been, accepted, for, incl...   \n",
              "6   [This, article, has, been, accepted, for, incl...   \n",
              "7   [This, article, has, been, accepted, for, incl...   \n",
              "8   [This, article, has, been, accepted, for, incl...   \n",
              "9   [This, article, has, been, accepted, for, incl...   \n",
              "10  [This, article, has, been, accepted, for, incl...   \n",
              "11  [This, article, has, been, accepted, for, incl...   \n",
              "12  [This, article, has, been, accepted, for, incl...   \n",
              "13  [This, article, has, been, accepted, for, incl...   \n",
              "14  [Process.Syst.,2015,, pp.10–18., [16], A., Khe...   \n",
              "15  [10,, pp.2261–2274,, Oct., 2015., [29], Y., Hu...   \n",
              "16  [This, article, has, been, accepted, for, incl...   \n",
              "\n",
              "                                      word_embeddings  \\\n",
              "0   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "1   [[-0.023993477, 0.024364106, -0.011307317, -0....   \n",
              "2   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "3   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "4   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "5   [[-0.023993539, 0.024364077, -0.0113073345, -0...   \n",
              "6   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "7   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "8   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "9   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "10  [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "11  [[-0.023993477, 0.024364106, -0.011307317, -0....   \n",
              "12  [[-0.023993539, 0.024364077, -0.0113073345, -0...   \n",
              "13  [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "14  [[-0.004314674, -0.051323794, 0.011045121, 0.0...   \n",
              "15  [[0.017610539, -0.037211094, -0.020187102, 0.0...   \n",
              "16  [[-0.023993539, 0.024364077, -0.0113073345, -0...   \n",
              "\n",
              "                                   document_embedding  \n",
              "0   [0.01831590532765253, 0.036117391744675084, -0...  \n",
              "1   [0.021275918194007582, 0.03146616457338843, -0...  \n",
              "2   [0.016969398394478568, 0.026701183117873787, -...  \n",
              "3   [0.01754943361979846, 0.0244253293871066, -0.0...  \n",
              "4   [0.017377774461851764, 0.026251409941161014, -...  \n",
              "5   [0.020575316407115806, 0.02532501490578474, -0...  \n",
              "6   [0.011563947027782798, 0.012915680992572594, -...  \n",
              "7   [0.012161504941071858, 0.0393414103283349, -0....  \n",
              "8   [0.01592404765243526, 0.028752114606936725, -0...  \n",
              "9   [0.014890766357942134, 0.03084731397283874, -0...  \n",
              "10  [0.013934290739080011, 0.02737133204566321, -0...  \n",
              "11  [0.010730812660485171, 0.038121874835327946, -...  \n",
              "12  [0.01694370908743994, 0.03405444481441076, -0....  \n",
              "13  [-0.0018773110310817158, 0.04650291931288832, ...  \n",
              "14  [-0.006383168377973299, 0.04019564950243037, -...  \n",
              "15  [0.005657049886792173, 0.03502947068364055, -0...  \n",
              "16  [0.012267822924323697, 0.04525713942683209, -0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a3204b8-e7d0-4910-b1f8-896115e1279f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>sentence_chunk</th>\n",
              "      <th>chunk_char_count</th>\n",
              "      <th>chunk_word_count</th>\n",
              "      <th>chunk_token_count</th>\n",
              "      <th>embedding</th>\n",
              "      <th>words</th>\n",
              "      <th>word_embeddings</th>\n",
              "      <th>document_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>7014</td>\n",
              "      <td>1001</td>\n",
              "      <td>1753.50</td>\n",
              "      <td>[0.022123985, 0.038127348, -0.009931545, 0.064...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.01831590532765253, 0.036117391744675084, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6134</td>\n",
              "      <td>926</td>\n",
              "      <td>1533.50</td>\n",
              "      <td>[0.02105509, 0.018980158, 0.0057187844, 0.0361...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993477, 0.024364106, -0.011307317, -0....</td>\n",
              "      <td>[0.021275918194007582, 0.03146616457338843, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6107</td>\n",
              "      <td>1000</td>\n",
              "      <td>1526.75</td>\n",
              "      <td>[0.023228876, -0.03220968, -0.013121449, 0.025...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.016969398394478568, 0.026701183117873787, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4871</td>\n",
              "      <td>897</td>\n",
              "      <td>1217.75</td>\n",
              "      <td>[0.017447194, 0.010092872, -0.00088838156, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.01754943361979846, 0.0244253293871066, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4136</td>\n",
              "      <td>805</td>\n",
              "      <td>1034.00</td>\n",
              "      <td>[-0.017802944, 0.00812479, -0.016401658, 0.038...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.017377774461851764, 0.026251409941161014, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4805</td>\n",
              "      <td>905</td>\n",
              "      <td>1201.25</td>\n",
              "      <td>[-0.005026503, -0.016304526, -0.013110023, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993539, 0.024364077, -0.0113073345, -0...</td>\n",
              "      <td>[0.020575316407115806, 0.02532501490578474, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4160</td>\n",
              "      <td>842</td>\n",
              "      <td>1040.00</td>\n",
              "      <td>[-0.030130707, 0.0073698177, -0.006446064, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.011563947027782798, 0.012915680992572594, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4333</td>\n",
              "      <td>814</td>\n",
              "      <td>1083.25</td>\n",
              "      <td>[-0.06060846, 0.030870685, -0.023983289, 0.024...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.012161504941071858, 0.0393414103283349, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>5685</td>\n",
              "      <td>929</td>\n",
              "      <td>1421.25</td>\n",
              "      <td>[0.010136956, 0.0011052894, -0.008268423, 0.03...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.01592404765243526, 0.028752114606936725, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>3349</td>\n",
              "      <td>510</td>\n",
              "      <td>837.25</td>\n",
              "      <td>[0.0014660318, 0.0242222, -0.009057149, 0.0415...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.014890766357942134, 0.03084731397283874, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4449</td>\n",
              "      <td>703</td>\n",
              "      <td>1112.25</td>\n",
              "      <td>[-0.003505797, 0.03479154, -9.8598524e-05, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.013934290739080011, 0.02737133204566321, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4195</td>\n",
              "      <td>660</td>\n",
              "      <td>1048.75</td>\n",
              "      <td>[-0.013576578, 0.047201198, 0.00043018148, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993477, 0.024364106, -0.011307317, -0....</td>\n",
              "      <td>[0.010730812660485171, 0.038121874835327946, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>3607</td>\n",
              "      <td>627</td>\n",
              "      <td>901.75</td>\n",
              "      <td>[0.004537486, 0.0271196, -0.023208935, 0.04752...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993539, 0.024364077, -0.0113073345, -0...</td>\n",
              "      <td>[0.01694370908743994, 0.03405444481441076, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>3357</td>\n",
              "      <td>523</td>\n",
              "      <td>839.25</td>\n",
              "      <td>[0.023264721, 0.035725858, -0.00034581448, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[-0.0018773110310817158, 0.04650291931288832, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>Process.Syst.,2015, pp.10–18. [16] A. Khetan, ...</td>\n",
              "      <td>2280</td>\n",
              "      <td>298</td>\n",
              "      <td>570.00</td>\n",
              "      <td>[0.037654877, 0.0320328, 0.0070856614, 0.03966...</td>\n",
              "      <td>[Process.Syst.,2015,, pp.10–18., [16], A., Khe...</td>\n",
              "      <td>[[-0.004314674, -0.051323794, 0.011045121, 0.0...</td>\n",
              "      <td>[-0.006383168377973299, 0.04019564950243037, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>13</td>\n",
              "      <td>10, pp.2261–2274, Oct. 2015. [29] Y. Huang, D....</td>\n",
              "      <td>1922</td>\n",
              "      <td>253</td>\n",
              "      <td>480.50</td>\n",
              "      <td>[-0.014664694, -0.015721906, -0.017424468, 0.0...</td>\n",
              "      <td>[10,, pp.2261–2274,, Oct., 2015., [29], Y., Hu...</td>\n",
              "      <td>[[0.017610539, -0.037211094, -0.020187102, 0.0...</td>\n",
              "      <td>[0.005657049886792173, 0.03502947068364055, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>14</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6392</td>\n",
              "      <td>909</td>\n",
              "      <td>1598.00</td>\n",
              "      <td>[0.01783076, 0.01293898, -0.0071832705, 0.0251...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993539, 0.024364077, -0.0113073345, -0...</td>\n",
              "      <td>[0.012267822924323697, 0.04525713942683209, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a3204b8-e7d0-4910-b1f8-896115e1279f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4a3204b8-e7d0-4910-b1f8-896115e1279f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4a3204b8-e7d0-4910-b1f8-896115e1279f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9dd6a790-454f-4780-a0bb-f282c2acfe7b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9dd6a790-454f-4780-a0bb-f282c2acfe7b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9dd6a790-454f-4780-a0bb-f282c2acfe7b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_68d7a201-c672-440f-9790-11503376bb3e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('text_chunks_and_embeddings_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_68d7a201-c672-440f-9790-11503376bb3e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('text_chunks_and_embeddings_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "text_chunks_and_embeddings_df",
              "summary": "{\n  \"name\": \"text_chunks_and_embeddings_df\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classi\\ufb01cation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract\\u2014Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classi\\ufb01er training and severely decrease the classi\\ufb01cation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classi\\ufb01cation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as \\u201clabel noise handling via side information\\u201d (LNSI).Speci\\ufb01cally, the observed label matrix is decomposed as the sum of two parts, in which the \\ufb01rst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been suf\\ufb01ciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the \\u201cSummit of the Six Top Talents\\u201d Program under Grant DZXX-027, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by Jiangsu Province, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the \\u201c111\\u201d Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi\\u2019an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Arti\\ufb01cial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the \\ufb01gures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identi\\ufb01er 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classi\\ufb01cation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets con\\ufb01rm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms\\u2014Classi\\ufb01cation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classi\\ufb01er, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the suf\\ufb01- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classi\\ufb01cation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signi\\ufb01cantly mislead the classi\\ufb01er training and then severely decrease the classi\\ufb01cation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signi\\ufb01cance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]\\u2013[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classi\\ufb01cation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite dif\\ufb01cult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X \\u00a9 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z\\u2217and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning \\ufb01elds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and \\ufb01rst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speci\\ufb01cally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]\\u2013[11].Therefore, our proposed method is named as \\u201clabel noise handling via side information\\u201d (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z\\u2217on the example features (i.e., T = X Z\\u2217), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classi\\ufb01cation, which does not need the one-versus-one or one-versus-the-rest operations; 2) suf\\ufb01cient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classi\\ufb01er parameter optimization into a uni\\ufb01ed framework.Due to the above merits, a reliable classi\\ufb01er can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classi\\ufb01er.II.RELATED WORK This section brie\\ufb02y reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches \\ufb01rst detect and eliminate label noise and then conduct the standard supervised classi\\ufb01cation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble \\ufb01l- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the in\\ufb02uence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classi\\ufb01cation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speci\\ufb01c tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Algorithm 1 Algorithm for Solving LNSI Input: feature matrix X, observed label matrix Y; trade-off parameters: \\u03bb1, \\u03bb2, and \\u03bb3; Z = O, J = Z, E = O, B = O, M1 = O, M2 = O, M3 = O; \\u03bc = 10\\u22123, \\u03bcmax = 106, \\u03c1 = 1.2, \\u03f5 = 10\\u22126, iter_max = 1000; iter = 0; 1: Construct graph G and calculate the Laplacian matrix L via (3); 2: while not converge do 3: Update Z via (10), 4: Update E via (14), 5: Update J via (16), 6: Update B via (19), 7: Update the multipliers M1 := M1 + \\u03bc(Y \\u2212B \\u2212E), M2 := M2 + \\u03bc(B \\u2212X J), M3 := M3 + \\u03bc(Z \\u2212J), 8: Update the parameter \\u03bc by \\u03bc := min(\\u03c1\\u03bc, \\u03bcmax), 9: iter := iter + 1, 10: Check the convergence conditions: \\u2225Y \\u2212B\\u2212E\\u2225F \\u2264\\u03f5 and \\u2225B\\u2212X J\\u2225F \\u2264\\u03f5 and \\u2225Z\\u2212J\\u2225F \\u2264 \\u03f5; or iter > iter_max.11: end while Output: optimized Z\\u2217and E\\u2217. B. Computational Complexity This section studies the computational complexity of Algorithm 1.The graph construction in Line 1 of Algorithm 1 takes O(n2) complexity.Line 3 is accomplished by using the SVD, of which the complexity is O(min(d2c, dc2)).In Line 4, one should compute the \\u21132-norm of each row of a n \\u00d7 c matrix E, so the complexity is O(nc).Note that a d \\u00d7 d matrix should be inverted in Line 5, so the complexity of this step is O(d3).Therefore, the total complexity of our proposed algorithm is O(n2 +(min(d2c, dc2)+nc+d3)k) by assuming that Lines 2\\u20139 are iterated k times.Note that the complexity of Algorithm 1 is squared to the number of training examples n, so its complexity is acceptable.C. Generalization Bound In this section, we derive the generalization bound of LNSI.1) Preliminaries: Recall that our goal is to \\ufb01nd a suit- able project matrix Z by recovering the clean label matrix X Z, given the observed noisy label matrix Y and example features X. Similar to [10], (6) can be reformulated to the following expression with hard constraints, namely: min Z,E \\u0002 (i, j)\\u2208{1,...,n}\\u00d7{1,...,c} \\u2113((X Z + E)ij , Y ij ) s.t.\\u2225Z\\u2225\\u2217\\u2264Z\\u2217, \\u2225Z\\u22252 F \\u2264ZF, X Z \\u2208[\\u22121, 1]n\\u00d7c tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1. (21) Let \\u03b8 = (Z, E) be any feasible solution, and \\r = {(Z, E) | \\u2225Z\\u2225\\u2217 \\u2264 Z\\u2217, \\u2225Z\\u2225F \\u2264 \\u221aZF, X Z \\u2208 [\\u22121, 1]n\\u00d7c, tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1} be the feasible solution set.Also, let f\\u03b8(i, j) = Xi ZI j + Eij be the estimation function for Yij parameterized by \\u03b8 = (Z, E), and F\\r = { f\\u03b8 | \\u03b8 \\u2208\\r} be the set of feasible functions.I j is the jth column of identity matrix I \\u2208Rc\\u00d7c.We are interested in the following two \\u201c\\u2113-risk\\u201d quantities: 1) expected \\u2113-risk: R\\u2113( f ) = Ei, j [\\u2113( f (i, j), Yij )]; 2) empirical \\u2113-risk: \\u02c6R\\u2113( f ) = (1/nr) \\u0011 (i, j) \\u2113( f (i, j)Yij ), where nr is the number of observed entries.Thus, LNSI is to \\ufb01nd a proper \\u03b8\\u2217= (Z\\u2217, E\\u2217) that parameterizes f \\u2217= arg min f \\u2208F\\r \\u02c6R\\u2113( f ).2) Generalization Bound of LNSI: To bound the generaliza- tion error of LNSI, we \\ufb01rst link the quality of training labels to Rademacher complexity, which theoretically measures the complexity of a function class.We will show that high-quality labels of training examples will result in a lower model complexity and thus a smaller error bound.To begin with, we apply the following lemma to bound the expected \\u2113-risk.Lemma 3 (Bound on Expected \\u2113-Risk [39]): Let \\u2113be the loss function bounded by B with Lipschitz constant L\\u2113, and \\u03b4 be a constant where 0 < \\u03b4 < 1.With probability at least 1 \\u2212\\u03b4, we have max f \\u2208F |R\\u2113( f ) \\u2212\\u02c6R\\u2113( f )| \\u22642L\\u2113Rn(F) + B \\u0012 ln(1/\\u03b4) 2nr where Rn(F) := E[R(F)] is the Rademacher complexity of the function class F and R(F) := E\\u03c3 \\u0013 sup f \\u2208F 1 nr nr \\u0002 \\u03b1=1 \\u03c3\\u03b1 f (\\u03b1) \\u0014 is the empirical Rademacher complexity on the training examples.Note that \\u03c3\\u03b1 (\\u03b1 = 1, 2, . . . ,nr) are independent identically distributed (i.i.d.)Rademacher random variables.Given Lemma 3, we see that the key to derive the upper bound of a function f \\u2208F is to bound the complexity Rn(F\\r).More formally, the Rademacher complexity can be bounded in terms of the constraints in (21).Before diving into the details, we \\ufb01rst provide several useful theorems and lemmas.Lemma 4 (Complexity Bound [40]): Let S be a closed con- vex set and let F : S \\u2192R be \\u03b2-strongly convex with respect to \\u2225\\u00b7 \\u2225. In addition, we assume that F\\u22c6(O) = 0 with F\\u22c6being the Fenchel conjugate of function F. Further, let A = {A : \\u2225A\\u2225\\u22c6\\u2264A} and de\\ufb01ne W = {W \\u2208S : F(W) \\u2264Fmax}.Considering the class of linear functions F = {A \\u2192 W, A : W \\u2208W}, we have R(F) \\u2264A \\u0012 2Fmax \\u03b2nr (22) where W, A = tr(W\\u22a4A).Lemma 5 [41]: The function F : Rn\\u00d7c \\u2192R de\\ufb01ned as F(W) = (1/2)\\u2225W\\u22252 2,q for q = (ln(c)/(ln(c) \\u22121)) is (1/(3 ln(c)))-strongly convex with respect to \\u2225\\u00b7\\u22252,1 over Rn\\u00d7c.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1417,\n        \"min\": 1922,\n        \"max\": 7014,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          7014,\n          6134,\n          4805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 231,\n        \"min\": 253,\n        \"max\": 1001,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1001,\n          926,\n          905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 354.4185164950596,\n        \"min\": 480.5,\n        \"max\": 1753.5,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1753.5,\n          1533.5,\n          1201.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"words\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "text_chunks_and_embedding_df = pd.read_csv(save_path)\n",
        "#convert embedding to array (it got converted to string when it saved)\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "text_chunks_and_embedding_df[\"document_embedding\"] = text_chunks_and_embedding_df[\"document_embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "\n",
        "embeddings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"embedding\"].tolist(), axis=0), dtype=torch.float32).to(device)\n",
        "doc_embedings = torch.tensor(np.stack(text_chunks_and_embedding_df[\"document_embedding\"].tolist(), axis=0), dtype=torch.float32).to(device)\n",
        "# Convert texts and embedding df to list of dicts\n",
        "pages_and_chunks = text = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "text_chunks_and_embeddings_df\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:03:37.362016Z",
          "iopub.execute_input": "2024-10-17T18:03:37.362973Z",
          "iopub.status.idle": "2024-10-17T18:03:39.72967Z",
          "shell.execute_reply.started": "2024-10-17T18:03:37.362929Z",
          "shell.execute_reply": "2024-10-17T18:03:39.728667Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_XC77lFC6s5t",
        "outputId": "b50e34ab-a188-4b8b-99ab-4bdb2ca739b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    page_number                                     sentence_chunk  \\\n",
              "0             0  This article has been accepted for inclusion i...   \n",
              "1             1  This article has been accepted for inclusion i...   \n",
              "2             2  This article has been accepted for inclusion i...   \n",
              "3             3  This article has been accepted for inclusion i...   \n",
              "4             4  This article has been accepted for inclusion i...   \n",
              "5             5  This article has been accepted for inclusion i...   \n",
              "6             6  This article has been accepted for inclusion i...   \n",
              "7             7  This article has been accepted for inclusion i...   \n",
              "8             8  This article has been accepted for inclusion i...   \n",
              "9             9  This article has been accepted for inclusion i...   \n",
              "10           10  This article has been accepted for inclusion i...   \n",
              "11           11  This article has been accepted for inclusion i...   \n",
              "12           12  This article has been accepted for inclusion i...   \n",
              "13           13  This article has been accepted for inclusion i...   \n",
              "14           13  Process.Syst.,2015, pp.10–18. [16] A. Khetan, ...   \n",
              "15           13  10, pp.2261–2274, Oct. 2015. [29] Y. Huang, D....   \n",
              "16           14  This article has been accepted for inclusion i...   \n",
              "\n",
              "    chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
              "0               7014              1001            1753.50   \n",
              "1               6134               926            1533.50   \n",
              "2               6107              1000            1526.75   \n",
              "3               4871               897            1217.75   \n",
              "4               4136               805            1034.00   \n",
              "5               4805               905            1201.25   \n",
              "6               4160               842            1040.00   \n",
              "7               4333               814            1083.25   \n",
              "8               5685               929            1421.25   \n",
              "9               3349               510             837.25   \n",
              "10              4449               703            1112.25   \n",
              "11              4195               660            1048.75   \n",
              "12              3607               627             901.75   \n",
              "13              3357               523             839.25   \n",
              "14              2280               298             570.00   \n",
              "15              1922               253             480.50   \n",
              "16              6392               909            1598.00   \n",
              "\n",
              "                                            embedding  \\\n",
              "0   [0.022123985, 0.038127348, -0.009931545, 0.064...   \n",
              "1   [0.02105509, 0.018980158, 0.0057187844, 0.0361...   \n",
              "2   [0.023228876, -0.03220968, -0.013121449, 0.025...   \n",
              "3   [0.017447194, 0.010092872, -0.00088838156, 0.0...   \n",
              "4   [-0.017802944, 0.00812479, -0.016401658, 0.038...   \n",
              "5   [-0.005026503, -0.016304526, -0.013110023, 0.0...   \n",
              "6   [-0.030130707, 0.0073698177, -0.006446064, 0.0...   \n",
              "7   [-0.06060846, 0.030870685, -0.023983289, 0.024...   \n",
              "8   [0.010136956, 0.0011052894, -0.008268423, 0.03...   \n",
              "9   [0.0014660318, 0.0242222, -0.009057149, 0.0415...   \n",
              "10  [-0.003505797, 0.03479154, -9.8598524e-05, 0.0...   \n",
              "11  [-0.013576578, 0.047201198, 0.00043018148, 0.0...   \n",
              "12  [0.004537486, 0.0271196, -0.023208935, 0.04752...   \n",
              "13  [0.023264721, 0.035725858, -0.00034581448, 0.0...   \n",
              "14  [0.037654877, 0.0320328, 0.0070856614, 0.03966...   \n",
              "15  [-0.014664694, -0.015721906, -0.017424468, 0.0...   \n",
              "16  [0.01783076, 0.01293898, -0.0071832705, 0.0251...   \n",
              "\n",
              "                                                words  \\\n",
              "0   [This, article, has, been, accepted, for, incl...   \n",
              "1   [This, article, has, been, accepted, for, incl...   \n",
              "2   [This, article, has, been, accepted, for, incl...   \n",
              "3   [This, article, has, been, accepted, for, incl...   \n",
              "4   [This, article, has, been, accepted, for, incl...   \n",
              "5   [This, article, has, been, accepted, for, incl...   \n",
              "6   [This, article, has, been, accepted, for, incl...   \n",
              "7   [This, article, has, been, accepted, for, incl...   \n",
              "8   [This, article, has, been, accepted, for, incl...   \n",
              "9   [This, article, has, been, accepted, for, incl...   \n",
              "10  [This, article, has, been, accepted, for, incl...   \n",
              "11  [This, article, has, been, accepted, for, incl...   \n",
              "12  [This, article, has, been, accepted, for, incl...   \n",
              "13  [This, article, has, been, accepted, for, incl...   \n",
              "14  [Process.Syst.,2015,, pp.10–18., [16], A., Khe...   \n",
              "15  [10,, pp.2261–2274,, Oct., 2015., [29], Y., Hu...   \n",
              "16  [This, article, has, been, accepted, for, incl...   \n",
              "\n",
              "                                      word_embeddings  \\\n",
              "0   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "1   [[-0.023993477, 0.024364106, -0.011307317, -0....   \n",
              "2   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "3   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "4   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "5   [[-0.023993539, 0.024364077, -0.0113073345, -0...   \n",
              "6   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "7   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "8   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "9   [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "10  [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "11  [[-0.023993477, 0.024364106, -0.011307317, -0....   \n",
              "12  [[-0.023993539, 0.024364077, -0.0113073345, -0...   \n",
              "13  [[-0.02399354, 0.024364082, -0.011307317, -0.0...   \n",
              "14  [[-0.004314674, -0.051323794, 0.011045121, 0.0...   \n",
              "15  [[0.017610539, -0.037211094, -0.020187102, 0.0...   \n",
              "16  [[-0.023993539, 0.024364077, -0.0113073345, -0...   \n",
              "\n",
              "                                   document_embedding  \n",
              "0   [0.01831590532765253, 0.036117391744675084, -0...  \n",
              "1   [0.021275918194007582, 0.03146616457338843, -0...  \n",
              "2   [0.016969398394478568, 0.026701183117873787, -...  \n",
              "3   [0.01754943361979846, 0.0244253293871066, -0.0...  \n",
              "4   [0.017377774461851764, 0.026251409941161014, -...  \n",
              "5   [0.020575316407115806, 0.02532501490578474, -0...  \n",
              "6   [0.011563947027782798, 0.012915680992572594, -...  \n",
              "7   [0.012161504941071858, 0.0393414103283349, -0....  \n",
              "8   [0.01592404765243526, 0.028752114606936725, -0...  \n",
              "9   [0.014890766357942134, 0.03084731397283874, -0...  \n",
              "10  [0.013934290739080011, 0.02737133204566321, -0...  \n",
              "11  [0.010730812660485171, 0.038121874835327946, -...  \n",
              "12  [0.01694370908743994, 0.03405444481441076, -0....  \n",
              "13  [-0.0018773110310817158, 0.04650291931288832, ...  \n",
              "14  [-0.006383168377973299, 0.04019564950243037, -...  \n",
              "15  [0.005657049886792173, 0.03502947068364055, -0...  \n",
              "16  [0.012267822924323697, 0.04525713942683209, -0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-67e3ae15-980a-494d-a11f-708c0d6dc036\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_number</th>\n",
              "      <th>sentence_chunk</th>\n",
              "      <th>chunk_char_count</th>\n",
              "      <th>chunk_word_count</th>\n",
              "      <th>chunk_token_count</th>\n",
              "      <th>embedding</th>\n",
              "      <th>words</th>\n",
              "      <th>word_embeddings</th>\n",
              "      <th>document_embedding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>7014</td>\n",
              "      <td>1001</td>\n",
              "      <td>1753.50</td>\n",
              "      <td>[0.022123985, 0.038127348, -0.009931545, 0.064...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.01831590532765253, 0.036117391744675084, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6134</td>\n",
              "      <td>926</td>\n",
              "      <td>1533.50</td>\n",
              "      <td>[0.02105509, 0.018980158, 0.0057187844, 0.0361...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993477, 0.024364106, -0.011307317, -0....</td>\n",
              "      <td>[0.021275918194007582, 0.03146616457338843, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6107</td>\n",
              "      <td>1000</td>\n",
              "      <td>1526.75</td>\n",
              "      <td>[0.023228876, -0.03220968, -0.013121449, 0.025...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.016969398394478568, 0.026701183117873787, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4871</td>\n",
              "      <td>897</td>\n",
              "      <td>1217.75</td>\n",
              "      <td>[0.017447194, 0.010092872, -0.00088838156, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.01754943361979846, 0.0244253293871066, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4136</td>\n",
              "      <td>805</td>\n",
              "      <td>1034.00</td>\n",
              "      <td>[-0.017802944, 0.00812479, -0.016401658, 0.038...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.017377774461851764, 0.026251409941161014, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4805</td>\n",
              "      <td>905</td>\n",
              "      <td>1201.25</td>\n",
              "      <td>[-0.005026503, -0.016304526, -0.013110023, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993539, 0.024364077, -0.0113073345, -0...</td>\n",
              "      <td>[0.020575316407115806, 0.02532501490578474, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4160</td>\n",
              "      <td>842</td>\n",
              "      <td>1040.00</td>\n",
              "      <td>[-0.030130707, 0.0073698177, -0.006446064, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.011563947027782798, 0.012915680992572594, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4333</td>\n",
              "      <td>814</td>\n",
              "      <td>1083.25</td>\n",
              "      <td>[-0.06060846, 0.030870685, -0.023983289, 0.024...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.012161504941071858, 0.0393414103283349, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>5685</td>\n",
              "      <td>929</td>\n",
              "      <td>1421.25</td>\n",
              "      <td>[0.010136956, 0.0011052894, -0.008268423, 0.03...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.01592404765243526, 0.028752114606936725, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>3349</td>\n",
              "      <td>510</td>\n",
              "      <td>837.25</td>\n",
              "      <td>[0.0014660318, 0.0242222, -0.009057149, 0.0415...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.014890766357942134, 0.03084731397283874, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4449</td>\n",
              "      <td>703</td>\n",
              "      <td>1112.25</td>\n",
              "      <td>[-0.003505797, 0.03479154, -9.8598524e-05, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[0.013934290739080011, 0.02737133204566321, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>4195</td>\n",
              "      <td>660</td>\n",
              "      <td>1048.75</td>\n",
              "      <td>[-0.013576578, 0.047201198, 0.00043018148, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993477, 0.024364106, -0.011307317, -0....</td>\n",
              "      <td>[0.010730812660485171, 0.038121874835327946, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>3607</td>\n",
              "      <td>627</td>\n",
              "      <td>901.75</td>\n",
              "      <td>[0.004537486, 0.0271196, -0.023208935, 0.04752...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993539, 0.024364077, -0.0113073345, -0...</td>\n",
              "      <td>[0.01694370908743994, 0.03405444481441076, -0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>3357</td>\n",
              "      <td>523</td>\n",
              "      <td>839.25</td>\n",
              "      <td>[0.023264721, 0.035725858, -0.00034581448, 0.0...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.02399354, 0.024364082, -0.011307317, -0.0...</td>\n",
              "      <td>[-0.0018773110310817158, 0.04650291931288832, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>13</td>\n",
              "      <td>Process.Syst.,2015, pp.10–18. [16] A. Khetan, ...</td>\n",
              "      <td>2280</td>\n",
              "      <td>298</td>\n",
              "      <td>570.00</td>\n",
              "      <td>[0.037654877, 0.0320328, 0.0070856614, 0.03966...</td>\n",
              "      <td>[Process.Syst.,2015,, pp.10–18., [16], A., Khe...</td>\n",
              "      <td>[[-0.004314674, -0.051323794, 0.011045121, 0.0...</td>\n",
              "      <td>[-0.006383168377973299, 0.04019564950243037, -...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>13</td>\n",
              "      <td>10, pp.2261–2274, Oct. 2015. [29] Y. Huang, D....</td>\n",
              "      <td>1922</td>\n",
              "      <td>253</td>\n",
              "      <td>480.50</td>\n",
              "      <td>[-0.014664694, -0.015721906, -0.017424468, 0.0...</td>\n",
              "      <td>[10,, pp.2261–2274,, Oct., 2015., [29], Y., Hu...</td>\n",
              "      <td>[[0.017610539, -0.037211094, -0.020187102, 0.0...</td>\n",
              "      <td>[0.005657049886792173, 0.03502947068364055, -0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>14</td>\n",
              "      <td>This article has been accepted for inclusion i...</td>\n",
              "      <td>6392</td>\n",
              "      <td>909</td>\n",
              "      <td>1598.00</td>\n",
              "      <td>[0.01783076, 0.01293898, -0.0071832705, 0.0251...</td>\n",
              "      <td>[This, article, has, been, accepted, for, incl...</td>\n",
              "      <td>[[-0.023993539, 0.024364077, -0.0113073345, -0...</td>\n",
              "      <td>[0.012267822924323697, 0.04525713942683209, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67e3ae15-980a-494d-a11f-708c0d6dc036')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-67e3ae15-980a-494d-a11f-708c0d6dc036 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-67e3ae15-980a-494d-a11f-708c0d6dc036');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-5806ace2-ec5e-4b0e-b6ef-3f784578c1dd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5806ace2-ec5e-4b0e-b6ef-3f784578c1dd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-5806ace2-ec5e-4b0e-b6ef-3f784578c1dd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_b68c2ef7-305d-419c-89a9-e988830993f9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('text_chunks_and_embeddings_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b68c2ef7-305d-419c-89a9-e988830993f9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('text_chunks_and_embeddings_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "text_chunks_and_embeddings_df",
              "summary": "{\n  \"name\": \"text_chunks_and_embeddings_df\",\n  \"rows\": 17,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 0,\n        \"max\": 14,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          9,\n          11,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17,\n        \"samples\": [\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classi\\ufb01cation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract\\u2014Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classi\\ufb01er training and severely decrease the classi\\ufb01cation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classi\\ufb01cation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as \\u201clabel noise handling via side information\\u201d (LNSI).Speci\\ufb01cally, the observed label matrix is decomposed as the sum of two parts, in which the \\ufb01rst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been suf\\ufb01ciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the \\u201cSummit of the Six Top Talents\\u201d Program under Grant DZXX-027, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by Jiangsu Province, in part by the \\u201cYoung Elite Scientists Sponsorship Program\\u201d by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the \\u201c111\\u201d Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi\\u2019an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Arti\\ufb01cial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the \\ufb01gures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identi\\ufb01er 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classi\\ufb01cation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets con\\ufb01rm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms\\u2014Classi\\ufb01cation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classi\\ufb01er, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the suf\\ufb01- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classi\\ufb01cation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signi\\ufb01cantly mislead the classi\\ufb01er training and then severely decrease the classi\\ufb01cation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signi\\ufb01cance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]\\u2013[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classi\\ufb01cation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite dif\\ufb01cult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X \\u00a9 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z\\u2217and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning \\ufb01elds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and \\ufb01rst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speci\\ufb01cally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]\\u2013[11].Therefore, our proposed method is named as \\u201clabel noise handling via side information\\u201d (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z\\u2217on the example features (i.e., T = X Z\\u2217), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classi\\ufb01cation, which does not need the one-versus-one or one-versus-the-rest operations; 2) suf\\ufb01cient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classi\\ufb01er parameter optimization into a uni\\ufb01ed framework.Due to the above merits, a reliable classi\\ufb01er can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classi\\ufb01er.II.RELATED WORK This section brie\\ufb02y reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches \\ufb01rst detect and eliminate label noise and then conduct the standard supervised classi\\ufb01cation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble \\ufb01l- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the in\\ufb02uence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classi\\ufb01cation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speci\\ufb01c tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\",\n          \"This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Algorithm 1 Algorithm for Solving LNSI Input: feature matrix X, observed label matrix Y; trade-off parameters: \\u03bb1, \\u03bb2, and \\u03bb3; Z = O, J = Z, E = O, B = O, M1 = O, M2 = O, M3 = O; \\u03bc = 10\\u22123, \\u03bcmax = 106, \\u03c1 = 1.2, \\u03f5 = 10\\u22126, iter_max = 1000; iter = 0; 1: Construct graph G and calculate the Laplacian matrix L via (3); 2: while not converge do 3: Update Z via (10), 4: Update E via (14), 5: Update J via (16), 6: Update B via (19), 7: Update the multipliers M1 := M1 + \\u03bc(Y \\u2212B \\u2212E), M2 := M2 + \\u03bc(B \\u2212X J), M3 := M3 + \\u03bc(Z \\u2212J), 8: Update the parameter \\u03bc by \\u03bc := min(\\u03c1\\u03bc, \\u03bcmax), 9: iter := iter + 1, 10: Check the convergence conditions: \\u2225Y \\u2212B\\u2212E\\u2225F \\u2264\\u03f5 and \\u2225B\\u2212X J\\u2225F \\u2264\\u03f5 and \\u2225Z\\u2212J\\u2225F \\u2264 \\u03f5; or iter > iter_max.11: end while Output: optimized Z\\u2217and E\\u2217. B. Computational Complexity This section studies the computational complexity of Algorithm 1.The graph construction in Line 1 of Algorithm 1 takes O(n2) complexity.Line 3 is accomplished by using the SVD, of which the complexity is O(min(d2c, dc2)).In Line 4, one should compute the \\u21132-norm of each row of a n \\u00d7 c matrix E, so the complexity is O(nc).Note that a d \\u00d7 d matrix should be inverted in Line 5, so the complexity of this step is O(d3).Therefore, the total complexity of our proposed algorithm is O(n2 +(min(d2c, dc2)+nc+d3)k) by assuming that Lines 2\\u20139 are iterated k times.Note that the complexity of Algorithm 1 is squared to the number of training examples n, so its complexity is acceptable.C. Generalization Bound In this section, we derive the generalization bound of LNSI.1) Preliminaries: Recall that our goal is to \\ufb01nd a suit- able project matrix Z by recovering the clean label matrix X Z, given the observed noisy label matrix Y and example features X. Similar to [10], (6) can be reformulated to the following expression with hard constraints, namely: min Z,E \\u0002 (i, j)\\u2208{1,...,n}\\u00d7{1,...,c} \\u2113((X Z + E)ij , Y ij ) s.t.\\u2225Z\\u2225\\u2217\\u2264Z\\u2217, \\u2225Z\\u22252 F \\u2264ZF, X Z \\u2208[\\u22121, 1]n\\u00d7c tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1. (21) Let \\u03b8 = (Z, E) be any feasible solution, and \\r = {(Z, E) | \\u2225Z\\u2225\\u2217 \\u2264 Z\\u2217, \\u2225Z\\u2225F \\u2264 \\u221aZF, X Z \\u2208 [\\u22121, 1]n\\u00d7c, tr((X Z)\\u22a4L(X Z)) \\u2264Ztr, \\u2225E\\u22252,1 \\u2264E2,1} be the feasible solution set.Also, let f\\u03b8(i, j) = Xi ZI j + Eij be the estimation function for Yij parameterized by \\u03b8 = (Z, E), and F\\r = { f\\u03b8 | \\u03b8 \\u2208\\r} be the set of feasible functions.I j is the jth column of identity matrix I \\u2208Rc\\u00d7c.We are interested in the following two \\u201c\\u2113-risk\\u201d quantities: 1) expected \\u2113-risk: R\\u2113( f ) = Ei, j [\\u2113( f (i, j), Yij )]; 2) empirical \\u2113-risk: \\u02c6R\\u2113( f ) = (1/nr) \\u0011 (i, j) \\u2113( f (i, j)Yij ), where nr is the number of observed entries.Thus, LNSI is to \\ufb01nd a proper \\u03b8\\u2217= (Z\\u2217, E\\u2217) that parameterizes f \\u2217= arg min f \\u2208F\\r \\u02c6R\\u2113( f ).2) Generalization Bound of LNSI: To bound the generaliza- tion error of LNSI, we \\ufb01rst link the quality of training labels to Rademacher complexity, which theoretically measures the complexity of a function class.We will show that high-quality labels of training examples will result in a lower model complexity and thus a smaller error bound.To begin with, we apply the following lemma to bound the expected \\u2113-risk.Lemma 3 (Bound on Expected \\u2113-Risk [39]): Let \\u2113be the loss function bounded by B with Lipschitz constant L\\u2113, and \\u03b4 be a constant where 0 < \\u03b4 < 1.With probability at least 1 \\u2212\\u03b4, we have max f \\u2208F |R\\u2113( f ) \\u2212\\u02c6R\\u2113( f )| \\u22642L\\u2113Rn(F) + B \\u0012 ln(1/\\u03b4) 2nr where Rn(F) := E[R(F)] is the Rademacher complexity of the function class F and R(F) := E\\u03c3 \\u0013 sup f \\u2208F 1 nr nr \\u0002 \\u03b1=1 \\u03c3\\u03b1 f (\\u03b1) \\u0014 is the empirical Rademacher complexity on the training examples.Note that \\u03c3\\u03b1 (\\u03b1 = 1, 2, . . . ,nr) are independent identically distributed (i.i.d.)Rademacher random variables.Given Lemma 3, we see that the key to derive the upper bound of a function f \\u2208F is to bound the complexity Rn(F\\r).More formally, the Rademacher complexity can be bounded in terms of the constraints in (21).Before diving into the details, we \\ufb01rst provide several useful theorems and lemmas.Lemma 4 (Complexity Bound [40]): Let S be a closed con- vex set and let F : S \\u2192R be \\u03b2-strongly convex with respect to \\u2225\\u00b7 \\u2225. In addition, we assume that F\\u22c6(O) = 0 with F\\u22c6being the Fenchel conjugate of function F. Further, let A = {A : \\u2225A\\u2225\\u22c6\\u2264A} and de\\ufb01ne W = {W \\u2208S : F(W) \\u2264Fmax}.Considering the class of linear functions F = {A \\u2192 W, A : W \\u2208W}, we have R(F) \\u2264A \\u0012 2Fmax \\u03b2nr (22) where W, A = tr(W\\u22a4A).Lemma 5 [41]: The function F : Rn\\u00d7c \\u2192R de\\ufb01ned as F(W) = (1/2)\\u2225W\\u22252 2,q for q = (ln(c)/(ln(c) \\u22121)) is (1/(3 ln(c)))-strongly convex with respect to \\u2225\\u00b7\\u22252,1 over Rn\\u00d7c.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1417,\n        \"min\": 1922,\n        \"max\": 7014,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          7014,\n          6134,\n          4805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 231,\n        \"min\": 253,\n        \"max\": 1001,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1001,\n          926,\n          905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 354.4185164950596,\n        \"min\": 480.5,\n        \"max\": 1753.5,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          1753.5,\n          1533.5,\n          1201.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"words\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"word_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "execution_count": 39
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:03:52.35972Z",
          "iopub.execute_input": "2024-10-17T18:03:52.360138Z",
          "iopub.status.idle": "2024-10-17T18:03:52.366654Z",
          "shell.execute_reply.started": "2024-10-17T18:03:52.360102Z",
          "shell.execute_reply": "2024-10-17T18:03:52.36555Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTOkh-nF6s5u",
        "outputId": "56fb77ef-79e5-402a-dee1-acc8cb9c448d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([17, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrival is done by following steps:\n",
        "1. Define a query string.\n",
        "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
        "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding to get similarity scores.\n",
        "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts."
      ],
      "metadata": {
        "id": "K-69VfIq6s5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T14:43:41.378698Z",
          "iopub.execute_input": "2024-10-17T14:43:41.379007Z",
          "iopub.status.idle": "2024-10-17T14:43:41.385054Z",
          "shell.execute_reply.started": "2024-10-17T14:43:41.37896Z",
          "shell.execute_reply": "2024-10-17T14:43:41.384039Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9VwK4TG6s5u",
        "outputId": "1ff88359-b81d-4485-c9d9-8e060e3d0328"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([17, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Similarity search"
      ],
      "metadata": {
        "id": "p_z_miQ16s5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "query = \"Tell me about the roles of E matrix in the whole optimization\"\n",
        "print(f\"Query : {query}\")\n",
        "\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
        "\n",
        "dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "\n",
        "top_results = torch.topk(dot_scores, k=17)\n",
        "top_results"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:12:26.405382Z",
          "iopub.execute_input": "2024-10-17T18:12:26.40629Z",
          "iopub.status.idle": "2024-10-17T18:12:26.458545Z",
          "shell.execute_reply.started": "2024-10-17T18:12:26.406247Z",
          "shell.execute_reply": "2024-10-17T18:12:26.45761Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDuaD3056s5u",
        "outputId": "f7c4ec30-7207-4a7d-f8a2-591edc290306"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query : Tell me about the roles of E matrix in the whole optimization\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([0.4298, 0.3959, 0.3726, 0.3018, 0.2987, 0.2934, 0.2934, 0.2571, 0.2393,\n",
              "        0.2341, 0.2130, 0.1988, 0.1617, 0.1565, 0.1109, 0.0991, 0.0902],\n",
              "       device='cuda:0'),\n",
              "indices=tensor([ 4,  5, 13, 16, 15,  7,  3, 11,  8,  2, 12,  1,  6,  0,  9, 10, 14],\n",
              "       device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings_tensor = torch.tensor(word_embeddings).to(\"cuda\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:12:27.202871Z",
          "iopub.execute_input": "2024-10-17T18:12:27.203272Z",
          "iopub.status.idle": "2024-10-17T18:12:27.209156Z",
          "shell.execute_reply.started": "2024-10-17T18:12:27.203235Z",
          "shell.execute_reply": "2024-10-17T18:12:27.20804Z"
        },
        "trusted": true,
        "id": "2rWR3-Db6s5u"
      },
      "outputs": [],
      "execution_count": 46
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import util\n",
        "\n",
        "query = \"Tell me about the roles of E matrix in the whole optimization\"\n",
        "print(f\"Query : {query}\")\n",
        "\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
        "\n",
        "dot_scores = util.dot_score(query_embedding, word_embeddings_tensor)[0]\n",
        "\n",
        "top_results1 = torch.topk(dot_scores, k=17)\n",
        "top_results1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:12:28.145163Z",
          "iopub.execute_input": "2024-10-17T18:12:28.145799Z",
          "iopub.status.idle": "2024-10-17T18:12:28.203741Z",
          "shell.execute_reply.started": "2024-10-17T18:12:28.145734Z",
          "shell.execute_reply": "2024-10-17T18:12:28.202661Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wtY7hiJ6s5u",
        "outputId": "51cf2789-372b-471c-9b15-ffe31ee4d3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query : Tell me about the roles of E matrix in the whole optimization\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.topk(\n",
              "values=tensor([0.4697, 0.3270, 0.2985, 0.2804, 0.2782, 0.2579, 0.2574, 0.2476, 0.2425,\n",
              "        0.2391, 0.2360, 0.2254, 0.2157, 0.2145, 0.2065, 0.2026, 0.1973],\n",
              "       device='cuda:0'),\n",
              "indices=tensor([209,  93,  76, 201,  75,  88,  95, 171, 221, 107,  50, 169, 750,  23,\n",
              "         78, 630, 219], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [
        "for score, idx in zip(top_results[0], top_results[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    print(\"Text\")\n",
        "    print(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:12:29.799996Z",
          "iopub.execute_input": "2024-10-17T18:12:29.800866Z",
          "iopub.status.idle": "2024-10-17T18:12:29.813276Z",
          "shell.execute_reply.started": "2024-10-17T18:12:29.800817Z",
          "shell.execute_reply": "2024-10-17T18:12:29.812005Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ztwhMx36s5u",
        "outputId": "70106ea3-509b-40f2-9e22-43a7d9239eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.4298\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 5 According to [36], the closed-form solution to (9) can be expressed as Z = Udiag(max{\u0002ii −τ, 0})V ⊤ ∀i =1, 2, . . . ,min(d, c) (10) where U and V are obtained by conducting the singular value decomposition (SVD) on ˆT (i.e., ˆT = U\u0002V ⊤), and \u0002ii is the ith diagonal element of the singular value matrix \u0002. Update E: By dropping the unrelated terms to E in (8), the subproblem of E is min E λ3∥E∥2,1+tr \u0003 M⊤ 1 (Y −B−E) \u0004 + μ 2 ∥Y −B−E∥2 F ⇒min E λ3∥E∥2,1 −tr\u0003M⊤ 1 E\u0004 + μ 2 tr(E⊤E −2(Y −B)⊤E) ⇒min E λ3∥E∥2,1 + μ 2 tr \b E⊤E −2 \u0005 Y −B + 1 μ M1 \u0006⊤ E \t ⇒min E λ3 μ ∥E∥2,1 + 1 2 \u0007\u0007\u0007\u0007E − \u0005 Y −B + M1 μ \u0006\u0007\u0007\u0007\u0007 2 F ⇒min E η∥E∥2,1 + 1 2∥E − M∥2 F (11) where M = Y −B + (M1/μ) and η = (λ3/μ).Herein, the closed-form solution to the general optimization problem related to ℓ2,1 norm is provided in the following lemma.Lemma 1 [27], [37]: Let ˜Q be a given matrix and ˜W is the variable to be optimized.If the optimal solution to min ˜W ˜α∥˜W∥2,1 + 1 2∥˜W −˜Q∥2 F (12) is ˜W ∗, then the ith row of ˜W ∗is ˜W ∗ i = ⎧ ⎪⎨ ⎪⎩ ∥˜Qi∥2 −˜α ∥˜Qi∥2 ˜Qi, if ∥˜Qi∥2 > ˜α 0, otherwise. (13) Obviously, the subproblem related to E has the same formulation with (13).Therefore, the closed-form solution to (11) is expressed as Ei = ⎧ ⎨ ⎩ ∥ Mi∥2 −η ∥ Mi∥2 Mi if ∥ Mi∥2 > η 0, otherwise (14) where Ei and Mi represent the ith row of the related matrices, respectively.Update J: The subproblem regarding J is min J λ2tr((X J)⊤L(X J)) + tr \u0003 M⊤ 2 (B −X J) \u0004 +tr\u0003M⊤ 3 (Z −J)\u0004 + μ 2 \u0003∥B −X J∥2 F + ∥Z −J∥2 F \u0004 ⇒min J tr \u000f λ2 J⊤(X⊤LX)J + μ 2 (J⊤(X⊤X)J + J⊤J) \u0010 −tr\u0003M⊤ 2 X J + M⊤ 3 J + μ(B⊤X J + Z⊤J)\u0004. (15) By computing the derivative of (15) with respect to J and then setting it to zero, J can be updated as J = (2λ2X⊤LX + μX⊤X + μI)−1 (X⊤M2 + M3 + μX⊤B + μZ) (16) where I denotes the identity matrix with proper size through- out this article.Update B: The subproblem on B with the continuous convex set B ∈[−1, 1]n×c is min B tr \u0003 M⊤ 1 (Y −B −E) \u0004 + tr \u0003 M⊤ 2 (B −X J) \u0004 + μ 2 (∥Y −B −E∥2 F + ∥B −X J∥2 F) s.t.B ∈[−1, 1]n×c. (17) By ﬁrst computing the derivative of (17) with respect to B and setting it to zero, the optimal B (i.e., ˆB) can be represented as ˆB = μ(Y −E + X J) + M1 −M2 2μ . (18) To restrict ˆB to the feasible region, we further project all its elements to [−1, 1] as Bij = \t( ˆBij ) (19) where the projection \t(x) is deﬁned as \t(x) = ⎧ ⎪⎨ ⎪⎩ 1, if x > 1 x, if x ∈[−1, 1] −1, if x < −1. (20) The entire optimization process for LNSI is summarized in Algorithm 1.V. THEORETICAL ANALYSES This section provides the theoretical analyses on LNSI.We ﬁrst prove that the optimization process explained in Section IV-B will converge to a stationary point and then analyze the computational complexity of Algorithm 1.Finally, we theoretically prove that the generalization risk of LNSI is upper bounded.A. Proof of Convergence In this section, we discuss the convergence property of the ADMM method in Algorithm 1.As discussed in [38], the convergence of ADMM has been proved when there are only two blocks of variables.However, (7) contains four vari- ables Z, E, J, and B, and thus, such a convergence property of ADMM is not theoretically guaranteed.By demonstrating that (7) is equivalent to the standard optimization problem with two variables, we show that the iterative solution provided in Algorithm 1 also enjoys the good property of convergence in Theorem 2.Theorem 2: Given the optimization problem (7), the itera- tive process of ADMM will converge to a stationary point.First, we provide the convergence conditions for general ADMM solver and then prove that the proposed algorithm satisﬁes the required conditions.Therefore, the iterative solu- tion in Algorithm 1 is guaranteed to converge to a stationary point.The detailed proof can be found in the Appendix.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.3959\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Algorithm 1 Algorithm for Solving LNSI Input: feature matrix X, observed label matrix Y; trade-off parameters: λ1, λ2, and λ3; Z = O, J = Z, E = O, B = O, M1 = O, M2 = O, M3 = O; μ = 10−3, μmax = 106, ρ = 1.2, ϵ = 10−6, iter_max = 1000; iter = 0; 1: Construct graph G and calculate the Laplacian matrix L via (3); 2: while not converge do 3: Update Z via (10), 4: Update E via (14), 5: Update J via (16), 6: Update B via (19), 7: Update the multipliers M1 := M1 + μ(Y −B −E), M2 := M2 + μ(B −X J), M3 := M3 + μ(Z −J), 8: Update the parameter μ by μ := min(ρμ, μmax), 9: iter := iter + 1, 10: Check the convergence conditions: ∥Y −B−E∥F ≤ϵ and ∥B−X J∥F ≤ϵ and ∥Z−J∥F ≤ ϵ; or iter > iter_max.11: end while Output: optimized Z∗and E∗. B. Computational Complexity This section studies the computational complexity of Algorithm 1.The graph construction in Line 1 of Algorithm 1 takes O(n2) complexity.Line 3 is accomplished by using the SVD, of which the complexity is O(min(d2c, dc2)).In Line 4, one should compute the ℓ2-norm of each row of a n × c matrix E, so the complexity is O(nc).Note that a d × d matrix should be inverted in Line 5, so the complexity of this step is O(d3).Therefore, the total complexity of our proposed algorithm is O(n2 +(min(d2c, dc2)+nc+d3)k) by assuming that Lines 2–9 are iterated k times.Note that the complexity of Algorithm 1 is squared to the number of training examples n, so its complexity is acceptable.C. Generalization Bound In this section, we derive the generalization bound of LNSI.1) Preliminaries: Recall that our goal is to ﬁnd a suit- able project matrix Z by recovering the clean label matrix X Z, given the observed noisy label matrix Y and example features X. Similar to [10], (6) can be reformulated to the following expression with hard constraints, namely: min Z,E \u0002 (i, j)∈{1,...,n}×{1,...,c} ℓ((X Z + E)ij , Y ij ) s.t.∥Z∥∗≤Z∗, ∥Z∥2 F ≤ZF, X Z ∈[−1, 1]n×c tr((X Z)⊤L(X Z)) ≤Ztr, ∥E∥2,1 ≤E2,1. (21) Let θ = (Z, E) be any feasible solution, and \r = {(Z, E) | ∥Z∥∗ ≤ Z∗, ∥Z∥F ≤ √ZF, X Z ∈ [−1, 1]n×c, tr((X Z)⊤L(X Z)) ≤Ztr, ∥E∥2,1 ≤E2,1} be the feasible solution set.Also, let fθ(i, j) = Xi ZI j + Eij be the estimation function for Yij parameterized by θ = (Z, E), and F\r = { fθ | θ ∈\r} be the set of feasible functions.I j is the jth column of identity matrix I ∈Rc×c.We are interested in the following two “ℓ-risk” quantities: 1) expected ℓ-risk: Rℓ( f ) = Ei, j [ℓ( f (i, j), Yij )]; 2) empirical ℓ-risk: ˆRℓ( f ) = (1/nr) \u0011 (i, j) ℓ( f (i, j)Yij ), where nr is the number of observed entries.Thus, LNSI is to ﬁnd a proper θ∗= (Z∗, E∗) that parameterizes f ∗= arg min f ∈F\r ˆRℓ( f ).2) Generalization Bound of LNSI: To bound the generaliza- tion error of LNSI, we ﬁrst link the quality of training labels to Rademacher complexity, which theoretically measures the complexity of a function class.We will show that high-quality labels of training examples will result in a lower model complexity and thus a smaller error bound.To begin with, we apply the following lemma to bound the expected ℓ-risk.Lemma 3 (Bound on Expected ℓ-Risk [39]): Let ℓbe the loss function bounded by B with Lipschitz constant Lℓ, and δ be a constant where 0 < δ < 1.With probability at least 1 −δ, we have max f ∈F |Rℓ( f ) −ˆRℓ( f )| ≤2LℓRn(F) + B \u0012 ln(1/δ) 2nr where Rn(F) := E[R(F)] is the Rademacher complexity of the function class F and R(F) := Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σα f (α) \u0014 is the empirical Rademacher complexity on the training examples.Note that σα (α = 1, 2, . . . ,nr) are independent identically distributed (i.i.d.)Rademacher random variables.Given Lemma 3, we see that the key to derive the upper bound of a function f ∈F is to bound the complexity Rn(F\r).More formally, the Rademacher complexity can be bounded in terms of the constraints in (21).Before diving into the details, we ﬁrst provide several useful theorems and lemmas.Lemma 4 (Complexity Bound [40]): Let S be a closed con- vex set and let F : S →R be β-strongly convex with respect to ∥· ∥. In addition, we assume that F⋆(O) = 0 with F⋆being the Fenchel conjugate of function F. Further, let A = {A : ∥A∥⋆≤A} and deﬁne W = {W ∈S : F(W) ≤Fmax}.Considering the class of linear functions F = {A → W, A : W ∈W}, we have R(F) ≤A \u0012 2Fmax βnr (22) where W, A = tr(W⊤A).Lemma 5 [41]: The function F : Rn×c →R deﬁned as F(W) = (1/2)∥W∥2 2,q for q = (ln(c)/(ln(c) −1)) is (1/(3 ln(c)))-strongly convex with respect to ∥·∥2,1 over Rn×c.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.3726\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS It can be easily veriﬁed that the above problem (47) can be represented in the form of (40) by setting P = \u0018B Z \u0019 , Q = ⎡ ⎣ E J K ⎤ ⎦ (49) and AP = ⎡ ⎢⎢⎣ I O I O I O O I ⎤ ⎥⎥⎦, BQ = ⎡ ⎢⎢⎣ I O O O −X O O O −I O −I O ⎤ ⎥⎥⎦, C = ⎡ ⎢⎢⎣ Y O O O ⎤ ⎥⎥⎦ (50) where I and O are the identity matrices and zero matrices with proper sizes, respectively.The functions f (P) and g( Q) in (40) can be, respectively, expressed as f (P) = ∥Z∥∗+ λ1∥Z∥2 F (51) g( Q) = λ2tr((X J)⊤L(X J)) + λ3∥E∥2,1 + IC(K). (52) The unaugmented Lagrangian is formulated as L0 = ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 + tr(M⊤ 1 (Y −B −E)) + tr\u0003M⊤ 2 (B −X J)\u0004 + tr \u0003 M⊤ 3 (Z −J) \u0004 . (53) Obviously, both f (P) and g( Q) are closed, proper, and convex, and the unaugmented Lagrangian L0 has a saddle point, which demonstrate that the optimization process for (7) is convergent.□ REFERENCES [1] R. J. Hickey, “Noise modelling and evaluating learning from examples,” Artif.Intell.,vol.82, nos.1–2, pp.157–179, 1996. [2] C. Gong, H. Zhang, J. Yang, and D. Tao, “Learning with inadequate and incorrect supervision,” in Proc.Int.Conf.Data Mining, Nov. 2017, pp.889–894. [3] W. Gao, L. Wang, Y.-F. Li, and Z.-H. Zhou, “Risk minimization in the presence of label noise,” in Proc.AAAI Conf.Artif.Intell.,2016, pp.1575–1581. [4] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learning with noisy labels,” in Proc.Adv.Neural Inf.Process.Syst.,2013, pp.1196–1204. [5] N. Manwani and P. Sastry, “Noise tolerance under risk minimization,” IEEE Trans.Cybern.,vol.43, no.3, pp.1146–1151, Jun. 2013. [6] G. Patrini, F. Nielsen, R. Nock, and M. Carioni, “Loss factorization, weakly supervised learning and label noise robustness,” in Proc.Int.Conf.Mach.Learn.,2016, pp.708–717. [7] R. Wang, T. Liu, and D. Tao, “Multiclass learning with partially corrupted labels,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.6, pp.2568–2580, Jun. 2018. [8] P. Zhao, Y. Jiang, and Z.-H. Zhou, “Multi-view matrix completion for clustering with side information,” in Proc.Paciﬁc–Asia Conf.Knowl.Discovery Data Mining, 2017, pp.403–415. [9] M. Xu, R. Jin, and Z. Zhou, “Speedup matrix completion with side information: Application to multi-label learning,” in Proc.Adv.Neural Inf.Process.Syst.,2013, pp.2301–2309. [10] K.-Y. Chiang, C.-J. Hsieh, and I. S. Dhillon, “Matrix completion with noisy side information,” in Proc.Adv.Neural Inf.Process.Syst.,2015, pp.3447–3455. [11] Y. Guo, “Convex co-embedding for matrix completion with predic- tive side information,” in Proc.31st AAAI Conf.Artif.Intell.,2017, pp.1955–1961. [12] K.-Y. Chiang, C.-J. Hsieh, and I. Dhillon, “Robust principal component analysis with side information,” in Proc.Int.Conf.Mach.Learn.,2016, pp.2291–2299. [13] F. Muhlenbach, S. Lallich, and D. A. Zighed, “Identifying and handling mislabelled instances,” J. Intell.Inf.Syst.,vol.22, no.1, pp.89–109, 2004. [14] X. Zhu, X. Wu, and Q. Chen, “Eliminating class noise in large datasets,” in Proc.Int.Conf.Mach.Learn.,2003, pp.920–927. [15] B. van Rooyen, A. K. Menon, and R. C. Williamson, “Learning with symmetric label noise: The importance of being unhinged,” in Proc.Adv.Neural Inf.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.3018\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 15 [39] P. L. Bartlett and S. Mendelson, “Rademacher and Gaussian complexities: Risk bounds and structural results,” J. Mach.Learn.Res.,vol.3, pp.463–482, Mar. 2003. [40] S. Kakade, S. Shalev-Shwartz, and A. Tewari. (2009).On the Duality of Strong Convexity and Strong Smoothness: Learn- ing Applications and Matrix Regularization. [Online].Available: http://ttic.uchicago.edu/shai/papers/KakadeShalevTewari09.pdf [41] S. M. Kakade, S. Shalev-Shwartz, and A. Tewari, “Regularization techniques for learning with matrices,” J. Mach.Learn.Res.,vol.13, pp.1865–1890, Jun. 2012. [42] P.-A. Absil and P. Van Dooren, “Two-sided Grassmann–Rayleigh quo- tient iteration,” Numerische Mathematik, vol.114, no.4, pp.549–571, 2010. [43] R. Mahony and P.-A. Absil, “The continuous-time Rayleigh quotient ﬂow on the sphere,” Linear Algebra Appl.,vol.368, pp.343–357, Jul. 2003. [44] H. Zhang and F. Ding, “On the kronecker products and their applica- tions,” J. Appl.Math.,vol.2013, Jun. 2013, Art.no.296185. [45] R. Meir and T. Zhang, “Generalization error bounds for Bayesian mix- ture algorithms,” J. Mach.Learn.Res.,vol.4, pp.839–860, Dec. 2003. [46] J. Yu, D. Tao, J. Li, and J. Cheng, “Semantic preserving distance metric learning and applications,” Inf.Sci.,vol.281, pp.674–686, Oct. 2014. [47] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of multipliers,” Found.Trends Mach.Learn.,vol.3, no.1, pp.1–122, Jan. 2011.Yang Wei received the B.S. degree from the School of Computer Science and Engineering, Nanjing Uni- versity of Science and Technology, Nanjing, China, in 2015, where she is currently pursuing the Ph.D. degree.Her current research interests include pattern recognition, incomplete data-based learning, and deep learning.Chen Gong (M’17) received the dual Ph.D. degree from Shanghai Jiao Tong University (SJTU), Shang- hai, China, and the University of Technology Sydney (UTS), Ultimo, NSW, Australia, in 2016.He is currently a Professor with the School of Computer Science and Engineering, Nanjing Uni- versity of Science and Technology, Nanjing, China.He has authored or coauthored more than 50 techni- cal articles at prominent journals and conferences such as the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), the IEEE TRANSACTIONS ON IMAGE PROCESSING (T-IP), the IEEE TRANS- ACTIONS ON CYBERNETICS (T-CYB), CVPR, AAAI, IJCAI, ICDM, and so on.His current research interests include machine learning and data mining.Dr. Gong was a recipient of the Excellent Doctoral Dissertation Award by SJTU and the Chinese Association for Artiﬁcial Intelligence (CAAI).He was also enrolled by the Summit of the Six Top Talents Program of Jiangsu Province, China.Shuo Chen received the B.S. degree from the School of Computer Science and Engineering, Jinling Insti- tute of Technology, Nanjing, China, in 2014.He is currently pursuing the Ph.D. degree with the Nanjing University of Science and Technology, Nanjing.His current research interests include pattern recognition, metric learning, and deep learning.Tongliang Liu (M’14) is currently a Lecturer with the School of Computer Science and the Fac- ulty of Engineering, and a Core Member with the UBTECH Sydney AI Centre, The University of Sydney, Darlington, NSW, Australia.He has authored and coauthored more than 60 research arti- cles, including the IEEE TRANSACTIONS ON PAT- TERN ANALYSIS AND MACHINE INTELLIGENCE (T-PAMI), the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), the IEEE TRANSACTIONS ON IMAGE PROCESSING (T-IP), ICML, AAAI, IJCAI, CVPR, ECCV, KDD, and ICME, with best paper awards.His current research interests include machine learning, computer vision, and data mining.Mr. Liu was a recipient of the 2019 ICME Best Paper Award and the Dis- covery Early Career Researcher Award (DECRA) from Australian Research Council (ARC).Jian Yang (M’08) received the Ph.D. degree in pattern recognition and intelligence systems from the Nanjing University of Science and Technology (NUST), Nanjing, China, in 2002.In 2003, he was a Post-Doctoral Researcher with the University of Zaragoza, Zaragoza, Spain.From 2004 to 2006, he was a Post-Doctoral Fellow with the Biometrics Centre, The Hong Kong Polytechnic University, Hong Kong.From 2006 to 2007, he was a Post-Doctoral Fellow with the Department of Com- puter Science, New Jersey Institute of Technology, Newark, NJ, USA.He is currently a Chang-Jiang Professor with the School of Computer Science and Technology, NUST.He has authored more than 200 scientiﬁc articles in pattern recognition and computer vision.His articles have been cited more than 5000 times in the Web of Science and 13000 times in the Scholar Google.His current research interests include pattern recogni- tion, computer vision, and machine learning.Dr. Yang is a fellow of IAPR.He is/was currently an Associate Editor of Pattern Recognition, Pattern Recognition Letters, the IEEE TRANSAC- TIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), and Neurocomputing.Dacheng Tao (F’15) is a Professor of computer sci- ence and an ARC Laureate Fellow with the School of Computer Science and the Faculty of Engineering, and the Inaugural Director of the UBTECH Syd- ney Artiﬁcial Intelligence Centre, The University of Sydney, Darlington, NSW, Australia.His research results in artiﬁcial intelligence have expounded in one monograph.He has authored or coauthored more than 200 publications at prestigious journals and prominent conferences, such as the IEEE TRANS- ACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (T-PAMI), the International Journal of Computer Vision (IJCV), the Journal of Machine Learning Research (JMLR), AAAI, IJCAI, NIPS, ICML, CVPR, ICCV, ECCV, ICDM, and KDD, with several best paper awards.Prof. Tao is a fellow of the Australian Academy of Science.He was a recipient of the 2018 IEEE ICDM Research Contributions Award and the 2015 Australian Scopus-Eureka Prize.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2987\n",
            "Text\n",
            "10, pp.2261–2274, Oct. 2015. [29] Y. Huang, D. Xu, and F. Nie, “Semi-supervised dimension reduction using trace ratio criterion,” IEEE Trans.Neural Netw.Learn.Syst.,vol.23, no.3, pp.519–526, Mar. 2012. [30] Y. Wang, Y. Jiang, Y. Wu, and Z.-H. Zhou, “Spectral clustering on multiple manifolds,” IEEE Trans.Neural Netw.,vol.22, no.7, pp.1149–1161, Jul. 2011. [31] J. Yu, C. Hong, Y. Rui, and D. Tao, “Multitask autoencoder model for recovering human poses,” IEEE Trans.Ind. Electron.,vol.65, no.6, pp.5060–5068, Jun. 2018. [32] J. Yu, X. Yang, F. Gao, and D. Tao, “Deep multimodal distance metric learning using click constraints for image ranking,” IEEE Trans.Cybern.,vol.47, no.12, pp.4014–4024, Dec. 2017. [33] C. Gong, D. Tao, W. Liu, L. Liu, and J. Yang, “Label propagation via teaching-to-learn and learning-to-teach,” IEEE Trans.Neural Netw.Learn.Syst.,vol.28, no.6, pp.1452–1465, Jun. 2017. [34] C.-J. Hsieh, N. Natarajan, and I. S. Dhillon, “PU learning for matrix completion,” in Proc.32nd Int.Conf.Mach.Learn.,2015, pp.2445–2453. [35] N. Komodakis and G. Tziritas, “Approximate labeling via graph cuts based on linear programming,” IEEE Trans.Pattern Anal.Mach.Intell.,vol.29, no.8, pp.1436–1453, Aug. 2007. [36] Z. Lin, M. Chen, and Y. Ma, “The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices,” 2010, arXiv:1009.5055. [Online].Available: https://arxiv.org/abs/1009.5055 [37] C. Gong, “Exploring commonality and individuality for multi-modal curriculum learning,” in Proc.AAAI Conf.Artif.Intell.,2017, pp.1926–1933. [38] C. Chen, B. He, Y. Ye, and X. Yuan, “The direct extension of ADMM for multi-block convex minimization problems is not necessarily con- vergent,” Math.Program.,vol.155, nos.1–2, pp.57–79, 2016.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2934\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Since Z and E are independent variables, the Rademacher complexity can be written as R(F\r) = Eσ \u0013 sup Z∈\rZ 1 nc nc \u0002 α=1 σα Xiα ZI jα \u0014 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σα Eiα, jα \u0014 = Eσ \u0013 sup Z∈\rZ 1 nc nc \u0002 α=1 σαtr \u0003 Z⊤X⊤ iα I jα⊤\u0004 \u0014 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σαtr(E⊤Iiα I jα⊤) \u0014 (35) where Ii is the ith column of identity matrix I ∈Rn×n, \rZ = {Z | ∥Z∥∗ ≤ Z∗, ∥Z∥F ≤ √ZF, X Z ∈ [−1, 1]n×c, tr((X Z)⊤L(X Z)) ≤Ztr}, and \rE = {E | ∥E∥2,1 ≤E2,1}.Since the last three constraints in \rZ can be rewritten as a much simpler formulation (32) according to Lemma 10, therefore, we have \rZ1 = {Z | ∥Z∥∗≤Z∗, ∥Z∥F ≤Z}.From the deﬁnition of Rademacher complexity, we know that it measures the richness of a class of real-valued functions with respect to a probability distribution.Hence, the tighter the constraints of functions are, the smaller the Rademacher complexity will be.By using the Rademacher contraction principle [45], (35) can be transformed as R(F\r) ≤Eσ \u0013 sup Z∈\rZ1 1 nc nc \u0002 α=1 σαtr \u0003 Z⊤X⊤ iα I jα⊤\u0004 \u0014 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σαtr(E⊤Iiα I jα⊤) \u0014 ≤min \u0015 Eσ \u0013 sup Z∈\rZ2 1 nc nc \u0002 α=1 σαtr \u0003 Z⊤X⊤ iα I jα⊤\u0004 \u0014 Eσ \u0013 sup Z∈\rZ3 1 nc nc \u0002 α=1 σαtr\u0003Z⊤X⊤ iα I jα⊤\u0004 \u0014\u0017 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σαtr(E⊤Iiα I jα⊤) \u0014 (36) where \rZ2 = {Z | ∥Z∥∗≤Z∗} and \rZ3 = {Z | ∥Z∥F ≤Z}.Taking Corollaries 7 and 8 and Lemma 9 into consideration, (36) leads to R(F\r) ≤E2,1∥Ii I j⊤∥2,∞ \u0016 3 ln(c) nc + min \u0015 Z∗max i, j \u0007\u0007X⊤ i I j⊤\u0007\u0007 2 \u0016 ln(2dc) nc Z max i, j \u0007\u0007X⊤ i I j⊤\u0007\u0007 F \u0016 2 nc \u0017 . (37) Since maxi, j ∥X⊤ i I j⊤∥2 = maxi ∥Xi∥2 max j ∥I j∥2 = maxi ∥Xi∥2, maxi, j ∥X⊤ i I j⊤∥F = maxi ∥Xi∥F and maxi, j ∥Ii I j⊤∥2,∞ = 1, we arrive at the upper bound of Rn(F\r), which is Rn(F\r) ≤min \u0015 Z∗X2 \u0016 ln(2dc) nc , ZXF \u0016 2 nc \u0017 + E2,1 \u0016 3 ln(c) nc . (38) □ Based on Theorem 11 and Lemma 3, the following inequal- ity holds: max f ∈F |Rℓ( f ) −ˆRℓ( f )| ≤2Lℓmin \u0015 Z∗X2 \u0016 ln(2dc) nc , ZXF \u0016 2 nc \u0017 + 2LℓE2,1 \u0016 3 ln(c) nc + B \u0016 ln(1/δ) 2nc (39) and thus the expected loss is upper bounded.As mentioned before, the matrix E is utilized to capture the label noise and the ℓ2,1 norm on it is upper bounded by E2,1.Specif- ically, we observe that E2,1 is governed by the label noise rate.A small noise rate will lead to a small E2,1, which further reduces the upper bound of the expected ℓ-risk in the right-hand side of (39).VI.EXPERIMENTS In this section, we ﬁrst use a toy data set to validate the motivation of LNSI (Section VI-A) and then com- pare LNSI with several representative baselines on various UCI benchmark data sets (Section VI-B).Next, we com- pare LNSI with these baseline algorithms on four practical data sets including ISOLET, COIL20, MNIST, and CIFAR-10 (Sections VI-C–VI-F).Also, the convergence process of the ADMM adopted in the Algorithm 1 is illustrated (Section VI-G).Afterward, we conduct the ablation study to verify the effectiveness of the critical regularizers of LNSI (Section VI-H) and also study the parametric sensitivity of LNSI (Section VI-I).Finally, we summarize the experimental results and give some insightful analyses (Section VI-J).Our LNSI is compared with ﬁve representative methods for noisy label handling, including labeled instance centroid smoothing (LICS) [3], unbiased logistic estimator (ULE) [4], μ stochastic gradient descent (μSGD) [6], learning with symmetric label noise (LSLN) [15], and coteaching [18].It is worth noting that the ﬁrst four baseline methods are only suitable for binary classiﬁcation, so we use the one-versus-the- rest strategy to make them applicable to multi-class situations.In addition, the prior knowledge such as the noise rate for all the compared methods is provided accurately.Note that coteaching can only handle images, so it is not compared on the nonimage data sets including ﬁve UCI data sets and ISOLET data set.A. Algorithm Validation First, we demonstrate the effectiveness of LNSI on a toy data set.As shown in Fig.2(a), we manually generate 8 examples in a 2-D space, which include 3 negative examples Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2934\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in which ∥Z∥∗with nuclear norm is employed to achieve low-rank effect, ∥E∥2,1 with ℓ2,1 norm is utilized to realize row sparsity, and λ1 and λ3 are the nonnegative tradeoff parameters.By solving (2), we can obtain the optimal Z∗, which can be used to compute the label y ∈Rc of a test example x ∈Rd as y = Z∗⊤x.Then x is classiﬁed into the jth class if j = arg max j′=1,2,...,c y j′ with y j′ being the j′th element in the label vector y. It is worth pointing out that although the formulation of (2) is similar to the low-rank representation (LRR) proposed in [27], their usages and implications are quite different.First, LRR is developed for subspace clustering while our method is for label noise removal.Second, LRR aims to select sparse atoms from a predeﬁned dictionary to reconstruct a clean space, while our method tries to learn a proper mapping Z from feature space to label space in presence of the label noise encoded by E. Therefore, these two models are different although they look similar at ﬁrst glance.To sufﬁciently exploit the side information, we further use the Laplacian regularizer based on graph embedding.Graph Laplacian has been widely utilized in several machine learning tasks such as semisupervised learning [28], [29], spectral clustering [30], multi-task learning [31], and metric learning [32].However, to the best of our knowledge, it has not been used to deal with side information.Let G = {V, E} be an undirected weighted graph with vertex set V consist- ing of all n examples and E is the edge set encoding the similarity between these examples.The symmetric adjacency matrix ˆW ∈Rn×n is utilized to quantify the graph G, where ˆWij = exp(−(∥Xi −X j∥2/2σ 2 k )) [33] (σk is the kernel width) measures the similarity between examples Xi and X j (i, j = 1, 2, . . . ,n).The diagonal matrix D and the Laplacian matrix L of the graph G are, respectively, deﬁned as Dii = \u0002 j ˆWij L = D −ˆW. (3) Ideally, we hope that the similar examples revealed by G obtain similar clean labels, and the labels of dissimilar examples can be quite different.Therefore, we have the Laplacian regularizer that is derived as \u0002 i \u0002 j ˆWij ∥Xi Z −X j Z∥2 2 = tr((X Z)⊤L(X Z)) (4) in which “tr(·)” computes the trace of the corresponding matrix.By combining (2) and (4), the proposed LNSI model is ﬁnally formulated as min Z,E ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X Z)⊤L(X Z))+λ3∥E∥2,1 s.t.Y = X Z + E, X Z ∈{−1, 1}n×c (5) in which λ1, λ2, and λ3 are the nonnegative tradeoff parame- ters.We note that (5) falls into an integer programming problem, which is generally NP-hard.To make problem (5) tractable, we relax the discrete constraint X Z ∈{−1, 1}n×c to a contin- uous convex set X Z ∈[−1, 1]n×c.It is a linear programming relaxation, which has been used in several prior works [34], [35].By doing so, we pursue to solve a simpler problem min Z,E ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X Z)⊤L(X Z))+λ3∥E∥2,1 s.t.Y = X Z + E, X Z ∈[−1, 1]n×c. (6) B. Optimization Directly solving the problem (6) is difﬁcult due to the exis- tence of coupled variables, which will make its optimization not have a closed-form solution.Consequently, we introduce two auxiliary variables J and B, and then, the problem (6) is converted to the following equivalent version: min Z,E,J,B ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 s.t.Y = B + E, B = X J, Z = J, B ∈[−1, 1]n×c. (7) The optimization problem (7) is convex and many off-the-shelf methods can be adopted to solve it.For efﬁciency, here we use the alternating direction method of multipliers (ADMMs), which alternatively optimizes the related variables in an iterative manner.The augmented Lagrangian function of (7) with the continuous convex constraint can be written as L (Z, E, B, J, M1, M2, M3) = ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 +tr \u0003 M⊤ 1 (Y −B −E) \u0004 +tr \u0003 M⊤ 2 (B −X J) \u0004 +tr \u0003 M⊤ 3 (Z −J) \u0004 + μ 2 (∥Y −B −E∥2 F +∥B −X J∥2 F + ∥Z −J∥2 F) (8) where M1, M2, and M3 are the Lagrangian multipliers, and μ > 0 is the penalty coefﬁcient.We can sequentially minimize each of the variables Z, E, B, and J by ﬁxing the others in every iteration.Update Z: The subproblem related to Z is min Z ∥Z∥∗+ λ1∥Z∥2 F + tr\u0003M⊤ 3 (Z −J)\u0004 + μ 2 ∥Z −J∥2 F ⇒min Z ∥Z∥∗+ λ1∥Z∥2 F + tr \u0003 M⊤ 3 Z −M⊤ 3 J \u0004 + μ 2 tr((Z⊤−J⊤)(Z −J)) ⇒min Z ∥Z∥∗+ λ1tr(Z⊤Z) + tr \u0003 M⊤ 3 Z \u0004 + μ 2 tr(Z⊤Z −2J⊤Z) ⇒min Z ∥Z∥∗+ 2λ1 + μ 2 × tr \u0005 Z⊤Z − 2 2λ1 + μ(μJ −M3)⊤Z \u0006 ⇒min Z 1 2λ1 + μ∥Z∥∗+ 1 2 \u0007\u0007\u0007\u0007Z − 1 μ + 2λ1 (μJ −M3) \u0007\u0007\u0007\u0007 2 F ⇒min Z τ∥Z∥∗+ 1 2∥Z −ˆT∥2 F (9) where ˆT = (1/(μ + 2λ1))(μJ −M3) and τ = (1/(2λ1 + μ)).Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2571\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.4.Illustration of convergence process of the ADMM method adopted by LNSI on the four practical data sets.For each data set, we present the convergence curves under different convergence criteria in the Algorithm 1. (a)–(c) ISOLET data set. (d)–(f) COIL20 data set. (g)–(i) MNIST data set. (j)–(l) CIFAR-10 data set.tr((X Z)⊤L(X Z)).To this end, we study the performances of three different settings on the above four practical data sets (such as ISOLET, COIL20, MNIST, and CIFAR-10).First, both low-rank regularizer and Laplacian regularizer are reserved to constitute the original model (abbreviated as “LNSI”); second, the Laplacian regularizer is removed from the original model to see how this term inﬂuences the model performance (abbreviated as “No Laplacian”); third, we remove the low-rank regularizer while keeping the Lapla- cian regularizer to observe the effect of low-rank regularizer (abbreviated as “No Low-Rank”).The experimental results of these three models are illus- trated in Fig.5, in which 40% and 60% of training examples have incorrect labels.The results reveal that LNSI achieves the best performance on all four data sets, especially when the label noise rate is relatively high.By contrast, the accuracy of LNSI will drop without any of the two terms such as low-rank regularizer and Laplacian regularizer, and therefore, these two regularization terms are essential to boost the performance of LNSI.I. Parametric Sensitivity Note that the objective function (8) in LNSI contains three tradeoff parameters λ1, λ2, and λ3 that should be manually Fig.5.Results of ablation study on four practical data sets.For convenience, the original model is denoted as “LNSI,” the setting without the Laplacian regularization term is dubbed as “No Laplacian,” and the setting without the low-rank term is named as “No Low-Rank.” (a) ISOLET data set. (b) COIL20 Data set. (c) MNIST data set. (d) CIFAR-10 data set.tuned.Therefore, in this section, we discuss whether the choices of them will signiﬁcantly inﬂuence the performance of LNSI.To this end, we examine the classiﬁcation accuracy at two different levels (20% and 60%) of label noise via changing one of λ1, λ2, and λ3, and meanwhile ﬁxing the others to the optimal constant values under different data sets and different noise rates.The above-mentioned four practical data sets are adopted here, and the results are shown in Fig.6.Fig.6(a)–(c) shows the experiments on the ISOLET data set, (d)–(f) shows the experiments on the COIL20 data set, (g)–(i) shows the experiments on the MNIST data set, and (j)–(l) shows the experiments on the CIFAR-10 data set.The results reveal that LNSI is robust to the variations of λ1 and λ2 in a wide range, so they can be easily tuned for practical use.Meanwhile, the performance of LNSI varies when λ3 changes in a wide range, as λ3 controls the capability of our method for capturing the label noise via the error matrix E. Speciﬁcally, if λ3 is large, the label noise will be greatly ignored, leading to the performance degradation of LNSI on COIL20 and MNIST when the noise rate is 60%.J. Summary of Experiments Based on the above-mentioned experimental results of Sections VI-B–VI-I, we observe that: 1) LNSI performs bet- ter than other baseline algorithms in most cases, both on benchmark data sets and practical data sets; 2) the proposed algorithm in Algorithm 1 converges quickly to a stationary point; 3) LNSI is robust to the variation of the two tradeoff parameters including λ1 and λ2; 4) when the label noise is serious, the performance of LNSI might drop when λ3 is set to a large value; 5) the introduced low-rank regularizer and Laplacian regularizer are both beneﬁcial to improve the classiﬁcation performance; and 6) the proposed LNSI outper- forms other compared baselines when 40% and 60% labels Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2393\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 9 Fig.2.Algorithm validation on the synthetic data set. (a) Initial state with labeled negative examples (in red) and labeled positive examples (in blue).Note that two examples are labeled incorrectly and they form the label noise. (b) Error matrix E, in which the nonzero rows capture the mislabeled examples successfully.TABLE I STATISTICS OF THE BENCHMARK DATA SETS (Nos.1–3) and 5 positive examples (Nos.4–8).The negative examples are denoted by red dots, while the positive examples are represented by blue dots.Note that the 2nd example is mistakenly labeled as positive and the 6th example is erroneously labeled as negative, so they form the label noise in this data set.As explained in Section I, the observed label matrix Y is decomposed as two parts, one is the groundtruth label matrix T = X Z and the other is the error capturing matrix E. After applying LNSI to this synthetic data set, we investigate whether the matrix E can accurately detect the mislabeled examples.In Fig.2(b), we see that all rows in E are zeros except the 2nd and 6th rows that exactly correspond to the examples with label noise.This implies that the matrix E is able to capture the label errors successfully and also indicates that a suitable projection matrix Z has been learned.B. Experiments on Benchmark Data Set In this section, we compare LNSI with LICS, ULE, μSGD, and LSLN on ﬁve UCI benchmark data sets1 including CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4.The information about the data sets is summarized in Table I. For each data set, all the algorithms are tested at different levels (0%, 20%, 40%, and 60%) of label noise on training sets.Given the noise rate ζ, we manually inject the label noise by randomly picking up ζ × n training examples and then switching the correct label of each of them to a random wrong label, in which n is the number of training examples.All the reported accuracies are the mean values of the outputs of ﬁve independent runs.Generally, the number of the nearest neighbors in the graph is suggested to set to a small value, as it has been widely 1https://archive.ics.uci.edu/ml/data sets.html observed that a sparse graph can usually lead to good per- formance.The parameter σk controls the connective strength of pairwise examples, and it is chosen below 1 as all features have been normalized.The number of the nearest neighbors in the graph was selected by searching the grid {5, 10, 15, 20}.Similarly, the kernel width σk was also turned by searching the grid {0.01, 0.1, 0.5, 1}.We established the 5-NN graph for LNSI on Wine and Breast Tissue and the 10-NN graph on CNAE9, Pendigits and Connect-4.The kernel width σk on Connect-4 was 1 and on the other four data sets was chosen as 0.1.The tradeoff parameters λ1, λ2, and λ3 were selected by searching the grid {10−4, 10−3, . . . ,105} on Wine and Breast Tissue, and the grid {10−2, 10−1, . . . ,103} on CNAE9, Pendigits, and Connect-4.The classiﬁcation accuracies of all the compared methods are shown in Fig.3.Note that LICS and LSLN are not compared on the Connect-4 data set as they are not scalable to this data set.From Fig.3, we observe that LNSI yields better perfor- mance than other baselines in most cases.An exceptional case is that ULE and LSLN obtain the best results on Breast Tissue and Pendigits under the noise level 0%, respectively.Besides, we note that ULE performs satisfactorily on Wine under the noise level 60%.However, LNSI is generally the most robust method compared with all the other baselines.Therefore, formulating the classiﬁcation task under label noise as a matrix recovery problem does help to improve the robustness of the trained classiﬁer.C. Experiments on ISOLET Data Set To demonstrate the superiority of LNSI in dealing with different kinds of practical problems, we ﬁrst use the ISOLET2 data set to test the ability of all compared methods on the speech recognition task.A subset of ISOLET is established for our experiments, which contains 30 speakers speaking the name of each letter of the alphabet (i.e., “A”–“Z”) twice.As a result, we have totally 30 × 26 × 2 = 1560 examples, and each example is encoded as a 617-dimensional feature vector.Our task is to identify which of the 26 letters each example belongs to.Among these examples, we randomly pick up 20% of examples to establish the test set, and the remaining 80% of examples form the training set.To incorporate different levels of label noise, we randomly select 0%, 20%, 40%, and 60% of examples in the training set and then switch their accurate labels to the wrong values.Such example selection and label corruption are conducted ﬁve times, so every compared algo- rithm should independently run ﬁve times on the contaminated data set and the average accuracy on the test set over these ﬁve different runs is particularly investigated.The standard deviation over the ﬁve runs is also reported to display the stability of each compared algorithm.In LNSI, a 10-NN graph with kernel width σk = 0.1 was established.The tradeoff parameters in (6) such as λ1, λ2 and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,103}.From Table II, we see that the performances of LNSI are signiﬁcantly better than baseline methods in almost all cases.2http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2341\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 3 Other representative works targeting label noise include [20] and [21].B. Side Information Side information serves as the additional information for accomplishing a certain task, which has been shown to be useful in solving many related problems.Practically, side information may have diverse formations.Some works on the recommender system treat the user features and product features as side information so that the quality of completion of user-product preference matrix can be improved.For instance, in order to recover the unknown user scores of the preference matrix, Chiang et al. [10] incorporate the user and product side information to “describe” the row entities and column entities of a matrix, respectively.Similarly, Guo [11] develops a coembedding framework for matrix com- pletion with side information, which learns a feature mapping as well as a label embedding.Zhao and Guo [22] propose a joint discriminative prediction model for personalized top-N recommendations with side information.Other works take the available relationship between exam- ples as side information.For example, Aggarwal et al. [23] perform text clustering by utilizing the links in the docu- ment, user-access behavior from web logs, or other nontextual attributes.Zhao et al. [8] conduct clustering with the aid of must-links and cannot-links between examples in addition to the regular input features.Zhang et al. [24] tackle the semi- supervised classiﬁcation by harnessing the example pairwise constraints as side information.Ahn et al. [25] study the binary rating estimation problem to quantify the value of graph side information, such as social graphs.In addition, Xue et al. [26] assimilate side information of the same format as observation in the robust principal component analysis.From the above-mentioned analyses, we see that side infor- mation has been widely used in matrix completion, clustering, and semisupervised learning.However, it has not been well deployed in label noise handling that is the main target of this article.III.PRELIMINARIES The utilization of side information for matrix completion has been studied by Chiang et al. [10], in which the model and theoretical analyses are studied when the side information contains noise.Suppose ˆR ∈Rn1×n2 is a matrix with rank r that should be recovered, and ˆX ∈Rn1×d1 and ˆY ∈Rn2×d2 (d1 and d2 denote feature dimensionality), respectively, record the row feature and column feature of ˆR. The matrices ˆX and ˆY serve as the side information for recovering ˆR. The model proposed by Chiang et al. [10] to recover ˆR is formulated as min M,N \u0002 (i, j)∈\u0002 ℓ(( ˆX M ˆY ⊤+N)ij , ˆRij )+λM∥M∥∗+λN∥N∥∗ (1) where “∥· ∥∗” is the nuclear norm, and “\u0002” is the index set containing all observed entries of ˆR. The observed ˆR can be decomposed into two parts: one is the low-rank matrix estimated from feature space ˆX M ˆY ⊤, in which M ∈Rd1×d2 is a coembedding matrix from features ˆX and ˆY, and the other part is N ∈Rn1×n2, which is used to capture the information that noisy features ( ˆX and ˆY) fail to describe.The loss function “ℓ(·)” requires that the recovered matrix is consistent with ˆR on the observed entries.Naturally, both ˆX M ˆY ⊤and N are preferred to be low-rank since they are aggregated to estimate a low-rank matrix ˆR, which further leads M to be a low-rank matrix.λN and λM are the tradeoff parameters.The underlying matrix ˆR can be estimated by ˆX M∗ˆY ⊤+ N∗, where M∗and N∗are the optimizers of (1).The effectiveness of this model for recovering the imperfect matrix has been theoretically and empirically veriﬁed.Inspired by this work, we treat the noisy label removal problem as a label matrix recovery problem, and the example features are regarded as the descriptions of the row enti- ties.Therefore, our LNSI inherits the good property of the model (1), and thus, the good performance is guaranteed.IV.OUR PROPOSED LNSI In this section, we ﬁrst describe our proposed model in Section IV-A and then provide the optimization algorithm in Section IV-B. A. Model We now introduce LNSI on label noise handling from the new viewpoint of side information.Let Y ∈Rn×c be the observed label matrix with corrupted entries, where n is the number of examples and c is the number of classes.X ∈Rn×d (d denotes the feature dimensionality) is the feature matrix, where the Xi representing the ith row of X records the feature of the ith example.The element Yij = 1 if the ith example has the label j ( j = 1, 2, . . . ,c), and Y ij = −1 otherwise.As explained in Section I, the features in X are taken as the side information for recovering the correct label matrix in this article.Speciﬁcally, we propose to decompose the observed Y into two parts, where the ﬁrst one is the low-rank groundtruth label matrix X Z estimated by the projection Z on the feature matrix X, and the second part is E ∈Rn×c describing the difference between the observed labels and accurate labels.Here Z ∈Rd×c is the projection matrix, and E can be used to capture the noisy labels.Since the “clean” label matrix X Z is low-rank, the projection matrix Z should be low-rank as well.This is because only a small fraction of columns in X are sufﬁcient to construct the low-rank space X Z. As X Z is the low-rank groundtruth label matrix, each entry in it should be −1 or 1.The label errors can be captured by a row-sparse matrix E since the label noise in training examples is usually sparse, and thus, the number of corresponding nonzero rows should be small.In addition, the Frobenius norm is imposed to Z to avoid overﬁtting.By putting all the above together, we consider solving the following problem: min Z,E ∥Z∥∗+ λ1∥Z∥2 F + λ3∥E∥2,1 s.t.Y = X Z + E, X Z ∈{−1, 1}n×c (2) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2130\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 13 Fig.6.Parametric sensitivity of LNSI. (a)–(c), (d)–(f), (g)–(i), and (j)–(l) ISOLET, COIL20, MNIST, and CIFAR-10 data sets, respectively. (a), (d), (g), and (j) Variation in accuracy with respect to the tradeoff parameter λ1 when λ2 and λ3 are ﬁxed to the values indicated in the legend. (b), (e), (h), and (k) Inﬂuence of λ2 to the classiﬁcation performance with other parameters ﬁxed. (c), (f), (i), and (l) Effect of λ3 while other two tradeoff parameters are ﬁxed.of training examples are incorrect because the error matrix E can capture the label errors successfully and the Laplacian regularizer is essential to boost the performance of LNSI.VII.CONCLUSION To solve the label inaccuracy that often occurs in plenty of real-world data sets for classiﬁcation, this article provides a novel paradigm that formulates the noisy label removal problem as a matrix recovery problem and treats the example features as the side information to aid the recovery process.Our proposed LNSI seamlessly forms the label noise removal and classiﬁer parameter optimization into a uniﬁed framework.The convergence property, computational complexity, and gen- eralization bound are also theoretically analyzed.We tested LNSI on various benchmark and practical data sets under different levels of label noise and found that LNSI outperforms other compared baseline methods and achieves robust results to different levels of label noise.APPENDIX PROOF OF THE CONVERGENCE To begin with, we provide the following two lemmas for the general ADMM solver.Lemma 12 [47]: Given the optimization problem with lin- ear constraints as min P, Q f (P) + g( Q) s.t.AP P + BQ Q = C (40) where AP and BQ are the coefﬁcient matrices, and f (P), g( Q) are two functions with respect to the variables P, Q, respectively.C is a constant.Then, the Lagrangian function of (40) is Lμ = f (P) + g( Q) + tr(M′⊤(AP P + BQ Q −C)) + μ 2 ∥AP P + BQ Q −C∥2 F (41) where M′ is the Lagrangian multiplier and μ > 0 is the penalty coefﬁcient.ADMM consists of the iterations Pk+1 := arg min P Lμ(P, Qk, M′k) (42) Qk+1 := arg min Q Lμ(Pk+1, Q, M′k) (43) M′k+1 := M′k + μ(AP Pk+1 + BQ Qk+1 −C). (44) If f (P) and g( Q) are convex, proper, and closed functions, and the unaugmented Lagrangian L0 = f (P) + g( Q) + tr(M′⊤(AP P + BQ Q −C)) has a saddle point, the sequences {Pk, Qk, M′k} generated by the ADMM algorithm are guar- anteed to converge.Lemma 13 [47]: The generic-constrained convex optimiza- tion problem regarding the variable P is min P f (P) s.t.P ∈C (45) where f (·) and C are convex.Problem (45) can then be rewritten in the form of ADMM as min f (P) + g(Zc) s.t.P −Zc = O (46) where g is the indicator function of C. Now we begin to formally verify the convergence of Algorithm 1.The proof of Theorem 2 is presented as follows.Proof: To facilitate the proof, we rewrite the optimization problem (7) in the formation of (40) according to Lemma 12.First, the optimization problem (7) with the convex set constraint can be transformed to the formation of (46) in Lemma 13.Therefore, problem (7) is equivalent to min Z,E ∥Z ∥∗+ λ1∥Z∥2 F + λ2tr((X J)⊤L(X J)) + λ3∥E∥2,1 + IC(K) s.t.Y = B + E, B = X J, J = Z, K = B (47) where IC(x) = \u0015 x, if x ∈C ∞, othrewise. (48) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1988\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z∗and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning ﬁelds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and ﬁrst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speciﬁcally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]–[11].Therefore, our proposed method is named as “label noise handling via side information” (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z∗on the example features (i.e., T = X Z∗), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classiﬁcation, which does not need the one-versus-one or one-versus-the-rest operations; 2) sufﬁcient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classiﬁer parameter optimization into a uniﬁed framework.Due to the above merits, a reliable classiﬁer can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classiﬁer.II.RELATED WORK This section brieﬂy reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches ﬁrst detect and eliminate label noise and then conduct the standard supervised classiﬁcation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble ﬁl- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the inﬂuence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classiﬁcation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speciﬁc tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1617\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 7 Lemma 6 [40]: The function F : Rd×c →R deﬁned as F(W) = (1/2)∥W∥2 2,2 is (1/2)-strongly convex with respect to ∥· ∥2,2 over Rd×c, where ∥· ∥2,2 := ∥· ∥F. By combining Lemmas 5 and 6 with the bound given in Lemma 4, we obtain the following two corollaries.Corollary 7: Let W = {W : ∥W∥2,1 ≤W2,1} and A = {A ∈Rn×c : ∥A∥2,∞≤A2,∞}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 2,q for q = (ln(c)/(ln(c) −1)) is bounded as Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σαtr(W⊤A(α)) \u0014 ≤W2,1A2,∞ \u0012 3 ln(c) nr (23) with the fact that the dual norm of ℓ2,1 is ℓ2,∞. Corollary 8: Let W = {W : ∥W∥F ≤WF} and A = {A ∈ Rd×c : ∥A∥F ≤AF}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 2,2 is bounded as Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σαtr(W⊤A(α)) \u0014 ≤WFAF \u0012 2 nr (24) with the fact that the dual norm of the Frobenius norm is the Frobenius norm.Lemma 9 [10]: Let W = {W : ∥W∥∗≤W∗} and A = {A ∈Rd×c : ∥A∥2 ≤A2}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 ∗is bounded as Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σαtr(W⊤A(α)) \u0014 ≤W∗A2 \u0012 ln(2dc) nr (25) with the fact that the dual norm of the nuclear norm is the spectral norm and dc = max(d, c).Lemma 10: Let Am1 ∈Rm1×n1 and Am2 ∈Rn1×m2 be two matrices, and then the following inequality holds: λmin \u0003 A⊤ m1 Am1 \u0004 ∥Am2∥2 F ≤∥Am1 Am2∥2 F (26) where λmin(·) represents the smallest eigenvalue of the matrix inside the bracket.Proof: For a given complex Hermitian matrix Mr and nonzero vector xr, the Rayleigh quotient R(Mr, xr) [42], [43] is deﬁned as R(Mr, xr) = x∗ r Mr xr x∗r xr (27) and the following inequality holds: λmin(Mr) ≤R(Mr, xr) ≤λmax(Mr). (28) Let a = Vec(Am2), where Vec(·) is an operator converting a matrix into a vector, and then, ∥Am1 Am2∥2 F can be rewritten in the form of ℓ2-norm as ∥Am1 Am2∥2 F = ∥(I ⊗Am1)a∥2 2 = a⊤(I ⊗Am1)⊤(I ⊗Am1)a = a⊤\u0003 I ⊗A⊤ m1 \u0004 (I ⊗Am1)a = a⊤\u0003 I ⊗A⊤ m1 Am1 \u0004 a (29) where ⊗represents the Kronecker product [44] and I is an identity matrix with proper size.Combining (28) with (29), the following inequality holds, namely: λmin \u0003 A⊤ m1 Am1 \u0004 ∥Am2∥2 F = λmin \u0003 I ⊗A⊤ m1 Am1 \u0004 ∥Am2∥2 F = λmin \u0003 I ⊗A⊤ m1 Am1 \u0004 (a⊤a) ≤a⊤\u0003 I ⊗A⊤ m1 Am1 \u0004 a = ∥Am1 Am2∥2 F (30) which completes the proof.□ Provided Lemma 10, tr((X Z)⊤L(X Z)) ≤Ztr in (21) can be derived into a constraint in the form of Frobenius norm on Z. Speciﬁcally, note that the Laplacian matrix is positive semideﬁne and its SVD is L = U L\u0002LU⊤ L = U L\u0002(1/2) L \u0002(1/2) L U⊤ L , so we have tr((X Z)⊤L(X Z)) = tr \u0003 (X Z)⊤U L\u0002 1 2 L\u0002 1 2 LU⊤ L (X Z) \u0004 = tr \u000f\u000f \u0002 1 2 LU⊤ L X Z \u0010⊤\u000f \u0002 1 2 LU⊤ L X Z \u0010\u0010 = \u0007\u0007\u0007\u0002 1 2 LU⊤ L X Z \u0007\u0007\u0007 2 F ≥λmin(X⊤LX)∥Z∥2 F. (31) If λmintr = λmin(X⊤LX) > 0, we have ∥Z∥F ≤Zt, where Zt = (Ztr/λmintr)1/2.In addition, we manage to rewrite the constraint X Z ∈ [−1, 1]n×c as the form of Frobenius norm on Z. Since X Z ∈ [−1, 1]n×c and ∥X Z∥2 F ≤nc, then similar to (31), we may obtain that ∥Z∥F ≤Zb if λminb = λmin(X⊤X) > 0, where Zb = (nc/λminb)1/2.Taking the three different Frobenius norm-based constraints (i.e., ∥Z∥F ≤√ZF, ∥Z∥F ≤Zt, ∥Z∥F ≤Zb) on the matrix Z into account, and let Zmin = min{√ZF, Zb, Zt}, we have ∥Z∥F ≤ \u0015√ZF, if λmintr ≤0 or λminb ≤0 Zmin, otherwise. (32) For convenience, we denote the upper bound of ∥Z∥F as Z that can be √ZF or Zmin according to different conditions in (32).Herein, we begin to formally derive the generalization error of LNSI.Theorem 11: Let X2 = maxi ∥Xi∥2 and XF = maxi ∥Xi∥F, and then the model complexity of function class F\r is upper bounded by Rn(F\r) ≤min \u0015 Z∗X2 \u0016 ln(2dc) nc , ZXF \u0016 2 nc \u0017 + E2,1 \u0016 3 ln(c) nc . (33) Proof: First, the Rademacher complexity of linear function class F\r in our case can be written as R(F\r) := Eσ \u0013 sup θ∈\r 1 nc nc \u0002 α=1 σα \u0003 Xiα ZI jα + Eiα, jα \u0004 \u0014 . (34) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1565\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classiﬁcation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract—Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classiﬁer training and severely decrease the classiﬁcation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classiﬁcation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as “label noise handling via side information” (LNSI).Speciﬁcally, the observed label matrix is decomposed as the sum of two parts, in which the ﬁrst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufﬁciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the “Summit of the Six Top Talents” Program under Grant DZXX-027, in part by the “Young Elite Scientists Sponsorship Program” by Jiangsu Province, in part by the “Young Elite Scientists Sponsorship Program” by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the “111” Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Artiﬁcial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identiﬁer 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classiﬁcation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets conﬁrm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms—Classiﬁcation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classiﬁer, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the sufﬁ- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classiﬁcation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signiﬁcantly mislead the classiﬁer training and then severely decrease the classiﬁcation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signiﬁcance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]–[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classiﬁcation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite difﬁcult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X © 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1109\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.3.Experimental results of the compared methods on ﬁve UCI benchmark data sets. (a)–(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively.TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD.)UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise.D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203.COIL20 is a popular public data set for object classiﬁcation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles.The resolution of each gray-level image is 32 × 32 [46].We use the output of the ﬁrst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions.Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data.Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label.In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with σk = 0.1.In addition, the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,103} in order to obtain the satisfactory results.The classiﬁcation accuracies of all compared methods under different label noise levels are presented in Table III.It can be observed that the performances of all methods decrease with the increase in noise level.However, LNSI achieves the best results in most cases when compared with other baseline methods.Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%.E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition.Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes.The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.0991\n",
            "Text\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 11 TABLE IV COMPARISON OF VARIOUS METHODS ON MNIST DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE V COMPARISON OF VARIOUS METHODS ON CIFAR-10 DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD class ranges from 359 to 454 and 80% of these examples are randomly picked up to establish the training set.The resolution of each gray-level handwritten digit image is 28 × 28.We use the output of the ﬁrst fully connected layer of VGGNet- 16 to extract the CNN features for each image, and therefore, the dimensionality of one feature vector is 4096.All experimental settings are the same as those in Sections VI-C and VI-D. In this data set, a 10-NN graph with σk = 1 was established and the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {100, 101, . . . ,105}.Similarly, the mean accuracies and standard deviations of ﬁve independent runs of all comparators are reported, which can be found in Table IV.We see that when the noise rate is 0%, 20%, and 40%, LNSI is comparable with other baseline methods.However, LNSI achieves very robust and satisfactory performances on MNIST when the label noise rate is 60%, while all other baseline methods perform unsatisfactorily.Consequently, the existing methods are inferior to LNSI in terms of the classiﬁcation accuracy under serious label noise.F. Experiments on CIFAR-10 Data Set This section tests the performance of LNSI and baselines on the natural image data set, i.e., the CIFAR-105.Here we use a subset of CIFAR-10 for our experiments which contains 30 000 image examples randomly sampled from original data set across different classes.The resolution of each color image is 32 × 32 × 3.We extract the CNN features for each image, which are calculated as the output of the ﬁrst fully connected layer of VGGNet-16, and therefore, the dimensionality of one feature vector is 4096.5https://www.cs.toronto.edu/ kriz/cifar.html All experimental settings are the same as those in Sections VI-C–VI-E. In this data set, 80% of the examples are randomly selected as the training set.A 15-NN graph with σk = 0.5 was established, and the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−1, 100, . . . ,103}.The classiﬁcation accuracies of the compared algorithms under different label noise levels are presented in Table V, from which we can see that LNSI outperforms other baselines and is much more robust to noisy labels with the increase in the level of label noise.Note that LICS and LSLN are not compared on the CIFAR-10 data set as they are not scalable to this data set.G. Illustration of Convergence In Section V-A, we have theoretically proved that the optimization process in Algorithm 1 will converge to a sta- tionary point.In this section, we present the convergence curves of LNSI on the four practical data sets appeared in Sections VI-C–VI-F. From the curves shown in Fig.4, we see that the differences between the two sides of equality con- straints in (7) decrease rapidly on all data sets.This observa- tion justiﬁes our previous theoretical results and demonstrates that ADMM is effective and efﬁcient for solving (7).H. Ablation Study From the above-mentioned experimental results presented in Sections VI-B–VI-F, we see that LNSI performs favorably to other existing methods.Therefore, this section investi- gates the effects of key components of LNSI that leads to the good performance.Speciﬁcally, we conduct the ablation study on LNSI to explore the individual contributions of the low-rank regularizer ∥Z∥∗and the Laplacian regularizer Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.0902\n",
            "Text\n",
            "Process.Syst.,2015, pp.10–18. [16] A. Khetan, Z. C. Lipton, and A. Anandkumar, “Learning from noisy singly-labeled data,” in Proc.Int.Conf.Learn.Represent.,2018. [Online].Available: https://openreview.net/forum?id=H1sUHgb0Z [17] G. Patrini, A. Rozza, A. K. Menon, R. Nock, and L. Qu, “Making deep neural networks robust to label noise: A loss correction approach,” in Proc.Int.Conf.Comput.Vis.Pattern Recognit.,2017, pp.2233–2241. [18] B. Han et al., “Co-teaching: Robust training of deep neural networks with extremely noisy labels,” in Proc.Adv.Neural Inf.Process.Syst.,2018, pp.8536–8546. [19] B. Han et al., “Masking: A new perspective of noisy supervision,” in Proc.Adv.Neural Inf.Process.Syst.,2018, pp.5836–5846. [20] B. Han, I. W. Tsang, L. Chen, C. P. Yu, and S.-F. Fung, “Progressive stochastic learning for noisy labels,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.10, pp.5136–5148, Oct. 2018. [21] J. Zhang, V. S. Sheng, T. Li, and X. Wu, “Improving crowdsourced label quality using noise correction,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.5, pp.1675–1688, May 2018. [22] F. Zhao and Y. Guo, “Learning discriminative recommendation systems with side information,” in Proc.Int.Joint Conf.Artif.Intell.,2017, pp.3469–3475. [23] C. C. Aggarwal, Y. Zhao, and P. S. Yu, “On text clustering with side information,” in Proc.28th Int.Conf.Data Eng.,Apr. 2012, pp.894–904. [24] R. Zhang, F. Nie, and X. Li, “Semisupervised learning with parameter- free similarity of label and side information,” IEEE Trans.Neural Netw.Learn.Syst.,vol.30, no.2, pp.405–414, Feb. 2019. [25] K. Ahn, K. Lee, H. Cha, and C. Suh, “Binary rating estimation with graph side information,” in Proc.Adv.Neural Inf.Process.Syst.,2018, pp.4272–4283. [26] N. Xue, Y. Panagakis, and S. Zafeiriou, “Side information in robust principal component analysis: Algorithms and applications,” in Proc.Int.Conf.Comput.Vis.,Oct. 2017, pp.4317–4325. [27] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” IEEE Trans.Pattern Anal.Mach.Intell.,vol.35, no.1, pp.171–184, Jan. 2013. [28] C. Gong, T. Liu, D. Tao, K. Fu, E. Tu, and J. Yang, “Deformed graph Laplacian for semisupervised learning,” IEEE Trans.Neural Netw.Learn.Syst.,vol.26, no.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "source": [
        "# Display top results with proper tensor conversion\n",
        "print(\"Top results:\")\n",
        "for score, idx in zip(top_results.values, top_results.indices):\n",
        "    # Convert tensor index to a standard integer\n",
        "    idx = idx.item()  # Convert to integer\n",
        "\n",
        "    # Access the correct sentence chunk from your DataFrame\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    print(\"Text:\")\n",
        "    print(df[\"sentence_chunk\"].iloc[idx])  # Use .iloc to avoid potential index mismatches\n",
        "    print(\"\\n\\n\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:14:30.396485Z",
          "iopub.execute_input": "2024-10-17T18:14:30.397489Z",
          "iopub.status.idle": "2024-10-17T18:14:30.409186Z",
          "shell.execute_reply.started": "2024-10-17T18:14:30.397444Z",
          "shell.execute_reply": "2024-10-17T18:14:30.40799Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn_INkdQ6s5u",
        "outputId": "65375286-7111-46d0-87b3-96099f666a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top results:\n",
            "Score: 0.4298\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 5 According to [36], the closed-form solution to (9) can be expressed as Z = Udiag(max{\u0002ii −τ, 0})V ⊤ ∀i =1, 2, . . . ,min(d, c) (10) where U and V are obtained by conducting the singular value decomposition (SVD) on ˆT (i.e., ˆT = U\u0002V ⊤), and \u0002ii is the ith diagonal element of the singular value matrix \u0002. Update E: By dropping the unrelated terms to E in (8), the subproblem of E is min E λ3∥E∥2,1+tr \u0003 M⊤ 1 (Y −B−E) \u0004 + μ 2 ∥Y −B−E∥2 F ⇒min E λ3∥E∥2,1 −tr\u0003M⊤ 1 E\u0004 + μ 2 tr(E⊤E −2(Y −B)⊤E) ⇒min E λ3∥E∥2,1 + μ 2 tr \b E⊤E −2 \u0005 Y −B + 1 μ M1 \u0006⊤ E \t ⇒min E λ3 μ ∥E∥2,1 + 1 2 \u0007\u0007\u0007\u0007E − \u0005 Y −B + M1 μ \u0006\u0007\u0007\u0007\u0007 2 F ⇒min E η∥E∥2,1 + 1 2∥E − M∥2 F (11) where M = Y −B + (M1/μ) and η = (λ3/μ).Herein, the closed-form solution to the general optimization problem related to ℓ2,1 norm is provided in the following lemma.Lemma 1 [27], [37]: Let ˜Q be a given matrix and ˜W is the variable to be optimized.If the optimal solution to min ˜W ˜α∥˜W∥2,1 + 1 2∥˜W −˜Q∥2 F (12) is ˜W ∗, then the ith row of ˜W ∗is ˜W ∗ i = ⎧ ⎪⎨ ⎪⎩ ∥˜Qi∥2 −˜α ∥˜Qi∥2 ˜Qi, if ∥˜Qi∥2 > ˜α 0, otherwise. (13) Obviously, the subproblem related to E has the same formulation with (13).Therefore, the closed-form solution to (11) is expressed as Ei = ⎧ ⎨ ⎩ ∥ Mi∥2 −η ∥ Mi∥2 Mi if ∥ Mi∥2 > η 0, otherwise (14) where Ei and Mi represent the ith row of the related matrices, respectively.Update J: The subproblem regarding J is min J λ2tr((X J)⊤L(X J)) + tr \u0003 M⊤ 2 (B −X J) \u0004 +tr\u0003M⊤ 3 (Z −J)\u0004 + μ 2 \u0003∥B −X J∥2 F + ∥Z −J∥2 F \u0004 ⇒min J tr \u000f λ2 J⊤(X⊤LX)J + μ 2 (J⊤(X⊤X)J + J⊤J) \u0010 −tr\u0003M⊤ 2 X J + M⊤ 3 J + μ(B⊤X J + Z⊤J)\u0004. (15) By computing the derivative of (15) with respect to J and then setting it to zero, J can be updated as J = (2λ2X⊤LX + μX⊤X + μI)−1 (X⊤M2 + M3 + μX⊤B + μZ) (16) where I denotes the identity matrix with proper size through- out this article.Update B: The subproblem on B with the continuous convex set B ∈[−1, 1]n×c is min B tr \u0003 M⊤ 1 (Y −B −E) \u0004 + tr \u0003 M⊤ 2 (B −X J) \u0004 + μ 2 (∥Y −B −E∥2 F + ∥B −X J∥2 F) s.t.B ∈[−1, 1]n×c. (17) By ﬁrst computing the derivative of (17) with respect to B and setting it to zero, the optimal B (i.e., ˆB) can be represented as ˆB = μ(Y −E + X J) + M1 −M2 2μ . (18) To restrict ˆB to the feasible region, we further project all its elements to [−1, 1] as Bij = \t( ˆBij ) (19) where the projection \t(x) is deﬁned as \t(x) = ⎧ ⎪⎨ ⎪⎩ 1, if x > 1 x, if x ∈[−1, 1] −1, if x < −1. (20) The entire optimization process for LNSI is summarized in Algorithm 1.V. THEORETICAL ANALYSES This section provides the theoretical analyses on LNSI.We ﬁrst prove that the optimization process explained in Section IV-B will converge to a stationary point and then analyze the computational complexity of Algorithm 1.Finally, we theoretically prove that the generalization risk of LNSI is upper bounded.A. Proof of Convergence In this section, we discuss the convergence property of the ADMM method in Algorithm 1.As discussed in [38], the convergence of ADMM has been proved when there are only two blocks of variables.However, (7) contains four vari- ables Z, E, J, and B, and thus, such a convergence property of ADMM is not theoretically guaranteed.By demonstrating that (7) is equivalent to the standard optimization problem with two variables, we show that the iterative solution provided in Algorithm 1 also enjoys the good property of convergence in Theorem 2.Theorem 2: Given the optimization problem (7), the itera- tive process of ADMM will converge to a stationary point.First, we provide the convergence conditions for general ADMM solver and then prove that the proposed algorithm satisﬁes the required conditions.Therefore, the iterative solu- tion in Algorithm 1 is guaranteed to converge to a stationary point.The detailed proof can be found in the Appendix.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.3959\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Algorithm 1 Algorithm for Solving LNSI Input: feature matrix X, observed label matrix Y; trade-off parameters: λ1, λ2, and λ3; Z = O, J = Z, E = O, B = O, M1 = O, M2 = O, M3 = O; μ = 10−3, μmax = 106, ρ = 1.2, ϵ = 10−6, iter_max = 1000; iter = 0; 1: Construct graph G and calculate the Laplacian matrix L via (3); 2: while not converge do 3: Update Z via (10), 4: Update E via (14), 5: Update J via (16), 6: Update B via (19), 7: Update the multipliers M1 := M1 + μ(Y −B −E), M2 := M2 + μ(B −X J), M3 := M3 + μ(Z −J), 8: Update the parameter μ by μ := min(ρμ, μmax), 9: iter := iter + 1, 10: Check the convergence conditions: ∥Y −B−E∥F ≤ϵ and ∥B−X J∥F ≤ϵ and ∥Z−J∥F ≤ ϵ; or iter > iter_max.11: end while Output: optimized Z∗and E∗. B. Computational Complexity This section studies the computational complexity of Algorithm 1.The graph construction in Line 1 of Algorithm 1 takes O(n2) complexity.Line 3 is accomplished by using the SVD, of which the complexity is O(min(d2c, dc2)).In Line 4, one should compute the ℓ2-norm of each row of a n × c matrix E, so the complexity is O(nc).Note that a d × d matrix should be inverted in Line 5, so the complexity of this step is O(d3).Therefore, the total complexity of our proposed algorithm is O(n2 +(min(d2c, dc2)+nc+d3)k) by assuming that Lines 2–9 are iterated k times.Note that the complexity of Algorithm 1 is squared to the number of training examples n, so its complexity is acceptable.C. Generalization Bound In this section, we derive the generalization bound of LNSI.1) Preliminaries: Recall that our goal is to ﬁnd a suit- able project matrix Z by recovering the clean label matrix X Z, given the observed noisy label matrix Y and example features X. Similar to [10], (6) can be reformulated to the following expression with hard constraints, namely: min Z,E \u0002 (i, j)∈{1,...,n}×{1,...,c} ℓ((X Z + E)ij , Y ij ) s.t.∥Z∥∗≤Z∗, ∥Z∥2 F ≤ZF, X Z ∈[−1, 1]n×c tr((X Z)⊤L(X Z)) ≤Ztr, ∥E∥2,1 ≤E2,1. (21) Let θ = (Z, E) be any feasible solution, and \r = {(Z, E) | ∥Z∥∗ ≤ Z∗, ∥Z∥F ≤ √ZF, X Z ∈ [−1, 1]n×c, tr((X Z)⊤L(X Z)) ≤Ztr, ∥E∥2,1 ≤E2,1} be the feasible solution set.Also, let fθ(i, j) = Xi ZI j + Eij be the estimation function for Yij parameterized by θ = (Z, E), and F\r = { fθ | θ ∈\r} be the set of feasible functions.I j is the jth column of identity matrix I ∈Rc×c.We are interested in the following two “ℓ-risk” quantities: 1) expected ℓ-risk: Rℓ( f ) = Ei, j [ℓ( f (i, j), Yij )]; 2) empirical ℓ-risk: ˆRℓ( f ) = (1/nr) \u0011 (i, j) ℓ( f (i, j)Yij ), where nr is the number of observed entries.Thus, LNSI is to ﬁnd a proper θ∗= (Z∗, E∗) that parameterizes f ∗= arg min f ∈F\r ˆRℓ( f ).2) Generalization Bound of LNSI: To bound the generaliza- tion error of LNSI, we ﬁrst link the quality of training labels to Rademacher complexity, which theoretically measures the complexity of a function class.We will show that high-quality labels of training examples will result in a lower model complexity and thus a smaller error bound.To begin with, we apply the following lemma to bound the expected ℓ-risk.Lemma 3 (Bound on Expected ℓ-Risk [39]): Let ℓbe the loss function bounded by B with Lipschitz constant Lℓ, and δ be a constant where 0 < δ < 1.With probability at least 1 −δ, we have max f ∈F |Rℓ( f ) −ˆRℓ( f )| ≤2LℓRn(F) + B \u0012 ln(1/δ) 2nr where Rn(F) := E[R(F)] is the Rademacher complexity of the function class F and R(F) := Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σα f (α) \u0014 is the empirical Rademacher complexity on the training examples.Note that σα (α = 1, 2, . . . ,nr) are independent identically distributed (i.i.d.)Rademacher random variables.Given Lemma 3, we see that the key to derive the upper bound of a function f ∈F is to bound the complexity Rn(F\r).More formally, the Rademacher complexity can be bounded in terms of the constraints in (21).Before diving into the details, we ﬁrst provide several useful theorems and lemmas.Lemma 4 (Complexity Bound [40]): Let S be a closed con- vex set and let F : S →R be β-strongly convex with respect to ∥· ∥. In addition, we assume that F⋆(O) = 0 with F⋆being the Fenchel conjugate of function F. Further, let A = {A : ∥A∥⋆≤A} and deﬁne W = {W ∈S : F(W) ≤Fmax}.Considering the class of linear functions F = {A → W, A : W ∈W}, we have R(F) ≤A \u0012 2Fmax βnr (22) where W, A = tr(W⊤A).Lemma 5 [41]: The function F : Rn×c →R deﬁned as F(W) = (1/2)∥W∥2 2,q for q = (ln(c)/(ln(c) −1)) is (1/(3 ln(c)))-strongly convex with respect to ∥·∥2,1 over Rn×c.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.3726\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.14 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS It can be easily veriﬁed that the above problem (47) can be represented in the form of (40) by setting P = \u0018B Z \u0019 , Q = ⎡ ⎣ E J K ⎤ ⎦ (49) and AP = ⎡ ⎢⎢⎣ I O I O I O O I ⎤ ⎥⎥⎦, BQ = ⎡ ⎢⎢⎣ I O O O −X O O O −I O −I O ⎤ ⎥⎥⎦, C = ⎡ ⎢⎢⎣ Y O O O ⎤ ⎥⎥⎦ (50) where I and O are the identity matrices and zero matrices with proper sizes, respectively.The functions f (P) and g( Q) in (40) can be, respectively, expressed as f (P) = ∥Z∥∗+ λ1∥Z∥2 F (51) g( Q) = λ2tr((X J)⊤L(X J)) + λ3∥E∥2,1 + IC(K). (52) The unaugmented Lagrangian is formulated as L0 = ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 + tr(M⊤ 1 (Y −B −E)) + tr\u0003M⊤ 2 (B −X J)\u0004 + tr \u0003 M⊤ 3 (Z −J) \u0004 . (53) Obviously, both f (P) and g( Q) are closed, proper, and convex, and the unaugmented Lagrangian L0 has a saddle point, which demonstrate that the optimization process for (7) is convergent.□ REFERENCES [1] R. J. Hickey, “Noise modelling and evaluating learning from examples,” Artif.Intell.,vol.82, nos.1–2, pp.157–179, 1996. [2] C. Gong, H. Zhang, J. Yang, and D. Tao, “Learning with inadequate and incorrect supervision,” in Proc.Int.Conf.Data Mining, Nov. 2017, pp.889–894. [3] W. Gao, L. Wang, Y.-F. Li, and Z.-H. Zhou, “Risk minimization in the presence of label noise,” in Proc.AAAI Conf.Artif.Intell.,2016, pp.1575–1581. [4] N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari, “Learning with noisy labels,” in Proc.Adv.Neural Inf.Process.Syst.,2013, pp.1196–1204. [5] N. Manwani and P. Sastry, “Noise tolerance under risk minimization,” IEEE Trans.Cybern.,vol.43, no.3, pp.1146–1151, Jun. 2013. [6] G. Patrini, F. Nielsen, R. Nock, and M. Carioni, “Loss factorization, weakly supervised learning and label noise robustness,” in Proc.Int.Conf.Mach.Learn.,2016, pp.708–717. [7] R. Wang, T. Liu, and D. Tao, “Multiclass learning with partially corrupted labels,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.6, pp.2568–2580, Jun. 2018. [8] P. Zhao, Y. Jiang, and Z.-H. Zhou, “Multi-view matrix completion for clustering with side information,” in Proc.Paciﬁc–Asia Conf.Knowl.Discovery Data Mining, 2017, pp.403–415. [9] M. Xu, R. Jin, and Z. Zhou, “Speedup matrix completion with side information: Application to multi-label learning,” in Proc.Adv.Neural Inf.Process.Syst.,2013, pp.2301–2309. [10] K.-Y. Chiang, C.-J. Hsieh, and I. S. Dhillon, “Matrix completion with noisy side information,” in Proc.Adv.Neural Inf.Process.Syst.,2015, pp.3447–3455. [11] Y. Guo, “Convex co-embedding for matrix completion with predic- tive side information,” in Proc.31st AAAI Conf.Artif.Intell.,2017, pp.1955–1961. [12] K.-Y. Chiang, C.-J. Hsieh, and I. Dhillon, “Robust principal component analysis with side information,” in Proc.Int.Conf.Mach.Learn.,2016, pp.2291–2299. [13] F. Muhlenbach, S. Lallich, and D. A. Zighed, “Identifying and handling mislabelled instances,” J. Intell.Inf.Syst.,vol.22, no.1, pp.89–109, 2004. [14] X. Zhu, X. Wu, and Q. Chen, “Eliminating class noise in large datasets,” in Proc.Int.Conf.Mach.Learn.,2003, pp.920–927. [15] B. van Rooyen, A. K. Menon, and R. C. Williamson, “Learning with symmetric label noise: The importance of being unhinged,” in Proc.Adv.Neural Inf.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.3018\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 15 [39] P. L. Bartlett and S. Mendelson, “Rademacher and Gaussian complexities: Risk bounds and structural results,” J. Mach.Learn.Res.,vol.3, pp.463–482, Mar. 2003. [40] S. Kakade, S. Shalev-Shwartz, and A. Tewari. (2009).On the Duality of Strong Convexity and Strong Smoothness: Learn- ing Applications and Matrix Regularization. [Online].Available: http://ttic.uchicago.edu/shai/papers/KakadeShalevTewari09.pdf [41] S. M. Kakade, S. Shalev-Shwartz, and A. Tewari, “Regularization techniques for learning with matrices,” J. Mach.Learn.Res.,vol.13, pp.1865–1890, Jun. 2012. [42] P.-A. Absil and P. Van Dooren, “Two-sided Grassmann–Rayleigh quo- tient iteration,” Numerische Mathematik, vol.114, no.4, pp.549–571, 2010. [43] R. Mahony and P.-A. Absil, “The continuous-time Rayleigh quotient ﬂow on the sphere,” Linear Algebra Appl.,vol.368, pp.343–357, Jul. 2003. [44] H. Zhang and F. Ding, “On the kronecker products and their applica- tions,” J. Appl.Math.,vol.2013, Jun. 2013, Art.no.296185. [45] R. Meir and T. Zhang, “Generalization error bounds for Bayesian mix- ture algorithms,” J. Mach.Learn.Res.,vol.4, pp.839–860, Dec. 2003. [46] J. Yu, D. Tao, J. Li, and J. Cheng, “Semantic preserving distance metric learning and applications,” Inf.Sci.,vol.281, pp.674–686, Oct. 2014. [47] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed optimization and statistical learning via the alternating direction method of multipliers,” Found.Trends Mach.Learn.,vol.3, no.1, pp.1–122, Jan. 2011.Yang Wei received the B.S. degree from the School of Computer Science and Engineering, Nanjing Uni- versity of Science and Technology, Nanjing, China, in 2015, where she is currently pursuing the Ph.D. degree.Her current research interests include pattern recognition, incomplete data-based learning, and deep learning.Chen Gong (M’17) received the dual Ph.D. degree from Shanghai Jiao Tong University (SJTU), Shang- hai, China, and the University of Technology Sydney (UTS), Ultimo, NSW, Australia, in 2016.He is currently a Professor with the School of Computer Science and Engineering, Nanjing Uni- versity of Science and Technology, Nanjing, China.He has authored or coauthored more than 50 techni- cal articles at prominent journals and conferences such as the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), the IEEE TRANSACTIONS ON IMAGE PROCESSING (T-IP), the IEEE TRANS- ACTIONS ON CYBERNETICS (T-CYB), CVPR, AAAI, IJCAI, ICDM, and so on.His current research interests include machine learning and data mining.Dr. Gong was a recipient of the Excellent Doctoral Dissertation Award by SJTU and the Chinese Association for Artiﬁcial Intelligence (CAAI).He was also enrolled by the Summit of the Six Top Talents Program of Jiangsu Province, China.Shuo Chen received the B.S. degree from the School of Computer Science and Engineering, Jinling Insti- tute of Technology, Nanjing, China, in 2014.He is currently pursuing the Ph.D. degree with the Nanjing University of Science and Technology, Nanjing.His current research interests include pattern recognition, metric learning, and deep learning.Tongliang Liu (M’14) is currently a Lecturer with the School of Computer Science and the Fac- ulty of Engineering, and a Core Member with the UBTECH Sydney AI Centre, The University of Sydney, Darlington, NSW, Australia.He has authored and coauthored more than 60 research arti- cles, including the IEEE TRANSACTIONS ON PAT- TERN ANALYSIS AND MACHINE INTELLIGENCE (T-PAMI), the IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), the IEEE TRANSACTIONS ON IMAGE PROCESSING (T-IP), ICML, AAAI, IJCAI, CVPR, ECCV, KDD, and ICME, with best paper awards.His current research interests include machine learning, computer vision, and data mining.Mr. Liu was a recipient of the 2019 ICME Best Paper Award and the Dis- covery Early Career Researcher Award (DECRA) from Australian Research Council (ARC).Jian Yang (M’08) received the Ph.D. degree in pattern recognition and intelligence systems from the Nanjing University of Science and Technology (NUST), Nanjing, China, in 2002.In 2003, he was a Post-Doctoral Researcher with the University of Zaragoza, Zaragoza, Spain.From 2004 to 2006, he was a Post-Doctoral Fellow with the Biometrics Centre, The Hong Kong Polytechnic University, Hong Kong.From 2006 to 2007, he was a Post-Doctoral Fellow with the Department of Com- puter Science, New Jersey Institute of Technology, Newark, NJ, USA.He is currently a Chang-Jiang Professor with the School of Computer Science and Technology, NUST.He has authored more than 200 scientiﬁc articles in pattern recognition and computer vision.His articles have been cited more than 5000 times in the Web of Science and 13000 times in the Scholar Google.His current research interests include pattern recogni- tion, computer vision, and machine learning.Dr. Yang is a fellow of IAPR.He is/was currently an Associate Editor of Pattern Recognition, Pattern Recognition Letters, the IEEE TRANSAC- TIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (T-NNLS), and Neurocomputing.Dacheng Tao (F’15) is a Professor of computer sci- ence and an ARC Laureate Fellow with the School of Computer Science and the Faculty of Engineering, and the Inaugural Director of the UBTECH Syd- ney Artiﬁcial Intelligence Centre, The University of Sydney, Darlington, NSW, Australia.His research results in artiﬁcial intelligence have expounded in one monograph.He has authored or coauthored more than 200 publications at prestigious journals and prominent conferences, such as the IEEE TRANS- ACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (T-PAMI), the International Journal of Computer Vision (IJCV), the Journal of Machine Learning Research (JMLR), AAAI, IJCAI, NIPS, ICML, CVPR, ICCV, ECCV, ICDM, and KDD, with several best paper awards.Prof. Tao is a fellow of the Australian Academy of Science.He was a recipient of the 2018 IEEE ICDM Research Contributions Award and the 2015 Australian Scopus-Eureka Prize.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2987\n",
            "Text:\n",
            "10, pp.2261–2274, Oct. 2015. [29] Y. Huang, D. Xu, and F. Nie, “Semi-supervised dimension reduction using trace ratio criterion,” IEEE Trans.Neural Netw.Learn.Syst.,vol.23, no.3, pp.519–526, Mar. 2012. [30] Y. Wang, Y. Jiang, Y. Wu, and Z.-H. Zhou, “Spectral clustering on multiple manifolds,” IEEE Trans.Neural Netw.,vol.22, no.7, pp.1149–1161, Jul. 2011. [31] J. Yu, C. Hong, Y. Rui, and D. Tao, “Multitask autoencoder model for recovering human poses,” IEEE Trans.Ind. Electron.,vol.65, no.6, pp.5060–5068, Jun. 2018. [32] J. Yu, X. Yang, F. Gao, and D. Tao, “Deep multimodal distance metric learning using click constraints for image ranking,” IEEE Trans.Cybern.,vol.47, no.12, pp.4014–4024, Dec. 2017. [33] C. Gong, D. Tao, W. Liu, L. Liu, and J. Yang, “Label propagation via teaching-to-learn and learning-to-teach,” IEEE Trans.Neural Netw.Learn.Syst.,vol.28, no.6, pp.1452–1465, Jun. 2017. [34] C.-J. Hsieh, N. Natarajan, and I. S. Dhillon, “PU learning for matrix completion,” in Proc.32nd Int.Conf.Mach.Learn.,2015, pp.2445–2453. [35] N. Komodakis and G. Tziritas, “Approximate labeling via graph cuts based on linear programming,” IEEE Trans.Pattern Anal.Mach.Intell.,vol.29, no.8, pp.1436–1453, Aug. 2007. [36] Z. Lin, M. Chen, and Y. Ma, “The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices,” 2010, arXiv:1009.5055. [Online].Available: https://arxiv.org/abs/1009.5055 [37] C. Gong, “Exploring commonality and individuality for multi-modal curriculum learning,” in Proc.AAAI Conf.Artif.Intell.,2017, pp.1926–1933. [38] C. Chen, B. He, Y. Ye, and X. Yuan, “The direct extension of ADMM for multi-block convex minimization problems is not necessarily con- vergent,” Math.Program.,vol.155, nos.1–2, pp.57–79, 2016.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2934\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Since Z and E are independent variables, the Rademacher complexity can be written as R(F\r) = Eσ \u0013 sup Z∈\rZ 1 nc nc \u0002 α=1 σα Xiα ZI jα \u0014 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σα Eiα, jα \u0014 = Eσ \u0013 sup Z∈\rZ 1 nc nc \u0002 α=1 σαtr \u0003 Z⊤X⊤ iα I jα⊤\u0004 \u0014 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σαtr(E⊤Iiα I jα⊤) \u0014 (35) where Ii is the ith column of identity matrix I ∈Rn×n, \rZ = {Z | ∥Z∥∗ ≤ Z∗, ∥Z∥F ≤ √ZF, X Z ∈ [−1, 1]n×c, tr((X Z)⊤L(X Z)) ≤Ztr}, and \rE = {E | ∥E∥2,1 ≤E2,1}.Since the last three constraints in \rZ can be rewritten as a much simpler formulation (32) according to Lemma 10, therefore, we have \rZ1 = {Z | ∥Z∥∗≤Z∗, ∥Z∥F ≤Z}.From the deﬁnition of Rademacher complexity, we know that it measures the richness of a class of real-valued functions with respect to a probability distribution.Hence, the tighter the constraints of functions are, the smaller the Rademacher complexity will be.By using the Rademacher contraction principle [45], (35) can be transformed as R(F\r) ≤Eσ \u0013 sup Z∈\rZ1 1 nc nc \u0002 α=1 σαtr \u0003 Z⊤X⊤ iα I jα⊤\u0004 \u0014 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σαtr(E⊤Iiα I jα⊤) \u0014 ≤min \u0015 Eσ \u0013 sup Z∈\rZ2 1 nc nc \u0002 α=1 σαtr \u0003 Z⊤X⊤ iα I jα⊤\u0004 \u0014 Eσ \u0013 sup Z∈\rZ3 1 nc nc \u0002 α=1 σαtr\u0003Z⊤X⊤ iα I jα⊤\u0004 \u0014\u0017 + Eσ \u0013 sup E∈\rE 1 nc nc \u0002 α=1 σαtr(E⊤Iiα I jα⊤) \u0014 (36) where \rZ2 = {Z | ∥Z∥∗≤Z∗} and \rZ3 = {Z | ∥Z∥F ≤Z}.Taking Corollaries 7 and 8 and Lemma 9 into consideration, (36) leads to R(F\r) ≤E2,1∥Ii I j⊤∥2,∞ \u0016 3 ln(c) nc + min \u0015 Z∗max i, j \u0007\u0007X⊤ i I j⊤\u0007\u0007 2 \u0016 ln(2dc) nc Z max i, j \u0007\u0007X⊤ i I j⊤\u0007\u0007 F \u0016 2 nc \u0017 . (37) Since maxi, j ∥X⊤ i I j⊤∥2 = maxi ∥Xi∥2 max j ∥I j∥2 = maxi ∥Xi∥2, maxi, j ∥X⊤ i I j⊤∥F = maxi ∥Xi∥F and maxi, j ∥Ii I j⊤∥2,∞ = 1, we arrive at the upper bound of Rn(F\r), which is Rn(F\r) ≤min \u0015 Z∗X2 \u0016 ln(2dc) nc , ZXF \u0016 2 nc \u0017 + E2,1 \u0016 3 ln(c) nc . (38) □ Based on Theorem 11 and Lemma 3, the following inequal- ity holds: max f ∈F |Rℓ( f ) −ˆRℓ( f )| ≤2Lℓmin \u0015 Z∗X2 \u0016 ln(2dc) nc , ZXF \u0016 2 nc \u0017 + 2LℓE2,1 \u0016 3 ln(c) nc + B \u0016 ln(1/δ) 2nc (39) and thus the expected loss is upper bounded.As mentioned before, the matrix E is utilized to capture the label noise and the ℓ2,1 norm on it is upper bounded by E2,1.Specif- ically, we observe that E2,1 is governed by the label noise rate.A small noise rate will lead to a small E2,1, which further reduces the upper bound of the expected ℓ-risk in the right-hand side of (39).VI.EXPERIMENTS In this section, we ﬁrst use a toy data set to validate the motivation of LNSI (Section VI-A) and then com- pare LNSI with several representative baselines on various UCI benchmark data sets (Section VI-B).Next, we com- pare LNSI with these baseline algorithms on four practical data sets including ISOLET, COIL20, MNIST, and CIFAR-10 (Sections VI-C–VI-F).Also, the convergence process of the ADMM adopted in the Algorithm 1 is illustrated (Section VI-G).Afterward, we conduct the ablation study to verify the effectiveness of the critical regularizers of LNSI (Section VI-H) and also study the parametric sensitivity of LNSI (Section VI-I).Finally, we summarize the experimental results and give some insightful analyses (Section VI-J).Our LNSI is compared with ﬁve representative methods for noisy label handling, including labeled instance centroid smoothing (LICS) [3], unbiased logistic estimator (ULE) [4], μ stochastic gradient descent (μSGD) [6], learning with symmetric label noise (LSLN) [15], and coteaching [18].It is worth noting that the ﬁrst four baseline methods are only suitable for binary classiﬁcation, so we use the one-versus-the- rest strategy to make them applicable to multi-class situations.In addition, the prior knowledge such as the noise rate for all the compared methods is provided accurately.Note that coteaching can only handle images, so it is not compared on the nonimage data sets including ﬁve UCI data sets and ISOLET data set.A. Algorithm Validation First, we demonstrate the effectiveness of LNSI on a toy data set.As shown in Fig.2(a), we manually generate 8 examples in a 2-D space, which include 3 negative examples Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2934\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS in which ∥Z∥∗with nuclear norm is employed to achieve low-rank effect, ∥E∥2,1 with ℓ2,1 norm is utilized to realize row sparsity, and λ1 and λ3 are the nonnegative tradeoff parameters.By solving (2), we can obtain the optimal Z∗, which can be used to compute the label y ∈Rc of a test example x ∈Rd as y = Z∗⊤x.Then x is classiﬁed into the jth class if j = arg max j′=1,2,...,c y j′ with y j′ being the j′th element in the label vector y. It is worth pointing out that although the formulation of (2) is similar to the low-rank representation (LRR) proposed in [27], their usages and implications are quite different.First, LRR is developed for subspace clustering while our method is for label noise removal.Second, LRR aims to select sparse atoms from a predeﬁned dictionary to reconstruct a clean space, while our method tries to learn a proper mapping Z from feature space to label space in presence of the label noise encoded by E. Therefore, these two models are different although they look similar at ﬁrst glance.To sufﬁciently exploit the side information, we further use the Laplacian regularizer based on graph embedding.Graph Laplacian has been widely utilized in several machine learning tasks such as semisupervised learning [28], [29], spectral clustering [30], multi-task learning [31], and metric learning [32].However, to the best of our knowledge, it has not been used to deal with side information.Let G = {V, E} be an undirected weighted graph with vertex set V consist- ing of all n examples and E is the edge set encoding the similarity between these examples.The symmetric adjacency matrix ˆW ∈Rn×n is utilized to quantify the graph G, where ˆWij = exp(−(∥Xi −X j∥2/2σ 2 k )) [33] (σk is the kernel width) measures the similarity between examples Xi and X j (i, j = 1, 2, . . . ,n).The diagonal matrix D and the Laplacian matrix L of the graph G are, respectively, deﬁned as Dii = \u0002 j ˆWij L = D −ˆW. (3) Ideally, we hope that the similar examples revealed by G obtain similar clean labels, and the labels of dissimilar examples can be quite different.Therefore, we have the Laplacian regularizer that is derived as \u0002 i \u0002 j ˆWij ∥Xi Z −X j Z∥2 2 = tr((X Z)⊤L(X Z)) (4) in which “tr(·)” computes the trace of the corresponding matrix.By combining (2) and (4), the proposed LNSI model is ﬁnally formulated as min Z,E ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X Z)⊤L(X Z))+λ3∥E∥2,1 s.t.Y = X Z + E, X Z ∈{−1, 1}n×c (5) in which λ1, λ2, and λ3 are the nonnegative tradeoff parame- ters.We note that (5) falls into an integer programming problem, which is generally NP-hard.To make problem (5) tractable, we relax the discrete constraint X Z ∈{−1, 1}n×c to a contin- uous convex set X Z ∈[−1, 1]n×c.It is a linear programming relaxation, which has been used in several prior works [34], [35].By doing so, we pursue to solve a simpler problem min Z,E ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X Z)⊤L(X Z))+λ3∥E∥2,1 s.t.Y = X Z + E, X Z ∈[−1, 1]n×c. (6) B. Optimization Directly solving the problem (6) is difﬁcult due to the exis- tence of coupled variables, which will make its optimization not have a closed-form solution.Consequently, we introduce two auxiliary variables J and B, and then, the problem (6) is converted to the following equivalent version: min Z,E,J,B ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 s.t.Y = B + E, B = X J, Z = J, B ∈[−1, 1]n×c. (7) The optimization problem (7) is convex and many off-the-shelf methods can be adopted to solve it.For efﬁciency, here we use the alternating direction method of multipliers (ADMMs), which alternatively optimizes the related variables in an iterative manner.The augmented Lagrangian function of (7) with the continuous convex constraint can be written as L (Z, E, B, J, M1, M2, M3) = ∥Z∥∗+λ1∥Z∥2 F +λ2tr((X J)⊤L(X J))+λ3∥E∥2,1 +tr \u0003 M⊤ 1 (Y −B −E) \u0004 +tr \u0003 M⊤ 2 (B −X J) \u0004 +tr \u0003 M⊤ 3 (Z −J) \u0004 + μ 2 (∥Y −B −E∥2 F +∥B −X J∥2 F + ∥Z −J∥2 F) (8) where M1, M2, and M3 are the Lagrangian multipliers, and μ > 0 is the penalty coefﬁcient.We can sequentially minimize each of the variables Z, E, B, and J by ﬁxing the others in every iteration.Update Z: The subproblem related to Z is min Z ∥Z∥∗+ λ1∥Z∥2 F + tr\u0003M⊤ 3 (Z −J)\u0004 + μ 2 ∥Z −J∥2 F ⇒min Z ∥Z∥∗+ λ1∥Z∥2 F + tr \u0003 M⊤ 3 Z −M⊤ 3 J \u0004 + μ 2 tr((Z⊤−J⊤)(Z −J)) ⇒min Z ∥Z∥∗+ λ1tr(Z⊤Z) + tr \u0003 M⊤ 3 Z \u0004 + μ 2 tr(Z⊤Z −2J⊤Z) ⇒min Z ∥Z∥∗+ 2λ1 + μ 2 × tr \u0005 Z⊤Z − 2 2λ1 + μ(μJ −M3)⊤Z \u0006 ⇒min Z 1 2λ1 + μ∥Z∥∗+ 1 2 \u0007\u0007\u0007\u0007Z − 1 μ + 2λ1 (μJ −M3) \u0007\u0007\u0007\u0007 2 F ⇒min Z τ∥Z∥∗+ 1 2∥Z −ˆT∥2 F (9) where ˆT = (1/(μ + 2λ1))(μJ −M3) and τ = (1/(2λ1 + μ)).Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2571\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.4.Illustration of convergence process of the ADMM method adopted by LNSI on the four practical data sets.For each data set, we present the convergence curves under different convergence criteria in the Algorithm 1. (a)–(c) ISOLET data set. (d)–(f) COIL20 data set. (g)–(i) MNIST data set. (j)–(l) CIFAR-10 data set.tr((X Z)⊤L(X Z)).To this end, we study the performances of three different settings on the above four practical data sets (such as ISOLET, COIL20, MNIST, and CIFAR-10).First, both low-rank regularizer and Laplacian regularizer are reserved to constitute the original model (abbreviated as “LNSI”); second, the Laplacian regularizer is removed from the original model to see how this term inﬂuences the model performance (abbreviated as “No Laplacian”); third, we remove the low-rank regularizer while keeping the Lapla- cian regularizer to observe the effect of low-rank regularizer (abbreviated as “No Low-Rank”).The experimental results of these three models are illus- trated in Fig.5, in which 40% and 60% of training examples have incorrect labels.The results reveal that LNSI achieves the best performance on all four data sets, especially when the label noise rate is relatively high.By contrast, the accuracy of LNSI will drop without any of the two terms such as low-rank regularizer and Laplacian regularizer, and therefore, these two regularization terms are essential to boost the performance of LNSI.I. Parametric Sensitivity Note that the objective function (8) in LNSI contains three tradeoff parameters λ1, λ2, and λ3 that should be manually Fig.5.Results of ablation study on four practical data sets.For convenience, the original model is denoted as “LNSI,” the setting without the Laplacian regularization term is dubbed as “No Laplacian,” and the setting without the low-rank term is named as “No Low-Rank.” (a) ISOLET data set. (b) COIL20 Data set. (c) MNIST data set. (d) CIFAR-10 data set.tuned.Therefore, in this section, we discuss whether the choices of them will signiﬁcantly inﬂuence the performance of LNSI.To this end, we examine the classiﬁcation accuracy at two different levels (20% and 60%) of label noise via changing one of λ1, λ2, and λ3, and meanwhile ﬁxing the others to the optimal constant values under different data sets and different noise rates.The above-mentioned four practical data sets are adopted here, and the results are shown in Fig.6.Fig.6(a)–(c) shows the experiments on the ISOLET data set, (d)–(f) shows the experiments on the COIL20 data set, (g)–(i) shows the experiments on the MNIST data set, and (j)–(l) shows the experiments on the CIFAR-10 data set.The results reveal that LNSI is robust to the variations of λ1 and λ2 in a wide range, so they can be easily tuned for practical use.Meanwhile, the performance of LNSI varies when λ3 changes in a wide range, as λ3 controls the capability of our method for capturing the label noise via the error matrix E. Speciﬁcally, if λ3 is large, the label noise will be greatly ignored, leading to the performance degradation of LNSI on COIL20 and MNIST when the noise rate is 60%.J. Summary of Experiments Based on the above-mentioned experimental results of Sections VI-B–VI-I, we observe that: 1) LNSI performs bet- ter than other baseline algorithms in most cases, both on benchmark data sets and practical data sets; 2) the proposed algorithm in Algorithm 1 converges quickly to a stationary point; 3) LNSI is robust to the variation of the two tradeoff parameters including λ1 and λ2; 4) when the label noise is serious, the performance of LNSI might drop when λ3 is set to a large value; 5) the introduced low-rank regularizer and Laplacian regularizer are both beneﬁcial to improve the classiﬁcation performance; and 6) the proposed LNSI outper- forms other compared baselines when 40% and 60% labels Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2393\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 9 Fig.2.Algorithm validation on the synthetic data set. (a) Initial state with labeled negative examples (in red) and labeled positive examples (in blue).Note that two examples are labeled incorrectly and they form the label noise. (b) Error matrix E, in which the nonzero rows capture the mislabeled examples successfully.TABLE I STATISTICS OF THE BENCHMARK DATA SETS (Nos.1–3) and 5 positive examples (Nos.4–8).The negative examples are denoted by red dots, while the positive examples are represented by blue dots.Note that the 2nd example is mistakenly labeled as positive and the 6th example is erroneously labeled as negative, so they form the label noise in this data set.As explained in Section I, the observed label matrix Y is decomposed as two parts, one is the groundtruth label matrix T = X Z and the other is the error capturing matrix E. After applying LNSI to this synthetic data set, we investigate whether the matrix E can accurately detect the mislabeled examples.In Fig.2(b), we see that all rows in E are zeros except the 2nd and 6th rows that exactly correspond to the examples with label noise.This implies that the matrix E is able to capture the label errors successfully and also indicates that a suitable projection matrix Z has been learned.B. Experiments on Benchmark Data Set In this section, we compare LNSI with LICS, ULE, μSGD, and LSLN on ﬁve UCI benchmark data sets1 including CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4.The information about the data sets is summarized in Table I. For each data set, all the algorithms are tested at different levels (0%, 20%, 40%, and 60%) of label noise on training sets.Given the noise rate ζ, we manually inject the label noise by randomly picking up ζ × n training examples and then switching the correct label of each of them to a random wrong label, in which n is the number of training examples.All the reported accuracies are the mean values of the outputs of ﬁve independent runs.Generally, the number of the nearest neighbors in the graph is suggested to set to a small value, as it has been widely 1https://archive.ics.uci.edu/ml/data sets.html observed that a sparse graph can usually lead to good per- formance.The parameter σk controls the connective strength of pairwise examples, and it is chosen below 1 as all features have been normalized.The number of the nearest neighbors in the graph was selected by searching the grid {5, 10, 15, 20}.Similarly, the kernel width σk was also turned by searching the grid {0.01, 0.1, 0.5, 1}.We established the 5-NN graph for LNSI on Wine and Breast Tissue and the 10-NN graph on CNAE9, Pendigits and Connect-4.The kernel width σk on Connect-4 was 1 and on the other four data sets was chosen as 0.1.The tradeoff parameters λ1, λ2, and λ3 were selected by searching the grid {10−4, 10−3, . . . ,105} on Wine and Breast Tissue, and the grid {10−2, 10−1, . . . ,103} on CNAE9, Pendigits, and Connect-4.The classiﬁcation accuracies of all the compared methods are shown in Fig.3.Note that LICS and LSLN are not compared on the Connect-4 data set as they are not scalable to this data set.From Fig.3, we observe that LNSI yields better perfor- mance than other baselines in most cases.An exceptional case is that ULE and LSLN obtain the best results on Breast Tissue and Pendigits under the noise level 0%, respectively.Besides, we note that ULE performs satisfactorily on Wine under the noise level 60%.However, LNSI is generally the most robust method compared with all the other baselines.Therefore, formulating the classiﬁcation task under label noise as a matrix recovery problem does help to improve the robustness of the trained classiﬁer.C. Experiments on ISOLET Data Set To demonstrate the superiority of LNSI in dealing with different kinds of practical problems, we ﬁrst use the ISOLET2 data set to test the ability of all compared methods on the speech recognition task.A subset of ISOLET is established for our experiments, which contains 30 speakers speaking the name of each letter of the alphabet (i.e., “A”–“Z”) twice.As a result, we have totally 30 × 26 × 2 = 1560 examples, and each example is encoded as a 617-dimensional feature vector.Our task is to identify which of the 26 letters each example belongs to.Among these examples, we randomly pick up 20% of examples to establish the test set, and the remaining 80% of examples form the training set.To incorporate different levels of label noise, we randomly select 0%, 20%, 40%, and 60% of examples in the training set and then switch their accurate labels to the wrong values.Such example selection and label corruption are conducted ﬁve times, so every compared algo- rithm should independently run ﬁve times on the contaminated data set and the average accuracy on the test set over these ﬁve different runs is particularly investigated.The standard deviation over the ﬁve runs is also reported to display the stability of each compared algorithm.In LNSI, a 10-NN graph with kernel width σk = 0.1 was established.The tradeoff parameters in (6) such as λ1, λ2 and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,103}.From Table II, we see that the performances of LNSI are signiﬁcantly better than baseline methods in almost all cases.2http://www.cad.zju.edu.cn/home/dengcai/Data/MLData.html Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2341\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 3 Other representative works targeting label noise include [20] and [21].B. Side Information Side information serves as the additional information for accomplishing a certain task, which has been shown to be useful in solving many related problems.Practically, side information may have diverse formations.Some works on the recommender system treat the user features and product features as side information so that the quality of completion of user-product preference matrix can be improved.For instance, in order to recover the unknown user scores of the preference matrix, Chiang et al. [10] incorporate the user and product side information to “describe” the row entities and column entities of a matrix, respectively.Similarly, Guo [11] develops a coembedding framework for matrix com- pletion with side information, which learns a feature mapping as well as a label embedding.Zhao and Guo [22] propose a joint discriminative prediction model for personalized top-N recommendations with side information.Other works take the available relationship between exam- ples as side information.For example, Aggarwal et al. [23] perform text clustering by utilizing the links in the docu- ment, user-access behavior from web logs, or other nontextual attributes.Zhao et al. [8] conduct clustering with the aid of must-links and cannot-links between examples in addition to the regular input features.Zhang et al. [24] tackle the semi- supervised classiﬁcation by harnessing the example pairwise constraints as side information.Ahn et al. [25] study the binary rating estimation problem to quantify the value of graph side information, such as social graphs.In addition, Xue et al. [26] assimilate side information of the same format as observation in the robust principal component analysis.From the above-mentioned analyses, we see that side infor- mation has been widely used in matrix completion, clustering, and semisupervised learning.However, it has not been well deployed in label noise handling that is the main target of this article.III.PRELIMINARIES The utilization of side information for matrix completion has been studied by Chiang et al. [10], in which the model and theoretical analyses are studied when the side information contains noise.Suppose ˆR ∈Rn1×n2 is a matrix with rank r that should be recovered, and ˆX ∈Rn1×d1 and ˆY ∈Rn2×d2 (d1 and d2 denote feature dimensionality), respectively, record the row feature and column feature of ˆR. The matrices ˆX and ˆY serve as the side information for recovering ˆR. The model proposed by Chiang et al. [10] to recover ˆR is formulated as min M,N \u0002 (i, j)∈\u0002 ℓ(( ˆX M ˆY ⊤+N)ij , ˆRij )+λM∥M∥∗+λN∥N∥∗ (1) where “∥· ∥∗” is the nuclear norm, and “\u0002” is the index set containing all observed entries of ˆR. The observed ˆR can be decomposed into two parts: one is the low-rank matrix estimated from feature space ˆX M ˆY ⊤, in which M ∈Rd1×d2 is a coembedding matrix from features ˆX and ˆY, and the other part is N ∈Rn1×n2, which is used to capture the information that noisy features ( ˆX and ˆY) fail to describe.The loss function “ℓ(·)” requires that the recovered matrix is consistent with ˆR on the observed entries.Naturally, both ˆX M ˆY ⊤and N are preferred to be low-rank since they are aggregated to estimate a low-rank matrix ˆR, which further leads M to be a low-rank matrix.λN and λM are the tradeoff parameters.The underlying matrix ˆR can be estimated by ˆX M∗ˆY ⊤+ N∗, where M∗and N∗are the optimizers of (1).The effectiveness of this model for recovering the imperfect matrix has been theoretically and empirically veriﬁed.Inspired by this work, we treat the noisy label removal problem as a label matrix recovery problem, and the example features are regarded as the descriptions of the row enti- ties.Therefore, our LNSI inherits the good property of the model (1), and thus, the good performance is guaranteed.IV.OUR PROPOSED LNSI In this section, we ﬁrst describe our proposed model in Section IV-A and then provide the optimization algorithm in Section IV-B. A. Model We now introduce LNSI on label noise handling from the new viewpoint of side information.Let Y ∈Rn×c be the observed label matrix with corrupted entries, where n is the number of examples and c is the number of classes.X ∈Rn×d (d denotes the feature dimensionality) is the feature matrix, where the Xi representing the ith row of X records the feature of the ith example.The element Yij = 1 if the ith example has the label j ( j = 1, 2, . . . ,c), and Y ij = −1 otherwise.As explained in Section I, the features in X are taken as the side information for recovering the correct label matrix in this article.Speciﬁcally, we propose to decompose the observed Y into two parts, where the ﬁrst one is the low-rank groundtruth label matrix X Z estimated by the projection Z on the feature matrix X, and the second part is E ∈Rn×c describing the difference between the observed labels and accurate labels.Here Z ∈Rd×c is the projection matrix, and E can be used to capture the noisy labels.Since the “clean” label matrix X Z is low-rank, the projection matrix Z should be low-rank as well.This is because only a small fraction of columns in X are sufﬁcient to construct the low-rank space X Z. As X Z is the low-rank groundtruth label matrix, each entry in it should be −1 or 1.The label errors can be captured by a row-sparse matrix E since the label noise in training examples is usually sparse, and thus, the number of corresponding nonzero rows should be small.In addition, the Frobenius norm is imposed to Z to avoid overﬁtting.By putting all the above together, we consider solving the following problem: min Z,E ∥Z∥∗+ λ1∥Z∥2 F + λ3∥E∥2,1 s.t.Y = X Z + E, X Z ∈{−1, 1}n×c (2) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.2130\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 13 Fig.6.Parametric sensitivity of LNSI. (a)–(c), (d)–(f), (g)–(i), and (j)–(l) ISOLET, COIL20, MNIST, and CIFAR-10 data sets, respectively. (a), (d), (g), and (j) Variation in accuracy with respect to the tradeoff parameter λ1 when λ2 and λ3 are ﬁxed to the values indicated in the legend. (b), (e), (h), and (k) Inﬂuence of λ2 to the classiﬁcation performance with other parameters ﬁxed. (c), (f), (i), and (l) Effect of λ3 while other two tradeoff parameters are ﬁxed.of training examples are incorrect because the error matrix E can capture the label errors successfully and the Laplacian regularizer is essential to boost the performance of LNSI.VII.CONCLUSION To solve the label inaccuracy that often occurs in plenty of real-world data sets for classiﬁcation, this article provides a novel paradigm that formulates the noisy label removal problem as a matrix recovery problem and treats the example features as the side information to aid the recovery process.Our proposed LNSI seamlessly forms the label noise removal and classiﬁer parameter optimization into a uniﬁed framework.The convergence property, computational complexity, and gen- eralization bound are also theoretically analyzed.We tested LNSI on various benchmark and practical data sets under different levels of label noise and found that LNSI outperforms other compared baseline methods and achieves robust results to different levels of label noise.APPENDIX PROOF OF THE CONVERGENCE To begin with, we provide the following two lemmas for the general ADMM solver.Lemma 12 [47]: Given the optimization problem with lin- ear constraints as min P, Q f (P) + g( Q) s.t.AP P + BQ Q = C (40) where AP and BQ are the coefﬁcient matrices, and f (P), g( Q) are two functions with respect to the variables P, Q, respectively.C is a constant.Then, the Lagrangian function of (40) is Lμ = f (P) + g( Q) + tr(M′⊤(AP P + BQ Q −C)) + μ 2 ∥AP P + BQ Q −C∥2 F (41) where M′ is the Lagrangian multiplier and μ > 0 is the penalty coefﬁcient.ADMM consists of the iterations Pk+1 := arg min P Lμ(P, Qk, M′k) (42) Qk+1 := arg min Q Lμ(Pk+1, Q, M′k) (43) M′k+1 := M′k + μ(AP Pk+1 + BQ Qk+1 −C). (44) If f (P) and g( Q) are convex, proper, and closed functions, and the unaugmented Lagrangian L0 = f (P) + g( Q) + tr(M′⊤(AP P + BQ Q −C)) has a saddle point, the sequences {Pk, Qk, M′k} generated by the ADMM algorithm are guar- anteed to converge.Lemma 13 [47]: The generic-constrained convex optimiza- tion problem regarding the variable P is min P f (P) s.t.P ∈C (45) where f (·) and C are convex.Problem (45) can then be rewritten in the form of ADMM as min f (P) + g(Zc) s.t.P −Zc = O (46) where g is the indicator function of C. Now we begin to formally verify the convergence of Algorithm 1.The proof of Theorem 2 is presented as follows.Proof: To facilitate the proof, we rewrite the optimization problem (7) in the formation of (40) according to Lemma 12.First, the optimization problem (7) with the convex set constraint can be transformed to the formation of (46) in Lemma 13.Therefore, problem (7) is equivalent to min Z,E ∥Z ∥∗+ λ1∥Z∥2 F + λ2tr((X J)⊤L(X J)) + λ3∥E∥2,1 + IC(K) s.t.Y = B + E, B = X J, J = Z, K = B (47) where IC(x) = \u0015 x, if x ∈C ∞, othrewise. (48) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1988\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.1.Motivation illustration. (a) Four examples from two classes, among which the label of the 3rd example is incorrect. (b) Y is the corrupted label matrix, of which the rows represent the label vectors of four examples displayed in (a).By taking the example feature matrix X as side information, the observed label matrix Y can be ideally decomposed as the sum of a low-rank recovered label matrix T = X Z∗and a row-sparse matrix E. Note that the nonzero row in E exactly corresponds to the 3rd example with noisy label.task, which has been widely used in many machine learning ﬁelds such as clustering [8] and multi-label learning [9].For example, Zhao et al. [8] propose the matrix completion-based approach for multi-view clustering and ﬁrst introduce the side information to aid the clustering process.Xu et al. [9] explore the side information to reduce the requirement on the number of observed entries for matrix completion and apply the method to transductive incomplete multi-label learning.From the review, we know that the side information has not been utilized for removing noisy labels.Therefore, this article provides a new paradigm for dealing with the label noise problem from the viewpoint of side information.Speciﬁcally, we formulate label correction as a label matrix recovery problem and treat the example features as side infor- mation to aid the recovery process [9]–[11].Therefore, our proposed method is named as “label noise handling via side information” (LNSI).The paradigm of this article is shown in Fig.1, which intuitively explains how to transform the noisy label removing problem to the label matrix recovery task by exploiting the side information.Fig.1(a) shows the case where four examples belong to two classes, in which the 3rd example has been mistakenly labeled as positive and the noisy label is constituted.As illustrated in Fig.1(b), given the observed label matrix Y and corresponding feature matrix X, the true labels T can be obtained by conducting a low-rank mapping Z∗on the example features (i.e., T = X Z∗), and the incorrect labels are captured by a row-sparse matrix E. The merits of our paradigm lie in three aspects: 1) LNSI is inherently suitable for multi-class classiﬁcation, which does not need the one-versus-one or one-versus-the-rest operations; 2) sufﬁcient theoretical results have demonstrated that the real label matrix can be exactly recovered under mild conditions [12], so LNSI is guaranteed to obtain satisfactory performance; and 3) LNSI seamlessly integrates the label noise removal and classiﬁer parameter optimization into a uniﬁed framework.Due to the above merits, a reliable classiﬁer can be learned to accurately classify the unseen test examples with different levels of training label noise.Furthermore, the experimental results on both benchmark data sets and practical data sets verify the superiority of the learned classiﬁer.II.RELATED WORK This section brieﬂy reviews the representative prior works on label noise handling and side information utilization, as they are related to the proposed LNSI.A. Label Noise Handling Practically, the labels of training examples are often not reliable due to various limitations in data acquisition and data processing, and the noisy labels often occur that hinders the machine learning model to achieve sound performance.One straightforward idea to address this problem is to improve the quality of training data.Since the training data are associated with noisy labels, the early-stage approaches ﬁrst detect and eliminate label noise and then conduct the standard supervised classiﬁcation algorithm.To implement noise detection, some works [13] explore the neighborhood relationship while some approaches rely on ensemble ﬁl- ters [14].Nevertheless, the performances of these approaches are very sensitive to the quality of noise detection, which makes them unreliable for practical use.To avoid the explicit noise detection step, plenty of efforts have been made recently to develop the algorithms that are inherently effective and robust to the noisy labels.Patrini et al. [6] decompose the conventional loss function into a label-independent part and a label-dependent part, in which only the latter is affected by label noise.Conse- quently, various surrogate loss functions can be designed.Similarly, Gao et al. [3] tackle the second part by deploying the labeled instance centroid to reduce the inﬂuence caused by label noise.Natarajan et al. [4] provide an unbiased estimator of loss function to deal with the symmetric noise, while Van Rooyen et al. [15] modify the traditional hinge loss and prove its robustness to label noise.Although these algorithms can reduce the adverse impact of label noise to some degree, they can only handle canonical binary classiﬁcation and lack the theoretical guarantee of exact recovery on accurate labels.Recently, some methods try to extend deep learning models to the case of noisy labels.For example, Khetan et al. [16] propose a new supervised learning algorithm which can jointly model labels and worker quality from noisy data.Patrini et al. [17] present a loss correction approach to train deep models that are robust to label noise.Han et al. [18] train two deep neural networks simultaneously and let them teach each other given every minibatch for combating with noisy labels.Meanwhile, Han et al. [19] also estimate the noise transition matrix with the assistance of human cognition and then derive a structure-aware probabilistic model for label noise handling.However, these deep learning-based methods are only suitable for speciﬁc tasks related to image analysis or natural language processing [17].Moreover, the perfor- mance of these methods generally lacks theoretical guarantees.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1617\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 7 Lemma 6 [40]: The function F : Rd×c →R deﬁned as F(W) = (1/2)∥W∥2 2,2 is (1/2)-strongly convex with respect to ∥· ∥2,2 over Rd×c, where ∥· ∥2,2 := ∥· ∥F. By combining Lemmas 5 and 6 with the bound given in Lemma 4, we obtain the following two corollaries.Corollary 7: Let W = {W : ∥W∥2,1 ≤W2,1} and A = {A ∈Rn×c : ∥A∥2,∞≤A2,∞}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 2,q for q = (ln(c)/(ln(c) −1)) is bounded as Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σαtr(W⊤A(α)) \u0014 ≤W2,1A2,∞ \u0012 3 ln(c) nr (23) with the fact that the dual norm of ℓ2,1 is ℓ2,∞. Corollary 8: Let W = {W : ∥W∥F ≤WF} and A = {A ∈ Rd×c : ∥A∥F ≤AF}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 2,2 is bounded as Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σαtr(W⊤A(α)) \u0014 ≤WFAF \u0012 2 nr (24) with the fact that the dual norm of the Frobenius norm is the Frobenius norm.Lemma 9 [10]: Let W = {W : ∥W∥∗≤W∗} and A = {A ∈Rd×c : ∥A∥2 ≤A2}, and then the empirical Rademacher complexity of the function class with F(W) = (1/2)∥W∥2 ∗is bounded as Eσ \u0013 sup f ∈F 1 nr nr \u0002 α=1 σαtr(W⊤A(α)) \u0014 ≤W∗A2 \u0012 ln(2dc) nr (25) with the fact that the dual norm of the nuclear norm is the spectral norm and dc = max(d, c).Lemma 10: Let Am1 ∈Rm1×n1 and Am2 ∈Rn1×m2 be two matrices, and then the following inequality holds: λmin \u0003 A⊤ m1 Am1 \u0004 ∥Am2∥2 F ≤∥Am1 Am2∥2 F (26) where λmin(·) represents the smallest eigenvalue of the matrix inside the bracket.Proof: For a given complex Hermitian matrix Mr and nonzero vector xr, the Rayleigh quotient R(Mr, xr) [42], [43] is deﬁned as R(Mr, xr) = x∗ r Mr xr x∗r xr (27) and the following inequality holds: λmin(Mr) ≤R(Mr, xr) ≤λmax(Mr). (28) Let a = Vec(Am2), where Vec(·) is an operator converting a matrix into a vector, and then, ∥Am1 Am2∥2 F can be rewritten in the form of ℓ2-norm as ∥Am1 Am2∥2 F = ∥(I ⊗Am1)a∥2 2 = a⊤(I ⊗Am1)⊤(I ⊗Am1)a = a⊤\u0003 I ⊗A⊤ m1 \u0004 (I ⊗Am1)a = a⊤\u0003 I ⊗A⊤ m1 Am1 \u0004 a (29) where ⊗represents the Kronecker product [44] and I is an identity matrix with proper size.Combining (28) with (29), the following inequality holds, namely: λmin \u0003 A⊤ m1 Am1 \u0004 ∥Am2∥2 F = λmin \u0003 I ⊗A⊤ m1 Am1 \u0004 ∥Am2∥2 F = λmin \u0003 I ⊗A⊤ m1 Am1 \u0004 (a⊤a) ≤a⊤\u0003 I ⊗A⊤ m1 Am1 \u0004 a = ∥Am1 Am2∥2 F (30) which completes the proof.□ Provided Lemma 10, tr((X Z)⊤L(X Z)) ≤Ztr in (21) can be derived into a constraint in the form of Frobenius norm on Z. Speciﬁcally, note that the Laplacian matrix is positive semideﬁne and its SVD is L = U L\u0002LU⊤ L = U L\u0002(1/2) L \u0002(1/2) L U⊤ L , so we have tr((X Z)⊤L(X Z)) = tr \u0003 (X Z)⊤U L\u0002 1 2 L\u0002 1 2 LU⊤ L (X Z) \u0004 = tr \u000f\u000f \u0002 1 2 LU⊤ L X Z \u0010⊤\u000f \u0002 1 2 LU⊤ L X Z \u0010\u0010 = \u0007\u0007\u0007\u0002 1 2 LU⊤ L X Z \u0007\u0007\u0007 2 F ≥λmin(X⊤LX)∥Z∥2 F. (31) If λmintr = λmin(X⊤LX) > 0, we have ∥Z∥F ≤Zt, where Zt = (Ztr/λmintr)1/2.In addition, we manage to rewrite the constraint X Z ∈ [−1, 1]n×c as the form of Frobenius norm on Z. Since X Z ∈ [−1, 1]n×c and ∥X Z∥2 F ≤nc, then similar to (31), we may obtain that ∥Z∥F ≤Zb if λminb = λmin(X⊤X) > 0, where Zb = (nc/λminb)1/2.Taking the three different Frobenius norm-based constraints (i.e., ∥Z∥F ≤√ZF, ∥Z∥F ≤Zt, ∥Z∥F ≤Zb) on the matrix Z into account, and let Zmin = min{√ZF, Zb, Zt}, we have ∥Z∥F ≤ \u0015√ZF, if λmintr ≤0 or λminb ≤0 Zmin, otherwise. (32) For convenience, we denote the upper bound of ∥Z∥F as Z that can be √ZF or Zmin according to different conditions in (32).Herein, we begin to formally derive the generalization error of LNSI.Theorem 11: Let X2 = maxi ∥Xi∥2 and XF = maxi ∥Xi∥F, and then the model complexity of function class F\r is upper bounded by Rn(F\r) ≤min \u0015 Z∗X2 \u0016 ln(2dc) nc , ZXF \u0016 2 nc \u0017 + E2,1 \u0016 3 ln(c) nc . (33) Proof: First, the Rademacher complexity of linear function class F\r in our case can be written as R(F\r) := Eσ \u0013 sup θ∈\r 1 nc nc \u0002 α=1 σα \u0003 Xiα ZI jα + Eiα, jα \u0004 \u0014 . (34) Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1565\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS 1 Harnessing Side Information for Classiﬁcation Under Label Noise Yang Wei , Chen Gong , Member, IEEE, Shuo Chen , Tongliang Liu , Member, IEEE, Jian Yang , Member, IEEE, and Dacheng Tao , Fellow, IEEE Abstract—Practical data sets often contain the label noise caused by various human factors or measurement errors, which means that a fraction of training examples might be mistakenly labeled.Such noisy labels will mislead the classiﬁer training and severely decrease the classiﬁcation performance.Existing approaches to handle this problem are usually developed through various surrogate loss functions under the framework of empiri- cal risk minimization.However, they are only suitable for binary classiﬁcation and also require strong prior knowledge.Therefore, this article treats the example features as side information and formulates the noisy label removal problem as a matrix recovery problem.We denote our proposed method as “label noise handling via side information” (LNSI).Speciﬁcally, the observed label matrix is decomposed as the sum of two parts, in which the ﬁrst part reveals the true labels and can be obtained by conducting a low-rank mapping on the side information; and the second part captures the incorrect labels and is modeled by a row-sparse matrix.The merits of such formulation lie in three aspects: 1) the strong recovery ability of this strategy has been sufﬁciently demonstrated by intensive theoretical works on side information; 2) multi-class situations can be directly handled Manuscript received August 13, 2018; revised January 17, 2019 and April 11, 2019; accepted August 26, 2019.This work was supported by NSF of China under Grant 61602246, Grant 61973162, and Grant U1713208, in part by NSF of Jiangsu Province under Grant BK20171430, in part by the Fundamental Research Funds for the Central Universities under Grant 30918011319, in part by the Open Project of the State Key Laboratory of Integrated Services Networks through Xidian University under Grant ISN19- 03, in part by the “Summit of the Six Top Talents” Program under Grant DZXX-027, in part by the “Young Elite Scientists Sponsorship Program” by Jiangsu Province, in part by the “Young Elite Scientists Sponsorship Program” by CAST under Grant 2018QNRC001, in part by the Program for Changjiang Scholars, in part by the “111” Program AH92005, in part by the ARC FL-170100117, in part by DP180103424, and in part by DE190101473. (Corresponding author: Chen Gong.)Y. Wei and C. Gong are with the School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the State Key Laboratory of Integrated Services Networks, Xidian University, Xi’an, China (e-mail: csywei@njust.edu.cn; chen.gong@njust.edu.cn).S. Chen and J. Yang are with the PCA Laboratory, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, with the Key Laboratory of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing 210094, China, and also with the Jiangsu Key Laboratory of Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing University of Science and Technology, Nan- jing 210094, China (e-mail: shuochen@njust.edu.cn; csjyang@njust.edu.cn).T. Liu and D. Tao are with the UBTECH Sydney Artiﬁcial Intelligence Centre, School of Computer Science, Faculty of Engineer- ing, The University of Sydney, Darlington, NSW 2008, Australia (e-mail: tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au).Color versions of one or more of the ﬁgures in this article are available online at http://ieeexplore.ieee.org.Digital Object Identiﬁer 10.1109/TNNLS.2019.2938782 with the aid of learned projection matrix; and 3) only very weak assumptions are required for model design, making LNSI applicable to a wide range of practical problems.Moreover, we theoretically derive the generalization bound of LNSI and show that the expected classiﬁcation error of LNSI is upper bounded.The experimental results on a variety of data sets including UCI benchmark data sets and practical data sets conﬁrm the superiority of LNSI to state-of-the-art approaches on label noise handling.Index Terms—Classiﬁcation, generalization bound, label noise, matrix recovery, side information.I. INTRODUCTION T RADITIONALLY, a reliable supervised classiﬁer, such as support vector machines (SVMs) or convolutional neural networks (CNNs), is usually trained based on the sufﬁ- cient correctly labeled data.Unfortunately, the real-world data sets often contain the noise in label space, which means that a fraction of training examples are erroneously labeled [1].For instance, as the numerous examples in many applications (e.g., image classiﬁcation and document categorization) are manu- ally annotated, the labeling errors are inevitably introduced due to the human fatigue.Disease diagnosis, in which the decision is strongly dependent on the experience and expertise of the doctors, is also very likely to include labeling errors.These noisy labels will signiﬁcantly mislead the classiﬁer training and then severely decrease the classiﬁcation performance [2].Hence, designing algorithms that account for the data with noisy labels is of great signiﬁcance and has become a critical issue in the machine learning community.Several approaches have been proposed to deal with the learning problem with label noise to prevent the performance decrease, and most of them are based on the minimization of empirical risk via a conditional probability model [3]–[6].For example, Gao et al. [3] and Patrini et al. [6] analyze the empirical risk minimization in the presence of label noise by decomposing the loss function into a label-independent part and a label-dependent part.Manwani and Sastry [5] study the noise tolerance properties of risk minimization under differ- ent loss functions and provide insightful theoretical results.However, they are only applicable to binary classiﬁcation and the extension to multi-class is nontrivial [7].Moreover, these methods require the estimation of class prior, which is actually quite difﬁcult in the presence of corrupted observed data.On the other hand, side information is often utilized as additional knowledge to boost the performance of the certain 2162-237X © 2019 IEEE.Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.1109\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS Fig.3.Experimental results of the compared methods on ﬁve UCI benchmark data sets. (a)–(e) CNAE9, Wine, Breast Tissue, Pendigits, and Connect-4 data sets, respectively.TABLE II COMPARISON OF VARIOUS METHODS ON ISOLET DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE III COMPARISON OF VARIOUS METHODS ON COIL20 DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD.)UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD In addition, LNSI is much more robust than other compared methods, as their performances decrease sharply with the increase of levels of label noise.D. Experiments on COIL20 Data Set This section tests the performance of LNSI on an image data set, i.e., the COIL203.COIL20 is a popular public data set for object classiﬁcation, which includes 1440 object images belonging to 20 classes, and each class has 72 images shot from different angles.The resolution of each gray-level image is 32 × 32 [46].We use the output of the ﬁrst fully connected layer of VGGNet-16 as the CNN features, and therefore, each image in COIL20 can be represented by a feature vector with 4096 dimensions.Similar to the experimental settings in Section VI-C, we randomly pick up 20% of examples for test, and the remaining 80% of examples are served as training data.Also, we ran- domly select 0%, 20%, 40%, and 60% of training examples and switch the accurate label of each of them to a random wrong label.In this data set, we establish a 10-NN graph 3http://www.cs.columbia.edu/CAVE/software/softlib/coil-20.php with σk = 0.1.In addition, the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−2, 10−1, . . . ,103} in order to obtain the satisfactory results.The classiﬁcation accuracies of all compared methods under different label noise levels are presented in Table III.It can be observed that the performances of all methods decrease with the increase in noise level.However, LNSI achieves the best results in most cases when compared with other baseline methods.Another notable fact is that LNSI performs robustly under different levels of label noise, while the performances of baselines dramatically decrease when the noise rate ranges from 0% to 60%.E. Experiments on MNIST Data Set We use the MNIST4 data set to evaluate the capabilities of various methods on handwritten digit recognition.Here we use a subset of MNIST for our experiments, which contains 2000 training examples and 2000 test examples across ten different classes.The number of examples belonging to each 4http://yann.lecun.com/exdb/mnist/ Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.0991\n",
            "Text:\n",
            "This article has been accepted for inclusion in a future issue of this journal.Content is final as presented, with the exception of pagination.WEI et al.:HARNESSING SIDE INFORMATION FOR CLASSIFICATION UNDER LABEL NOISE 11 TABLE IV COMPARISON OF VARIOUS METHODS ON MNIST DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD TABLE V COMPARISON OF VARIOUS METHODS ON CIFAR-10 DATA SET.THE CLASSIFICATION ACCURACIES (MEAN ± STD) UNDER DIFFERENT LEVELS OF LABEL NOISE ARE PRESENTED. •/◦INDICATES THAT LNSI IS SIGNIFICANTLY BETTER/WORSE THAN THE CORRESPONDING METHOD (PAIRED t-TEST AT 95% CONFIDENCE LEVEL).THE BEST ACCURACY UNDER EACH LABEL NOISE LEVEL IS MARKED IN BOLD class ranges from 359 to 454 and 80% of these examples are randomly picked up to establish the training set.The resolution of each gray-level handwritten digit image is 28 × 28.We use the output of the ﬁrst fully connected layer of VGGNet- 16 to extract the CNN features for each image, and therefore, the dimensionality of one feature vector is 4096.All experimental settings are the same as those in Sections VI-C and VI-D. In this data set, a 10-NN graph with σk = 1 was established and the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {100, 101, . . . ,105}.Similarly, the mean accuracies and standard deviations of ﬁve independent runs of all comparators are reported, which can be found in Table IV.We see that when the noise rate is 0%, 20%, and 40%, LNSI is comparable with other baseline methods.However, LNSI achieves very robust and satisfactory performances on MNIST when the label noise rate is 60%, while all other baseline methods perform unsatisfactorily.Consequently, the existing methods are inferior to LNSI in terms of the classiﬁcation accuracy under serious label noise.F. Experiments on CIFAR-10 Data Set This section tests the performance of LNSI and baselines on the natural image data set, i.e., the CIFAR-105.Here we use a subset of CIFAR-10 for our experiments which contains 30 000 image examples randomly sampled from original data set across different classes.The resolution of each color image is 32 × 32 × 3.We extract the CNN features for each image, which are calculated as the output of the ﬁrst fully connected layer of VGGNet-16, and therefore, the dimensionality of one feature vector is 4096.5https://www.cs.toronto.edu/ kriz/cifar.html All experimental settings are the same as those in Sections VI-C–VI-E. In this data set, 80% of the examples are randomly selected as the training set.A 15-NN graph with σk = 0.5 was established, and the tradeoff parameters in (6) such as λ1, λ2, and λ3 were tuned by searching the grid {10−1, 100, . . . ,103}.The classiﬁcation accuracies of the compared algorithms under different label noise levels are presented in Table V, from which we can see that LNSI outperforms other baselines and is much more robust to noisy labels with the increase in the level of label noise.Note that LICS and LSLN are not compared on the CIFAR-10 data set as they are not scalable to this data set.G. Illustration of Convergence In Section V-A, we have theoretically proved that the optimization process in Algorithm 1 will converge to a sta- tionary point.In this section, we present the convergence curves of LNSI on the four practical data sets appeared in Sections VI-C–VI-F. From the curves shown in Fig.4, we see that the differences between the two sides of equality con- straints in (7) decrease rapidly on all data sets.This observa- tion justiﬁes our previous theoretical results and demonstrates that ADMM is effective and efﬁcient for solving (7).H. Ablation Study From the above-mentioned experimental results presented in Sections VI-B–VI-F, we see that LNSI performs favorably to other existing methods.Therefore, this section investi- gates the effects of key components of LNSI that leads to the good performance.Speciﬁcally, we conduct the ablation study on LNSI to explore the individual contributions of the low-rank regularizer ∥Z∥∗and the Laplacian regularizer Authorized licensed use limited to: NANJING UNIVERSITY OF SCIENCE AND TECHNOLOGY.Downloaded on July 20,2020 at 14:14:03 UTC from IEEE Xplore. Restrictions apply.\n",
            "\n",
            "\n",
            "\n",
            "Score: 0.0902\n",
            "Text:\n",
            "Process.Syst.,2015, pp.10–18. [16] A. Khetan, Z. C. Lipton, and A. Anandkumar, “Learning from noisy singly-labeled data,” in Proc.Int.Conf.Learn.Represent.,2018. [Online].Available: https://openreview.net/forum?id=H1sUHgb0Z [17] G. Patrini, A. Rozza, A. K. Menon, R. Nock, and L. Qu, “Making deep neural networks robust to label noise: A loss correction approach,” in Proc.Int.Conf.Comput.Vis.Pattern Recognit.,2017, pp.2233–2241. [18] B. Han et al., “Co-teaching: Robust training of deep neural networks with extremely noisy labels,” in Proc.Adv.Neural Inf.Process.Syst.,2018, pp.8536–8546. [19] B. Han et al., “Masking: A new perspective of noisy supervision,” in Proc.Adv.Neural Inf.Process.Syst.,2018, pp.5836–5846. [20] B. Han, I. W. Tsang, L. Chen, C. P. Yu, and S.-F. Fung, “Progressive stochastic learning for noisy labels,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.10, pp.5136–5148, Oct. 2018. [21] J. Zhang, V. S. Sheng, T. Li, and X. Wu, “Improving crowdsourced label quality using noise correction,” IEEE Trans.Neural Netw.Learn.Syst.,vol.29, no.5, pp.1675–1688, May 2018. [22] F. Zhao and Y. Guo, “Learning discriminative recommendation systems with side information,” in Proc.Int.Joint Conf.Artif.Intell.,2017, pp.3469–3475. [23] C. C. Aggarwal, Y. Zhao, and P. S. Yu, “On text clustering with side information,” in Proc.28th Int.Conf.Data Eng.,Apr. 2012, pp.894–904. [24] R. Zhang, F. Nie, and X. Li, “Semisupervised learning with parameter- free similarity of label and side information,” IEEE Trans.Neural Netw.Learn.Syst.,vol.30, no.2, pp.405–414, Feb. 2019. [25] K. Ahn, K. Lee, H. Cha, and C. Suh, “Binary rating estimation with graph side information,” in Proc.Adv.Neural Inf.Process.Syst.,2018, pp.4272–4283. [26] N. Xue, Y. Panagakis, and S. Zafeiriou, “Side information in robust principal component analysis: Algorithms and applications,” in Proc.Int.Conf.Comput.Vis.,Oct. 2017, pp.4317–4325. [27] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma, “Robust recovery of subspace structures by low-rank representation,” IEEE Trans.Pattern Anal.Mach.Intell.,vol.35, no.1, pp.171–184, Jan. 2013. [28] C. Gong, T. Liu, D. Tao, K. Fu, E. Tu, and J. Yang, “Deformed graph Laplacian for semisupervised learning,” IEEE Trans.Neural Netw.Learn.Syst.,vol.26, no.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. FUnction for the same"
      ],
      "metadata": {
        "id": "TDejr4lz6s5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_resources1(query: str, n_resources_to_return: int=17):\n",
        "    \"\"\"\n",
        "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
        "    \"\"\"\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
        "\n",
        "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
        "\n",
        "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n",
        "\n",
        "def retrieve_relevant_resources2(query: str, n_resources_to_return: int=17):\n",
        "    \"\"\"\n",
        "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
        "    \"\"\"\n",
        "    query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(\"cuda\")\n",
        "\n",
        "    dot_scores = util.dot_score(query_embedding, word_embeddings_tensor)[0]\n",
        "\n",
        "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
        "\n",
        "    return scores, indices\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:20:25.054294Z",
          "iopub.execute_input": "2024-10-17T18:20:25.05544Z",
          "iopub.status.idle": "2024-10-17T18:20:25.06307Z",
          "shell.execute_reply.started": "2024-10-17T18:20:25.055383Z",
          "shell.execute_reply": "2024-10-17T18:20:25.062003Z"
        },
        "trusted": true,
        "id": "iv-dRomt6s5u"
      },
      "outputs": [],
      "execution_count": 57
    },
    {
      "cell_type": "code",
      "source": [
        "scores,index_embeddings = retrieve_relevant_resources1(query)\n",
        "scores,index_embeddings"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:20:32.000754Z",
          "iopub.execute_input": "2024-10-17T18:20:32.001494Z",
          "iopub.status.idle": "2024-10-17T18:20:32.046018Z",
          "shell.execute_reply.started": "2024-10-17T18:20:32.00145Z",
          "shell.execute_reply": "2024-10-17T18:20:32.044998Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkpBm8bD6s5u",
        "outputId": "8b76eade-f08b-4e99-9a71-0042ee4a87c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4298, 0.3959, 0.3726, 0.3018, 0.2987, 0.2934, 0.2934, 0.2571, 0.2393,\n",
              "         0.2341, 0.2130, 0.1988, 0.1617, 0.1565, 0.1109, 0.0991, 0.0902],\n",
              "        device='cuda:0'),\n",
              " tensor([ 4,  5, 13, 16, 15,  7,  3, 11,  8,  2, 12,  1,  6,  0,  9, 10, 14],\n",
              "        device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "execution_count": 58
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clearly the vector at 4th index has the best retrievel but can we get the best output but there are other vectors containing some similarity score which might be of relevance to answer the query but if we take all this vectors then the context length of the LLM might be exhausted\n",
        "\n",
        "***Thus can we do some weighted sum of all this document to make it happen***"
      ],
      "metadata": {
        "id": "6V3ax5Sb6s5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores1,index_embeddings1 = retrieve_relevant_resources2(query)\n",
        "scores1,index_embeddings1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:20:56.329082Z",
          "iopub.execute_input": "2024-10-17T18:20:56.329489Z",
          "iopub.status.idle": "2024-10-17T18:20:56.378168Z",
          "shell.execute_reply.started": "2024-10-17T18:20:56.329447Z",
          "shell.execute_reply": "2024-10-17T18:20:56.376835Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-NHD99_6s5v",
        "outputId": "6ade25f6-03a4-4390-aa40-9a37fa00d8ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0.4697, 0.3270, 0.2985, 0.2804, 0.2782, 0.2579, 0.2574, 0.2476, 0.2425,\n",
              "         0.2391, 0.2360, 0.2254, 0.2157, 0.2145, 0.2065, 0.2026, 0.1973],\n",
              "        device='cuda:0'),\n",
              " tensor([209,  93,  76, 201,  75,  88,  95, 171, 221, 107,  50, 169, 750,  23,\n",
              "          78, 630, 219], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "execution_count": 59
    },
    {
      "cell_type": "code",
      "source": [
        "def print_top_results_and_scores(query: str, n_resources_to_return: int=5):\n",
        "    \"\"\"\n",
        "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
        "    \"\"\"\n",
        "    scores, indices = retrieve_relevant_resources(query, n_resources_to_return=n_resources_to_return)\n",
        "    for score, idx in zip(scores, indices):\n",
        "        print(f\"Score: {score:.4f}\")\n",
        "        print(\"Text\")\n",
        "        print(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "        print(\"\\n\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T18:21:17.627953Z",
          "iopub.execute_input": "2024-10-17T18:21:17.628377Z",
          "iopub.status.idle": "2024-10-17T18:21:17.637466Z",
          "shell.execute_reply.started": "2024-10-17T18:21:17.628338Z",
          "shell.execute_reply": "2024-10-17T18:21:17.636516Z"
        },
        "trusted": true,
        "id": "ZYH-XR8G6s5v"
      },
      "outputs": [],
      "execution_count": 60
    },
    {
      "cell_type": "code",
      "source": [
        "# print_top_results_and_scores(query)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-17T14:47:52.386949Z",
          "iopub.execute_input": "2024-10-17T14:47:52.387353Z",
          "iopub.status.idle": "2024-10-17T14:47:52.428757Z",
          "shell.execute_reply.started": "2024-10-17T14:47:52.387317Z",
          "shell.execute_reply": "2024-10-17T14:47:52.427875Z"
        },
        "trusted": true,
        "id": "zAtr4vo86s5v"
      },
      "outputs": [],
      "execution_count": 62
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Gemma-2b\n",
        "We will be using Gemma_instruct_2b for this."
      ],
      "metadata": {
        "id": "klnm1oMz6s5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combined Embeddings"
      ],
      "metadata": {
        "id": "2O_ZRZiV6s5v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zo2AfHGF6s5v"
      },
      "outputs": [],
      "execution_count": 62
    }
  ]
}